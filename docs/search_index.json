[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source: Flaticon This is the class manual for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2021. The content here was made by Leslie Myint and draws upon our class textbook, An Introduction to Statistical Learning with Applications in R, and on materials prepared by Alicia Johnson and Brianna Heggeseth. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule.html", "Schedule", " Schedule The schedule below is a tentative outline of our plans for the semester. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Goals. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Remember to take notes on where you paused/rewound/reread or smiled/nodded during the videos/readings. This is essential for the Metacognitive Reflection part of the course. Week 1: Intros and Evangelizing Evaluation (1/20 - 1/27) Dates Topic Videos/Readings Video Slides Class Slides 1/20 Introductions ISLR: Ch 1, Ch 2--Section 2.1(Skip 2.1.2, 2.1.3 for now.) 1/23 Evaluating Regression Models Evaluating Regression Models R: Introduction to TidyModels ISLR: 2.2 1/25 Overfitting Overfitting R: Preprocessing and Recipes ISLR: 5.1 1/27 Cross-validation Cross-validation R: Training, Testing and Cross-Validation ISLR: 5.1 N/A Start Homework 1 due Friday, 2/3 at midnight CST Week 2: A Soirée with Selection Strategies (1/30 - 2/3) Dates Topic Videos/Readings Video Slides Class Slides 1/30 Subset Selection Variable Subset Selection R: Subset Selection ISLR: 6.1 2/1 LASSO (Shrinkage/Regularization) LASSO (Shrinkage/Regularization) R: LASSO and Regularization ISLR: 6.2 2/3 LASSO (continued) Finish Homework 1 due Friday, 2/3 at midnight CST Week 3: A Treasure in Tradeoffs (2/6 - 2/10) Dates Topic Videos/Readings Video Slides Class Slides 2/6 KNN Regression and the Bias-Variance Tradeoff KNN Regression and the Bias-Variance Tradeoff R: KNN Regression ISLR: 2.2.2 (bias-variance tradeoff); 3.5 (KNN regression) 2/8 Quiz 1 2/10 Modeling Nonlinearity: Polynomial Regression and Splines Modeling Nonlinearity: Polynomial Regression and Splines R: Nonlinearity: Polynomial Regression and Splines ISLR: 7.1-7.4 Start Homework 2 due Friday, 2/17 at midnight CST Week 4: A Feast with Flexibility (2/13 - 2/17) Dates Topic Videos/Readings Video Slides Class Slides 2/13 Local Regression and Generalized Additive Models Local Regression and Generalized Additive Models R: Local Regression and GAMs ISLR: 7.6-7.7 2/15 Catch-up day: Regression 2/17 Catch-up day / Project work day Finish Homework 2 due Friday, 2/17 at midnight CST Week 5: Getting Classy with Classification Dates Topic Videos/Readings Video Slides Class Slides 2/17 Logistic Regression Logistic Regression R: Logistic Regression ISLR: 4.1 - 4.3 Start Homework 3 due Friday, 3/3 at midnight CST "],["learning-goals.html", "Learning Goals", " Learning Goals Learning goals for our course topics are listed below. Use these to guide your synthesis of video and reading material. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN (not in the video–will be discussed in class) Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Logistic regression Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Evaluating classification models Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Decision trees Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Bagging and random forests Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class) K-means clustering Clearly describe / implement by hand the k-means algorithm Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Hierarchical clustering Clearly describe / implement by hand the hierarchical clustering algorithm Compare and contrast k-means and hierarchical clustering in their outputs and algorithms Interpret cuts of the dendrogram for single and complete linkage Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before class on Monday, January 23, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. As of January 17, 2023, the latest version of R is 4.2.2. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. As of January 17, 2023, the latest version of RStudio is 2022.12.0+353. Suggested: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;tidymodels&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(tidymodels) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(\"rlang\") in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],["r-resources.html", "R Resources Outside resources Example code", " R Resources Outside resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) RStudio cheat sheets ggplot2 reference R Programming Wikibook Debugging in R Article Video Colors in R Data import tutorial Free online textbooks R for Data Science Exploratory Data Analysis with R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "],["introductions.html", "Topic 1 Introductions Explorations", " Topic 1 Introductions Slides from today are available here. Explorations Phase 1 Each of the data contexts below prompts a broad research goal. Pick one context, and brainstorm as many research questions that would be worth investigating. Context 1 (Media and Publishing): The New York Times is trying to better understand the popularity of its different articles with the hope of using this understanding to improve hiring of writers and improve their website layout. Context 2 (Public Health): The Minnesota Department of Health is trying to better understand the different health trajectories of people who have contracted a certain illness to improve funding for relevant health services. Context 3 (Civics and Politics): The campaign management team for a political candidate is trying to better understand voter turnout in different regions of the country to run a better campaign in the next election. Context 4 (Sports): A coach (you pick the sport) wants to use data from games and practices to improve player and team performance. If you have time, repeat the brainstorm for additional contexts. Phase 2 Consider each of your brainstormed questions. Which can be framed as a regression exploration? Classification? Unsupervised learning? Which questions seem to not fit under any of these areas? Are there harms that you anticipate arising from the collection of data or its analysis? Put your responses in this Google Doc. "],["evaluating-regression-models.html", "Topic 2 Evaluating Regression Models Learning Goals Exercises", " Topic 2 Evaluating Regression Models Learning Goals Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way We’ll also start to develop some new ideas relating to our next topics. Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) (If you have not already installed the tidymodels package, install it by running install.packages(\"tidymodels\") in the Console.) library(readr) library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tidymodels) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────── tidymodels 1.0.0 ── ## ✔ broom 1.0.2 ✔ rsample 1.1.1 ## ✔ dials 1.1.0 ✔ tibble 3.1.8 ## ✔ infer 1.0.4 ✔ tidyr 1.2.1 ## ✔ modeldata 1.0.1 ✔ tune 1.0.1 ## ✔ parsnip 1.0.3 ✔ workflows 1.1.2 ## ✔ purrr 1.0.1 ✔ workflowsets 1.0.0 ## ✔ recipes 1.0.4 ✔ yardstick 1.1.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ yardstick::spec() masks readr::spec() ## ✖ recipes::step() masks stats::step() ## • Use tidymodels_prefer() to resolve common conflicts. tidymodels_prefer() bodyfat &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) ## Rows: 80 Columns: 17 ## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (17): fatBrozek, fatSiri, density, age, weight, height, neck, chest, abd... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Class investigations We’ll work through this section together to review concepts and code. You’ll then work on the remainder of the exercises in your groups. # Exploratory plots # Univariate distribution of outcome ggplot(bodyfat, aes(???)) + geom_???() # Scatterplot of fatSiri vs. weight ggplot(bodyfat, aes(???)) + geom_???() Let’s fit a linear regression model using tidymodels to predict body fat percentage from weight. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) mod1 &lt;- fit(lm_spec, ?? ~ ??, data = bodyfat) mod1 %&gt;% tidy() We can use the predict() function to use our fit model to predict the outcome based on values in our original data. We can calculate residuals by taking our true outcome values and subtracting the predicted value, stored as .pred. mod1_output &lt;- mod1 %&gt;% predict(new_data = bodyfat) %&gt;% bind_cols(??) %&gt;% mutate(resid = ?? - ??) head(mod1_output) We can use this data frame to compute error metrics by hand or by using functions from the yardstick package. # MSE - what is the interpretation with units? mod1_output %&gt;% summarize(mean(resid^2)) # RMSE - what is the interpretation with units? mod1_output %&gt;% summarize(sqrt(mean(resid^2))) mod1_output %&gt;% rmse(truth = ??, estimate = .pred) # MAE - what is the interpretation with units? mod1_output %&gt;% summarize(mean(abs(resid))) mod1_output %&gt;% mae(truth = ??, estimate = .pred) # R-squared - interpretation? (unit-less) mod1_output %&gt;% summarize(1 - (var(resid) / var(fatSiri))) mod1_output %&gt;% rsq(truth = ??, estimate = .pred) …and to create residual plots: # Residuals vs. predictions ggplot(mod1_output, aes(x = .pred, y = resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() # Residuals vs. predictors (x&#39;s) ggplot(mod1_output, aes(x = height, y = resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() Exercise 1 First decide on what you think would be a good model by picking variables based on context. Fit this model, calling it mod_initial. (Remember that you can include several predictors with a + in the model formula - like y ~ x1+x2.) # Code to fit initial model Use residual plot explorations to check if you need to update your model. # Residual plot explorations Fit your updated model, and call it model_updated. # Code to fit updated model Exercise 2 Compute and contextually interpret relevant evaluation metrics for your model. # Code to compute evaluation metrics Exercise 3 Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the new_data to use bodyfat_test instead of bodyfat. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Compare your evaluation metrics from Exercise 2 the metrics here. What do you notice? (Note: this observation is just based on your one particular fitted model. You’ll make some more comprehensive observations in the next exercise.) In general, do you think that models with more or fewer variables would perform better in this comparison? Explain. Exercise 4 The code below systematically looks at the same comparison that you made in Exercise 3 but for every possible linear regression model formed from inclusion/exclusion of the predictors (without transformations or interactions). See the plot below for the results. Note: You can try to run the code to make a plot of the results of this systematic investigation (it might take several minutes). Feel free to inspect the code if you’re curious, but otherwise, don’t worry about understanding it fully. What do you notice? What do you wonder? # This helper function computes MAE on the supplied datasets get_maes &lt;- function(mod, train_data, test_data) { mod_output_train &lt;- bind_cols(train_data, predict(mod, new_data = train_data)) mod_output_test &lt;- bind_cols(test_data, predict(mod, new_data = test_data)) train_mae &lt;- mod_output_train %&gt;% mae(truth = fatSiri, estimate = .pred) %&gt;% pull(.estimate) test_mae &lt;- mod_output_test %&gt;% mae(truth = fatSiri, estimate = .pred) %&gt;% pull(.estimate) c(train_mae, test_mae) } # Get just the possible PREDICTOR variables by taking all variables # and removing the fatSiri and hipin variables # hipin is hip circumference in inches (redundant with the hip variable) possible_predictors &lt;- setdiff(colnames(bodyfat), c(&quot;fatSiri&quot;, &quot;hipin&quot;)) # This code loops through all models (run time can be long) results &lt;- bind_rows(lapply(1:13, function(i) { combos &lt;- combn(possible_predictors, i) bind_rows(lapply(seq_len(ncol(combos)), function(j) { form &lt;- paste(&quot;fatSiri ~&quot;, paste(combos[,j], collapse = &quot;+&quot;)) mod &lt;- fit(lm_spec, as.formula(form), data = bodyfat) maes &lt;- get_maes(mod = mod, train_data = bodyfat, test_data = bodyfat_test) tibble( form = form, train_mae = maes[1], test_mae = maes[2], num_predictors = i ) })) })) # I save the results so that I don&#39;t have to rerun the code above save(results, file = &quot;allsubsets.rda&quot;) load(&quot;allsubsets.rda&quot;) # Relabel the categories (levels) of the num_predictors variable results &lt;- results %&gt;% mutate(num_predictors = factor(paste(&quot;# predictors:&quot;, num_predictors), levels = paste(&quot;# predictors:&quot;, 1:13))) # Plot results results %&gt;% ggplot(aes(x = train_mae, y = test_mae, color = num_predictors)) + geom_point() + coord_cartesian(xlim = c(2.5,7.5), ylim = c(2.5,7.5)) + geom_abline(slope = 1, intercept = 0) + facet_wrap(~ num_predictors) + guides(color = &quot;none&quot;) + labs(x = &quot;MAE on original data&quot;, y = &quot;MAE on new data on 172 men&quot;) + theme_classic() "],["overfitting.html", "Topic 3 Overfitting Learning Goals Exercises", " Topic 3 Overfitting Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Slides from today are available here. Exercises Exercise 1 Consider the following relationship. (From the code, we can tell that the true relationship between x and y is quadratic: \\(y=x^2\\) is the underlying relationship.) Imagine a model that is overfit to this training data. (You are not limited to lines.) Draw a picture of the predictions from this overfit model. Explain why your picture demonstrates overfitting: How does your model perform on the training data (the data displayed here)? If we received a new dataset from this same setting (test data), what would a plot of y vs. x look like? How would your model perform on this new test data? set.seed(123) data_train &lt;- tibble::tibble( x = runif(15,0,7), # Generate random numbers for predictor x y = x^2 + rnorm(15,sd = 7) # Generate y using y = x^2 + noise ) data_train %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + theme_classic() Exercise 2 Consider this xkcd comic on “modeling” the outcomes of US presidential elections. How is this comic related to overfitting? Check-in with groupmates What ideas have you found confusing, less clear, or intriguing based on videos and class exercises? Based on this, what would help improve your understanding or help you improve others’ understanding? You’ll reflect on these noticings in the Metacognitive Reflection part of your homework assignments. "],["cross-validation.html", "Topic 4 Cross-validation Learning Goals Exercises", " Topic 4 Cross-validation Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower # of folds in CV in terms of sample size and computing time Implement cross-validation in R using the tidymodels package Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat percentage dataset from before. Our goal is to build a good predictive model of body fat percentage from body circumference measurements. library(readr) library(ggplot2) library(dplyr) library(tidymodels) tidymodels_prefer() bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the extraneous outcome variables (fatBrozek and density) and the # redundant hip circumference variable (hipin = hip circ. in inches) bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Consider 4 models for body fat percentage: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) mod1 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- fit(lm_spec, fatSiri ~ ., # The . means all predictors data = bodyfat_train) Exercise 1: Cross-validation concepts Let’s conceptually step through exactly what our code will be doing to perform 10-fold cross-validation (CV) on the 80 cases in the training set. In your groups, take turns explaining these steps. Exercise 2: Cross-validation with tidymodels The code below performs 10-fold CV for mod1 to estimate the test RMSE (\\(\\text{CV}_{(10)}\\)). Do we need to use set.seed()? Why or why not? (Is there a number of folds for which we would not need to set the seed?) # Do we need to use set.seed()? bodyfat_cv &lt;- vfold_cv(bodyfat_train, v = 10) model_wf &lt;- workflow() %&gt;% add_formula(fatSiri ~ age+weight+neck+abdomen+thigh+forearm) %&gt;% add_model(lm_spec) mod1_cv &lt;- fit_resamples(model_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, mae, rsq) ) Explore mod1_cv %&gt;% unnest(.metrics). What information seems to be contained in this output, and how would you use this to calculate the 10-fold cross-validated RMSE by hand? (If you feel up to it, try using code to perform this calculation–use filter() and summarize().) mod1_cv %&gt;% unnest(.metrics) Check your manual calculation by directly printing out the CV metrics: mod1_cv %&gt;% collect_metrics(). Interpret this metric. mod1_cv %&gt;% collect_metrics() Exercise 3: Looking at the evaluation metrics Look at the completed table below of evaluation metrics for the 4 models. Which model performed the best on the training data? Which model performed best on test set (through CV)? Explain why there’s a discrepancy between these 2 answers and why CV, in general, can help prevent overfitting. Model Training RMSE \\(\\text{CV}_{(10)}\\) mod1 3.810712 4.389568 mod2 3.766645 4.438637 mod3 3.752362 4.517281 mod4 3.572299 4.543343 Exercise 4: Practical issues: choosing the number of folds In terms of sample size, what are the pros/cons of low vs. high number of folds? In terms of computational time, what are the pros/cons of low vs. high number of folds? If possible, it is advisable to choose the number of folds to be a divisor of the sample size. Why do you think that is? Digging deeper If you have time, consider the following questions to further explore concepts related to today’s ideas. Consider leave-one-out-cross-validation (LOOCV). Would we need set.seed()? Why or why not? How might you adapt the code above to implement this? Using the information from your_output %&gt;% unnest(.metrics) (which is a dataset), construct a visualization to examine the variability of RMSE from case to case. What might explain any very large values? What does this highlight about the quality of estimation of the LOOCV process? "],["variable-subset-selection.html", "Topic 5 Variable Subset Selection Learning Goals Exercises", " Topic 5 Variable Subset Selection Learning Goals Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time Describe how selection algorithms can give a measure of variable importance Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. library(readr) library(ggplot2) library(dplyr) library(tidymodels) tidymodels_prefer() bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove unneeded variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Exercise 1: Backward stepwise selection: by hand In the backward stepwise procedure, we start with the full model, full_model, with all predictors: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) full_model &lt;- fit(lm_spec, fatSiri ~ ., data = bodyfat_train) full_model %&gt;% tidy() %&gt;% arrange(desc(p.value)) To practice the backward selection algorithm, step through a few steps of the algorithm using p-values as a selection criterion: Identify which predictor contributes the least to the model. One (problematic) approach is to identify the least significant predictor (the one with the largest p-value). Fit a new model which eliminates this predictor. Identify the least significant predictor in this model. Fit a new model which eliminates this predictor. (Code note: to remove predictor X from the model, update the model formula to fatSiri ~ . - X.) Repeat 1 more time to get the hang of it. (We discussed in the video how the use of p-values for selection is problematic, but for now you’re just getting a handle on the algorithm. You’ll think about the problems with p-values in the next exercise.) Exercise 2: Interpreting the results Examine which predictors remain after the previous exercise. Are you surprised that, for example, wrist is still in the model but hip is not? Does this mean that wrist is a better predictor of body fat percentage than hip is? What statistical idea is relevant here? How would you determine which variables are the most important in predicting the outcome using the backwards algorithm? How about with forward selection? Exercise 3: Planning forward selection using CV Using p-values to perform stepwise selection presents some problems, as was discussed in the concept video. A better alternative to target predictive accuracy is to evaluate the models using cross-validation. Fully outline the steps required to use cross-validation to perform forward selection. Make sure to provide enough detail such that the stepwise selection and CV algorithms are made clear and could be implemented (no code, just describing the steps). Exercise 4: Practical considerations for variable subset selection Forward and backward selection provide computational shortcuts to the all (best) subsets approach. Let’s examine the computation requirements for these methods. Say that we have 5 predictors. How many models would be fit in all/best subsets selection? With forward/backward stepwise selection? Extra: Can we express the number of models that need to be fit for a general number of predictors \\(p\\)? The tidymodels package does not include a straightforward way to implement forward or backward selection because the authors of the package do not believe that it is a good technique for variable selection. (We’ll learn better approaches next.) List some reasons why these algorithms may not be encouraged for selecting variables to include in a model. Consider computational time and a situation where we have hundreds of variables, some of which may be collinear. "],["lasso-shrinkageregularization.html", "Topic 6 LASSO: Shrinkage/Regularization Learning Goals Exercises", " Topic 6 LASSO: Shrinkage/Regularization Learning Goals Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain why variable scaling is important for the performance of shrinkage methods Explain how the lambda tuning parameter affects model performance and how this is related to overfitting Describe how output from LASSO models can give a measure of variable importance Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. We’ll use a new data set to explore LASSO modeling. This data comes from the US Department of Energy. You will predict the fuel efficiency of modern cars from characteristics of these cars, like transmission and engine displacement. Fuel efficiency is a numeric value that ranges smoothly from about 15 to 40 miles per gallon. library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(123) cars2018 &lt;- read_csv(&quot;https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/cars2018.csv&quot;) head(cars2018) # Cleaning cars2018 &lt;- cars2018 %&gt;% select(-model_index) Exercise 1: A least squares model Let’s start by building an ordinary (not penalized) least squares model to review important concepts. We’ll fit a model to predict fuel efficiency measured in miles per gallon (mpg) with all possible predictors. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) full_rec &lt;- recipe(mpg ~ ., data = cars2018) %&gt;% update_role(model, new_role = &quot;ID&quot;) %&gt;% # we want to keep the name of the car model but not as a predictor or outcome step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for LASSO step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables full_lm_wf &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% add_model(lm_spec) full_model &lt;- fit(full_lm_wf, data = cars2018) full_model %&gt;% tidy() Use tidymodels to perform 10-fold cross-validation to estimate test MAE for this model. (The code below comes from our body fat modeling previously. Adapt it for our context here.) # Do we need to use set.seed()? bodyfat_cv &lt;- vfold_cv(bodyfat_train, v = 10) mod1_cv &lt;- fit_resamples(model_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, mae) ) How do you think the estimated test error would change with fewer predictors? This model fit with ordinary least squares corresponds to a special case of penalized least squares. What is the value of \\(\\lambda\\) in this special case? As \\(\\lambda\\) increases, what would you expect to happen to the number of predictors that remain in the model? Notice that our data preprocessing recipe (full_rec) contained a step to normalize all numeric predictors (step_normalize(all_numeric_predictors())). Why is this an important step for LASSO? Do you think that this is an important step for ordinary linear regression? (Hint: think about two models for body weight–one with height in inches as a predictor and one with height in feet as a predictor. Do the predictions from these two models differ?) Exercise 2: Fitting a LASSO model in tidymodels The code below (and in part d) fits a set of LASSO models with the following parameters: Use 10-fold CV. Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of 30 \\(\\lambda\\) values from 0.001 to 10. Before running the code, run install.packages(\"glmnet\") in the Console. set.seed(74) # Create CV folds data_cv10 &lt;- vfold_cv(cars2018, v = 10) # LASSO model specification where then `penalty` parameter needs to be tuned lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = &quot;glmnet&quot;) %&gt;% # note we are using a different engine set_mode(&quot;regression&quot;) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% # recipe defined above add_model(lm_lasso_spec_tune) # Tuning the model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-3, 1)), #log10 transformed levels = 30) tune_output &lt;- tune_grid( # new function for tuning parameters lasso_wf_tune, # workflow resamples = data_cv10, # CV folds metrics = metric_set(rmse, mae), grid = penalty_grid # penalty grid defined above ) Let’s visualize the model evaluation metrics from tuning. We can use autoplot(). # Visualize Model Evaluation Metrics from Tuning autoplot(tune_output) + theme_classic() # Zoomed-in version to focus on curvature of MAE plot autoplot(tune_output) + theme_classic() + coord_cartesian(ylim = c(1.96, 2.01)) Inspect the shape of the plot. The errors go down at the very beginning then start going back up. Based on this, what are the consequences of picking a \\(\\lambda\\) that is too small or too large? (This is an example of a very important idea that we’ll see shortly: the bias-variance tradeoff.) Next, we need to choose the lambda that leads to the best model. We can choose the lambda penalty value that leads to the lowest CV MAE, or we can take into account the variation of the CV MAE and choose the largest lambda penalty value that is within 1 standard error of the lowest CV MAE. How might the models that result from these two penalties differ? best_penalty &lt;- select_best(tune_output, metric = &quot;mae&quot;) # choose penalty value based on lowest CV MAE best_penalty best_se_penalty &lt;- select_by_one_std_err(tune_output, metric = &quot;mae&quot;, desc(penalty)) # choose largest penalty value within 1 se of the lowest CV MAE best_se_penalty Now check your understanding by fitting both “final” models and comparing the coefficients. How are these two models different? # Fit Final Model final_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow final_wf_se &lt;- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow final_fit &lt;- fit(final_wf, data = cars2018) final_fit_se &lt;- fit(final_wf_se, data = cars2018) tidy(final_fit) tidy(final_fit_se) Going forward, we’ll examine output from the model chosen by select_by_one_std_err() (final_fit_se). Exercise 3: Examining output: plot of coefficient paths A useful plot allows us to examine coefficient paths resulting from the final fitted LASSO models: coefficient estimates as a function of \\(\\lambda\\). glmnet_output &lt;- final_fit_se %&gt;% extract_fit_parsnip() %&gt;% pluck(&quot;fit&quot;) # get the original glmnet output # Plot coefficient paths as a function of lambda plot(glmnet_output, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20)) # Codebook for which variables the numbers correspond to rownames(glmnet_output$beta) # e.g., What are variables 2 and 4? rownames(glmnet_output$beta)[c(2,4)] There’s a lot of information in this plot! Each colored line corresponds to a different predictor. (Note that categorical variables have been split into different predictors via indicator variable creation.) The small number to the left of each line indicates a predictor by its position in rownames(glmnet_output$beta). The x-axis reflects the range of different \\(\\lambda\\) values (on the log-scale). At each \\(\\lambda\\), the y-axis reflects the coefficient estimates for the predictors in the corresponding LASSO model. At each \\(\\lambda\\), the numbers at the top of the plot indicate how many predictors remain in the corresponding model. Consider the coefficient estimates at the smallest value of \\(\\lambda\\). How closely should they correspond to the coefficient estimates from ordinary least squares in exercise 1? Why do all of the lines head toward y = 0 on the far right of the plot? What variables seem to be more “important” or “persistent” (persistently present in the model) variable? Does this make sense in context? In general, how might we use these “coefficient paths” to measure the relative importance of our predictors? Note: If you’re curious about code to automate this visual inspection of variable importance, look at the Digging Deeper exercise at the end. Exercise 4: Examining and evaluating the best LASSO model Take a look at the predictors and coefficients for the “best” LASSO model. Are the predictors that remain in the model sensible? Do the coefficient signs make sense? # Obtain the predictors and coefficients of the &quot;best&quot; model # Filter out the coefficient are 0 final_fit_se %&gt;% tidy() %&gt;% filter(estimate != 0) Evaluate the best LASSO model: Contextually interpret (with units) the CV MAE error for the best model. Make residual plots for the model by creating a dataset called lasso_mod_out which contains the original data as well as predicted values and residuals (.pred and resid). # Evaluation metrics tune_output %&gt;% collect_metrics() %&gt;% filter(penalty == (best_se_penalty %&gt;% pull(penalty))) # Residual plots lasso_mod_out &lt;- final_fit_se %&gt;% predict(new_data = cars2018) %&gt;% bind_cols(cars2018) %&gt;% mutate(resid = mpg - .pred) Digging deeper These exercises are recommended for further exploring code useful in an applied analysis. We used the plot of coefficient paths to evaluate the variable importance of our predictors. The code below does this systematically for each predictor so that we don’t have to eyeball. Step through and work out what each part is doing. It may help to look up function documentation with ?function_name in the Console. # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- glmnet_output$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { this_coeff_path &lt;- bool_predictor_exclude[row,] if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{ return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)} }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) If you want more practice, the Hitters data in the ISLR package (be sure to to install and load) contains the salaries and performance measures for 322 Major League Baseball players. Use LASSO to determine the “best” predictive model of Salary. "],["homework-1.html", "Homework 1 Project Work Portfolio Work Metacognitive Reflection", " Homework 1 Due Friday, February 3 at midnight CST on Moodle Please turn in a single PDF document containing (1) your responses for the Project Work and Metacognitive Reflection sections and (2) a LINK to the Google Doc with your responses for the Portfolio Work section. Project Work Goal: Find a dataset to use for your final project, and begin getting acquainted with the data. Details: Your dataset should allow you to perform a supervised learning analysis (regression or classification) and an unsupervised learning analysis (clustering or dimension reduction). The following resources are good places to start looking for data: Google Dataset Search Kaggle UCI Machine Learning Repository Harvard Dataverse Macalester’s librarians are a great resource too! They can help you find data aligning with your interests. You can make an appointment using the Ask Us page on the library website. Even if you end up working in a group on the project (which isn’t required - working alone is fine), please complete this initial work individually. Check in with the instructor early if you need help. Deliverables: Write 1-2 paragraphs (no more than 350 words) summarizing: The information in the dataset and the context behind the data. Use the prompts below to guide your thoughts. (Note: in some situations, there may be incomplete information on the data context. That’s fine. Just do your best to summarize what information is available, and acknowledge the lack of information where relevant.) What are the cases? Broadly describe the variables contained in the data. Who collected the data? When, why, and how? 2 (tentative) research questions 1 that can be investigated in a supervised learning setting (regression OR classification–you’re welcome to do both if you wish!) 1 that can be investigated in an unsupervised learning setting (clustering OR dimension reduction–you’re welcome to do both if you wish!) Note: These research questions might evolve over the course of the semester, and that’s fine! Having some idea of potential research directions now will still be helpful. Also make sure that you can read the data into R. You don’t need to do any analysis in R yet, but making sure that you can read the data will make the next steps go more smoothly. Portfolio Work Over the course of the semester, you will use your understanding of course ideas to build up a written portfolio that you can refer to years down the line. Each homework assignment will have prompts that require you to reflect on Data Ethics as well as concepts related to machine learning methods that are enduring, important, and worth being familiar with. (Refer to our syllabus on Moodle for a reminder of this breakdown.) You will receive comments on your responses from the instructor and preceptors that you can use to evaluate your understanding and to revise your work on subsequent assignments throughout the semester. Logistics: Make a copy of this Google Doc. You will use this SINGLE Google Doc for your homework responses (and revisions) all semester. (A single doc facilitates seeing feedback and revisions over time.) Update the sharing permissions on your doc so that anyone with the link can provide comments. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Areas to address: Data Ethics: Read the article Amazon scraps secret AI recruiting tool that showed bias against women. Write a short (roughly 250 words), thoughtful response about the themes and cautions that the article brings forth. Overfitting: The video used the analogy of a cat picture model to explain overfitting. Come up with your own analogy to explain overfitting. Evaluating regression models: Describe how residuals are central to the evaluation of regression models. Explain how they arise in quantitative evaluation metrics and how they are used in evaluation plots. Include examples of plots that show desirable and undesirable model behavior (feel free to draw them by hand if you wish) and what steps can be taken to address that undesirable behavior. Cross-validation: In your own words, explain the rationale for cross-validation in relation to overfitting and model evaluation. Describe the algorithm in your own words in at most 2 sentences. Subset selection: Algorithmic understanding: Look at Conceptual exercise 1, parts (a) and (b) in ISLR Section 6.6. What are the aspects of the subset selection algorithm(s) that are essential to answering these questions, and why? (Note: you’ll have to try to answer the ISLR questions to respond to this prompt, but the focus of your writing should be on the question in bold here.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? LASSO: Algorithmic understanding: Come up with your own analogy for explaining how the penalized least squares criterion works. Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? Metacognitive Reflection How do you feel about your level of understanding for the topics that we have covered so far? What were some notable themes in self- and peer-noticings? (What has been confusing and/or intriguing based on pre-class videos, in-class activities, and this homework assignment?) How do these reflections inform your next steps for developing stronger understanding and/or advising future students learning about these topics? "],["final-project.html", "Final Project Requirements", " Final Project Requirements You will be analyzing a dataset using both supervised and unsupervised learning tools. Collaboration: You may work in teams of up to 3 members. Individual work is fine. The homework assignments will indicate whether work for that assignment should be submitted individually or if just one team member should submit work. There will be a required synthesis of the weekly homework investigations at the end of the course. If working on a team, this should be done in groups, rather than individually. Final deliverables: Each group will be required to give a practice presentation and a final presentation. The instructor will provide feedback on the practice presentation to incorporate into the final presentation. (See our syllabus on Moodle for details.) Requirements for what to cover in the presentation will be updated here later in the semester. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

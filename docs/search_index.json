[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source: Flaticon This is the class manual for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2021. The content here was made by Leslie Myint and draws upon our class textbook, An Introduction to Statistical Learning with Applications in R, and on materials prepared by Alicia Johnson and Brianna Heggeseth. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule.html", "Schedule", " Schedule The schedule below is a tentative outline of our plans for the semester. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Goals. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Remember to take notes on where you paused/rewound/reread or smiled/nodded during the videos/readings. This is essential for the Metacognitive Reflection part of the course. Week 1: Intros and Evangelizing Evaluation (1/20 - 1/27) Dates Topic Videos/Readings Video Slides Class Slides 1/20 Introductions ISLR: Ch 1, Ch 2--Section 2.1(Skip 2.1.2, 2.1.3 for now.) 1/23 Evaluating Regression Models Evaluating Regression Models R: Introduction to TidyModels ISLR: 2.2 1/25 Overfitting Overfitting R: Preprocessing and Recipes ISLR: 5.1 1/27 Cross-validation Cross-validation R: Training, Testing and Cross-Validation ISLR: 5.1 N/A Start Homework 1 due Friday, 2/3 at midnight CST Week 2: A Soirée with Selection Strategies (1/30 - 2/3) Dates Topic Videos/Readings Video Slides Class Slides 1/30 Subset Selection Variable Subset Selection R: Subset Selection ISLR: 6.1 2/1 LASSO (Shrinkage/Regularization) LASSO (Shrinkage/Regularization) R: LASSO and Regularization ISLR: 6.2 2/3 LASSO (continued) Finish Homework 1 due Friday, 2/3 at midnight CST Week 3: A Treasure in Tradeoffs (2/6 - 2/10) Dates Topic Videos/Readings Video Slides Class Slides 2/6 KNN Regression and the Bias-Variance Tradeoff KNN Regression and the Bias-Variance Tradeoff R: KNN Regression ISLR: 2.2.2 (bias-variance tradeoff); 3.5 (KNN regression) 2/8 Quiz 1 (Covers Topics 1 to 6. You may bring a 3x5 notecard with you.) 2/10 Catch-up day: We'll practice deepening our understanding of the bias-variance tradeoff and work towards building a tidymodels reference sheet. Start Homework 2 due Friday, 2/17 at midnight CST Week 4: A Feast with Flexibility (2/13 - 2/17) Dates Topic Videos/Readings Video Slides Class Slides 2/13 Modeling Nonlinearity: Polynomial Regression and Splines Modeling Nonlinearity: Polynomial Regression and Splines R: Nonlinearity: Polynomial Regression and Splines ISLR: 7.1-7.4 2/15 Review of Quiz 1 2/17 Local Regression and Generalized Additive Models Local Regression and Generalized Additive Models R: Local Regression and GAMs ISLR: 7.6-7.7 Finish Homework 2 due Friday, 2/17 at midnight CST Week 5: Regression Wrap-Up and Commencing Classification (2/20 - 2/24) Dates Topic Videos/Readings Video Slides Class Slides 2/20 Review and synthesis of our regression unit 2/22 Quiz 2 (Covers bias-variance tradeoff, KNN, local regression, GAMs. You may bring a 3x5 notecard with you.) 2/24 Logistic Regression Logistic Regression R: Logistic Regression ISLR: 4.1 - 4.3 Start Homework 3 due Friday, 3/3 at midnight CST (Portfolio + Reflection) and Friday 3/10 at midnight (Project Work) Week 6: Staying Classy with Classification (2/27 - 3/3) Dates Topic Videos/Readings Video Slides Class Slides 2/27 Evaluating Classification Models & Revisiting LASSO+KNN Evaluating Classification Models R: Evaluating Classification 3/1 Evaluating Classification Models & Revisiting LASSO+KNN (continued) 3/3 Capstone Days! (no class - attend a talk instead!) Finish Homework 3 due Friday, 3/3 at midnight CST (Portfolio + Reflection) and Friday 3/10 at midnight (Project Work) Week 7: Learning Conferences (3/6 - 3/10) No class this week to make space for Learning Conferences. Schedule a conference with the instructor this week using the Calendly link under the Moodle course calendar. "],["learning-goals.html", "Learning Goals", " Learning Goals Learning goals for our course topics are listed below. Use these to guide your synthesis of video and reading material. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN (not in the video–will be discussed in class) Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Logistic regression Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Evaluating classification models Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Decision trees Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Bagging and random forests Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class) K-means clustering Clearly describe / implement by hand the k-means algorithm Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Hierarchical clustering Clearly describe / implement by hand the hierarchical clustering algorithm Compare and contrast k-means and hierarchical clustering in their outputs and algorithms Interpret cuts of the dendrogram for single and complete linkage Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before class on Monday, January 23, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. As of January 17, 2023, the latest version of R is 4.2.2. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. As of January 17, 2023, the latest version of RStudio is 2022.12.0+353. Suggested: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;tidymodels&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(tidymodels) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(\"rlang\") in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],["r-resources.html", "R Resources Outside resources Example code", " R Resources Outside resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) RStudio cheat sheets ggplot2 reference R Programming Wikibook Debugging in R Article Video Colors in R Data import tutorial Free online textbooks R for Data Science Exploratory Data Analysis with R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "],["introductions.html", "Topic 1 Introductions Explorations", " Topic 1 Introductions Slides from today are available here. Explorations Phase 1 Each of the data contexts below prompts a broad research goal. Pick one context, and brainstorm as many research questions that would be worth investigating. Context 1 (Media and Publishing): The New York Times is trying to better understand the popularity of its different articles with the hope of using this understanding to improve hiring of writers and improve their website layout. Context 2 (Public Health): The Minnesota Department of Health is trying to better understand the different health trajectories of people who have contracted a certain illness to improve funding for relevant health services. Context 3 (Civics and Politics): The campaign management team for a political candidate is trying to better understand voter turnout in different regions of the country to run a better campaign in the next election. Context 4 (Sports): A coach (you pick the sport) wants to use data from games and practices to improve player and team performance. If you have time, repeat the brainstorm for additional contexts. Phase 2 Consider each of your brainstormed questions. Which can be framed as a regression exploration? Classification? Unsupervised learning? Which questions seem to not fit under any of these areas? Are there harms that you anticipate arising from the collection of data or its analysis? Put your responses in this Google Doc. "],["evaluating-regression-models.html", "Topic 2 Evaluating Regression Models Learning Goals Exercises", " Topic 2 Evaluating Regression Models Learning Goals Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way We’ll also start to develop some new ideas relating to our next topics. Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) (If you have not already installed the tidymodels package, install it by running install.packages(\"tidymodels\") in the Console.) library(readr) library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tidymodels) ## ── Attaching packages ────────────────────────────────────────────────────────────── tidymodels 1.0.0 ── ## ✔ broom 1.0.3 ✔ rsample 1.1.1 ## ✔ dials 1.1.0 ✔ tibble 3.1.8 ## ✔ infer 1.0.4 ✔ tidyr 1.3.0 ## ✔ modeldata 1.0.1 ✔ tune 1.0.1 ## ✔ parsnip 1.0.3 ✔ workflows 1.1.2 ## ✔ purrr 1.0.1 ✔ workflowsets 1.0.0 ## ✔ recipes 1.0.4 ✔ yardstick 1.1.0 ## ── Conflicts ───────────────────────────────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ yardstick::spec() masks readr::spec() ## ✖ recipes::step() masks stats::step() ## • Dig deeper into tidy modeling with R at https://www.tmwr.org tidymodels_prefer() bodyfat &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) ## Rows: 80 Columns: 17 ## ── Column specification ──────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (17): fatBrozek, fatSiri, density, age, weight, height, neck, chest, abd... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Class investigations We’ll work through this section together to review concepts and code. You’ll then work on the remainder of the exercises in your groups. # Exploratory plots # Univariate distribution of outcome ggplot(bodyfat, aes(???)) + geom_???() # Scatterplot of fatSiri vs. weight ggplot(bodyfat, aes(???)) + geom_???() Let’s fit a linear regression model using tidymodels to predict body fat percentage from weight. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) mod1 &lt;- fit(lm_spec, ?? ~ ??, data = bodyfat) mod1 %&gt;% tidy() We can use the predict() function to use our fit model to predict the outcome based on values in our original data. We can calculate residuals by taking our true outcome values and subtracting the predicted value, stored as .pred. mod1_output &lt;- mod1 %&gt;% predict(new_data = bodyfat) %&gt;% bind_cols(??) %&gt;% mutate(resid = ?? - ??) head(mod1_output) We can use this data frame to compute error metrics by hand or by using functions from the yardstick package. # MSE - what is the interpretation with units? mod1_output %&gt;% summarize(mean(resid^2)) # RMSE - what is the interpretation with units? mod1_output %&gt;% summarize(sqrt(mean(resid^2))) mod1_output %&gt;% rmse(truth = ??, estimate = .pred) # MAE - what is the interpretation with units? mod1_output %&gt;% summarize(mean(abs(resid))) mod1_output %&gt;% mae(truth = ??, estimate = .pred) # R-squared - interpretation? (unit-less) mod1_output %&gt;% summarize(1 - (var(resid) / var(fatSiri))) mod1_output %&gt;% rsq(truth = ??, estimate = .pred) …and to create residual plots: # Residuals vs. predictions ggplot(mod1_output, aes(x = .pred, y = resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() # Residuals vs. predictors (x&#39;s) ggplot(mod1_output, aes(x = height, y = resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() Exercise 1 First decide on what you think would be a good model by picking variables based on context. Fit this model, calling it mod_initial. (Remember that you can include several predictors with a + in the model formula - like y ~ x1+x2.) # Code to fit initial model Use residual plot explorations to check if you need to update your model. # Residual plot explorations Fit your updated model, and call it model_updated. # Code to fit updated model Exercise 2 Compute and contextually interpret relevant evaluation metrics for your model. # Code to compute evaluation metrics Exercise 3 Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the new_data to use bodyfat_test instead of bodyfat. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Compare your evaluation metrics from Exercise 2 the metrics here. What do you notice? (Note: this observation is just based on your one particular fitted model. You’ll make some more comprehensive observations in the next exercise.) In general, do you think that models with more or fewer variables would perform better in this comparison? Explain. Exercise 4 The code below systematically looks at the same comparison that you made in Exercise 3 but for every possible linear regression model formed from inclusion/exclusion of the predictors (without transformations or interactions). See the plot below for the results. Note: You can try to run the code to make a plot of the results of this systematic investigation (it might take several minutes). Feel free to inspect the code if you’re curious, but otherwise, don’t worry about understanding it fully. What do you notice? What do you wonder? # This helper function computes MAE on the supplied datasets get_maes &lt;- function(mod, train_data, test_data) { mod_output_train &lt;- bind_cols(train_data, predict(mod, new_data = train_data)) mod_output_test &lt;- bind_cols(test_data, predict(mod, new_data = test_data)) train_mae &lt;- mod_output_train %&gt;% mae(truth = fatSiri, estimate = .pred) %&gt;% pull(.estimate) test_mae &lt;- mod_output_test %&gt;% mae(truth = fatSiri, estimate = .pred) %&gt;% pull(.estimate) c(train_mae, test_mae) } # Get just the possible PREDICTOR variables by taking all variables # and removing the fatSiri and hipin variables # hipin is hip circumference in inches (redundant with the hip variable) possible_predictors &lt;- setdiff(colnames(bodyfat), c(&quot;fatSiri&quot;, &quot;hipin&quot;)) # This code loops through all models (run time can be long) results &lt;- bind_rows(lapply(1:13, function(i) { combos &lt;- combn(possible_predictors, i) bind_rows(lapply(seq_len(ncol(combos)), function(j) { form &lt;- paste(&quot;fatSiri ~&quot;, paste(combos[,j], collapse = &quot;+&quot;)) mod &lt;- fit(lm_spec, as.formula(form), data = bodyfat) maes &lt;- get_maes(mod = mod, train_data = bodyfat, test_data = bodyfat_test) tibble( form = form, train_mae = maes[1], test_mae = maes[2], num_predictors = i ) })) })) # I save the results so that I don&#39;t have to rerun the code above save(results, file = &quot;allsubsets.rda&quot;) load(&quot;allsubsets.rda&quot;) # Relabel the categories (levels) of the num_predictors variable results &lt;- results %&gt;% mutate(num_predictors = factor(paste(&quot;# predictors:&quot;, num_predictors), levels = paste(&quot;# predictors:&quot;, 1:13))) # Plot results results %&gt;% ggplot(aes(x = train_mae, y = test_mae, color = num_predictors)) + geom_point() + coord_cartesian(xlim = c(2.5,7.5), ylim = c(2.5,7.5)) + geom_abline(slope = 1, intercept = 0) + facet_wrap(~ num_predictors) + guides(color = &quot;none&quot;) + labs(x = &quot;MAE on original data&quot;, y = &quot;MAE on new data on 172 men&quot;) + theme_classic() "],["overfitting.html", "Topic 3 Overfitting Learning Goals Exercises", " Topic 3 Overfitting Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Slides from today are available here. Exercises Exercise 1 Consider the following relationship. (From the code, we can tell that the true relationship between x and y is quadratic: \\(y=x^2\\) is the underlying relationship.) Imagine a model that is overfit to this training data. (You are not limited to lines.) Draw a picture of the predictions from this overfit model. Explain why your picture demonstrates overfitting: How does your model perform on the training data (the data displayed here)? If we received a new dataset from this same setting (test data), what would a plot of y vs. x look like? How would your model perform on this new test data? set.seed(123) data_train &lt;- tibble::tibble( x = runif(15,0,7), # Generate random numbers for predictor x y = x^2 + rnorm(15,sd = 7) # Generate y using y = x^2 + noise ) data_train %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + theme_classic() Exercise 2 Consider this xkcd comic on “modeling” the outcomes of US presidential elections. How is this comic related to overfitting? Check-in with groupmates What ideas have you found confusing, less clear, or intriguing based on videos and class exercises? Based on this, what would help improve your understanding or help you improve others’ understanding? You’ll reflect on these noticings in the Metacognitive Reflection part of your homework assignments. "],["cross-validation.html", "Topic 4 Cross-validation Learning Goals Exercises", " Topic 4 Cross-validation Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower # of folds in CV in terms of sample size and computing time Implement cross-validation in R using the tidymodels package Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat percentage dataset from before. Our goal is to build a good predictive model of body fat percentage from body circumference measurements. library(readr) library(ggplot2) library(dplyr) library(tidymodels) tidymodels_prefer() bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the extraneous outcome variables (fatBrozek and density) and the # redundant hip circumference variable (hipin = hip circ. in inches) bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Consider 4 models for body fat percentage: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) mod1 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- fit(lm_spec, fatSiri ~ ., # The . means all predictors data = bodyfat_train) Exercise 1: Cross-validation concepts Let’s conceptually step through exactly what our code will be doing to perform 10-fold cross-validation (CV) on the 80 cases in the training set. In your groups, take turns explaining these steps. Exercise 2: Cross-validation with tidymodels The code below performs 10-fold CV for mod1 to estimate the test RMSE (\\(\\text{CV}_{(10)}\\)). Do we need to use set.seed()? Why or why not? (Is there a number of folds for which we would not need to set the seed?) # Do we need to use set.seed()? bodyfat_cv &lt;- vfold_cv(bodyfat_train, v = 10) model_wf &lt;- workflow() %&gt;% add_formula(fatSiri ~ age+weight+neck+abdomen+thigh+forearm) %&gt;% add_model(lm_spec) mod1_cv &lt;- fit_resamples(model_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, mae, rsq) ) Explore mod1_cv %&gt;% unnest(.metrics). What information seems to be contained in this output, and how would you use this to calculate the 10-fold cross-validated RMSE by hand? (If you feel up to it, try using code to perform this calculation–use filter() and summarize().) mod1_cv %&gt;% unnest(.metrics) Check your manual calculation by directly printing out the CV metrics: mod1_cv %&gt;% collect_metrics(). Interpret this metric. mod1_cv %&gt;% collect_metrics() Exercise 3: Looking at the evaluation metrics Look at the completed table below of evaluation metrics for the 4 models. Which model performed the best on the training data? Which model performed best on test set (through CV)? Explain why there’s a discrepancy between these 2 answers and why CV, in general, can help prevent overfitting. Model Training RMSE \\(\\text{CV}_{(10)}\\) mod1 3.810712 4.389568 mod2 3.766645 4.438637 mod3 3.752362 4.517281 mod4 3.572299 4.543343 Exercise 4: Practical issues: choosing the number of folds In terms of sample size, what are the pros/cons of low vs. high number of folds? In terms of computational time, what are the pros/cons of low vs. high number of folds? If possible, it is advisable to choose the number of folds to be a divisor of the sample size. Why do you think that is? Digging deeper If you have time, consider the following questions to further explore concepts related to today’s ideas. Consider leave-one-out-cross-validation (LOOCV). Would we need set.seed()? Why or why not? How might you adapt the code above to implement this? Using the information from your_output %&gt;% unnest(.metrics) (which is a dataset), construct a visualization to examine the variability of RMSE from case to case. What might explain any very large values? What does this highlight about the quality of estimation of the LOOCV process? "],["variable-subset-selection.html", "Topic 5 Variable Subset Selection Learning Goals Exercises", " Topic 5 Variable Subset Selection Learning Goals Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time Describe how selection algorithms can give a measure of variable importance Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. library(readr) library(ggplot2) library(dplyr) library(tidymodels) tidymodels_prefer() bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove unneeded variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Exercise 1: Backward stepwise selection: by hand In the backward stepwise procedure, we start with the full model, full_model, with all predictors: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) full_model &lt;- fit(lm_spec, fatSiri ~ ., data = bodyfat_train) full_model %&gt;% tidy() %&gt;% arrange(desc(p.value)) To practice the backward selection algorithm, step through a few steps of the algorithm using p-values as a selection criterion: Identify which predictor contributes the least to the model. One (problematic) approach is to identify the least significant predictor (the one with the largest p-value). Fit a new model which eliminates this predictor. Identify the least significant predictor in this model. Fit a new model which eliminates this predictor. (Code note: to remove predictor X from the model, update the model formula to fatSiri ~ . - X.) Repeat 1 more time to get the hang of it. (We discussed in the video how the use of p-values for selection is problematic, but for now you’re just getting a handle on the algorithm. You’ll think about the problems with p-values in the next exercise.) Exercise 2: Interpreting the results Examine which predictors remain after the previous exercise. Are you surprised that, for example, wrist is still in the model but hip is not? Does this mean that wrist is a better predictor of body fat percentage than hip is? What statistical idea is relevant here? How would you determine which variables are the most important in predicting the outcome using the backwards algorithm? How about with forward selection? Exercise 3: Planning forward selection using CV Using p-values to perform stepwise selection presents some problems, as was discussed in the concept video. A better alternative to target predictive accuracy is to evaluate the models using cross-validation. Fully outline the steps required to use cross-validation to perform forward selection. Make sure to provide enough detail such that the stepwise selection and CV algorithms are made clear and could be implemented (no code, just describing the steps). Exercise 4: Practical considerations for variable subset selection Forward and backward selection provide computational shortcuts to the all (best) subsets approach. Let’s examine the computation requirements for these methods. Say that we have 5 predictors. How many models would be fit in all/best subsets selection? With forward/backward stepwise selection? Extra: Can we express the number of models that need to be fit for a general number of predictors \\(p\\)? The tidymodels package does not include a straightforward way to implement forward or backward selection because the authors of the package do not believe that it is a good technique for variable selection. (We’ll learn better approaches next.) List some reasons why these algorithms may not be encouraged for selecting variables to include in a model. Consider computational time and a situation where we have hundreds of variables, some of which may be collinear. "],["lasso-shrinkageregularization.html", "Topic 6 LASSO: Shrinkage/Regularization Learning Goals LASSO models in tidymodels Exercises", " Topic 6 LASSO: Shrinkage/Regularization Learning Goals Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain why variable scaling is important for the performance of shrinkage methods Explain how the lambda tuning parameter affects model performance and how this is related to overfitting Describe how output from LASSO models can give a measure of variable importance Slides from today are available here. LASSO models in tidymodels To build LASSO models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code to fit a LASSO model to a data set using several lambda values: # Lasso Model Spec lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% # mixture = 1 indicates LASSO set_engine(engine = &quot;glmnet&quot;) %&gt;% # note different engine than &quot;lm&quot; set_mode(&quot;regression&quot;) # Recipe with standardization (!) data_rec &lt;- recipe(___ ~ ___ , data = ___) %&gt;% # formula: outcome ~ predictors (generally predictors is . for all predictors) step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_novel(all_nominal_predictors()) %&gt;% # important if you have rare categorical variables step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for LASSO step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(data_rec) %&gt;% add_model(lm_lasso_spec_tune) # Create CV folds data_cv10 &lt;- vfold_cv(___, v = 10) # Tune Model (trying a variety of values of lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-5, 3)), # log10 transformed 10^-5 to 10^3 levels = 30 ) lasso_tune_out &lt;- tune_grid( # new function for tuning parameters lasso_wf_tune, # workflow resamples = data_cv10, # cv folds metrics = metric_set(rmse, mae), grid = penalty_grid # penalty grid defined above ) Examining the LASSO model for each \\(\\lambda\\) The glmnet engine fits models for each \\(\\lambda\\) automatically, so we can visualize the estimates for each penalty value. plot(lasso_fit %&gt;% extract_fit_parsnip() %&gt;% pluck(&#39;fit&#39;), # way to get the original glmnet output xvar = &quot;lambda&quot;) # glmnet fits the model with a variety of lambda penalty values Identifying the “best” LASSO model To identify the best model, we need to tune the model using cross validation. Adapt the following code to tune a Lasso Model to choose lambda: # Visualize model evaluation metrics from tuning autoplot(lasso_tune_out) + theme_classic() # Summarize model evaluation metrics collect_metrics(lasso_tune_out) %&gt;% filter(.metric == &quot;mae&quot;) %&gt;% # or choose rmse select(penalty, rmse = mean) # Choose penalty value based on lowest mae (or choose rmse) best_penalty &lt;- select_best(lasso_tune_out, metric = &quot;mae&quot;) # Choose largest penalty value within 1 SE of the lowest CV MAE best_se_penalty &lt;- select_by_one_std_err(lasso_tune_out, metric = &quot;mae&quot;, desc(penalty)) # Fit final model final_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow (or replace best_penalty with best_se_penalty) final_fit &lt;- fit(final_wf, data = ___) tidy(final_fit) Exercises You can download a template RMarkdown file to start from here. We’ll use a new data set to explore LASSO modeling. This data comes from the US Department of Energy. You will predict the fuel efficiency of modern cars from characteristics of these cars, like transmission and engine displacement. Fuel efficiency is a numeric value that ranges smoothly from about 15 to 40 miles per gallon. library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(123) cars2018 &lt;- read_csv(&quot;https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/cars2018.csv&quot;) head(cars2018) # Cleaning cars2018 &lt;- cars2018 %&gt;% select(-model_index) Exercise 1: A least squares model Let’s start by building an ordinary (not penalized) least squares model to review important concepts. We’ll fit a model to predict fuel efficiency measured in miles per gallon (mpg) with all possible predictors. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) full_rec &lt;- recipe(mpg ~ ., data = cars2018) %&gt;% update_role(model, new_role = &quot;ID&quot;) %&gt;% # we want to keep the name of the car model but not as a predictor or outcome step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for LASSO step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables full_lm_wf &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% add_model(lm_spec) full_model &lt;- fit(full_lm_wf, data = cars2018) full_model %&gt;% tidy() Use tidymodels to perform 10-fold cross-validation to estimate test MAE for this model. (The code below comes from our body fat modeling previously. Adapt it for our context here.) # Do we need to use set.seed()? bodyfat_cv &lt;- vfold_cv(bodyfat_train, v = 10) mod1_cv &lt;- fit_resamples(model_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, mae) ) How do you think the estimated test error would change with fewer predictors? This model fit with ordinary least squares corresponds to a special case of penalized least squares. What is the value of \\(\\lambda\\) in this special case? As \\(\\lambda\\) increases, what would you expect to happen to the number of predictors that remain in the model? Notice that our data preprocessing recipe (full_rec) contained a step to normalize all numeric predictors (step_normalize(all_numeric_predictors())). Why is this an important step for LASSO? Do you think that this is an important step for ordinary linear regression? (Hint: think about two models for body weight–one with height in inches as a predictor and one with height in feet as a predictor. Do the predictions from these two models differ?) Exercise 2: Fitting a LASSO model in tidymodels The code below (and in part d) fits a set of LASSO models with the following parameters: Use 10-fold CV. Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of 30 \\(\\lambda\\) values from 0.001 to 10. Before running the code, run install.packages(\"glmnet\") in the Console. set.seed(74) # Create CV folds data_cv10 &lt;- vfold_cv(cars2018, v = 10) # LASSO model specification where then `penalty` parameter needs to be tuned lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = &quot;glmnet&quot;) %&gt;% # note we are using a different engine set_mode(&quot;regression&quot;) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% # recipe defined above add_model(lm_lasso_spec_tune) # Tuning the model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-3, 1)), #log10 transformed levels = 30) tune_output &lt;- tune_grid( # new function for tuning parameters lasso_wf_tune, # workflow resamples = data_cv10, # CV folds metrics = metric_set(rmse, mae), grid = penalty_grid # penalty grid defined above ) Let’s visualize the model evaluation metrics from tuning. We can use autoplot(). # Visualize Model Evaluation Metrics from Tuning autoplot(tune_output) + theme_classic() # Zoomed-in version to focus on curvature of MAE plot autoplot(tune_output) + theme_classic() + coord_cartesian(ylim = c(1.96, 2.01)) Inspect the shape of the plot. The errors go down at the very beginning then start going back up. Based on this, what are the consequences of picking a \\(\\lambda\\) that is too small or too large? (This is an example of a very important idea that we’ll see shortly: the bias-variance tradeoff.) Next, we need to choose the lambda that leads to the best model. We can choose the lambda penalty value that leads to the lowest CV MAE, or we can take into account the variation of the CV MAE and choose the largest lambda penalty value that is within 1 standard error of the lowest CV MAE. How might the models that result from these two penalties differ? best_penalty &lt;- select_best(tune_output, metric = &quot;mae&quot;) # choose penalty value based on lowest CV MAE best_penalty best_se_penalty &lt;- select_by_one_std_err(tune_output, metric = &quot;mae&quot;, desc(penalty)) # choose largest penalty value within 1 se of the lowest CV MAE best_se_penalty Now check your understanding by fitting both “final” models and comparing the coefficients. How are these two models different? # Fit Final Model final_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow final_wf_se &lt;- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow final_fit &lt;- fit(final_wf, data = cars2018) final_fit_se &lt;- fit(final_wf_se, data = cars2018) tidy(final_fit) tidy(final_fit_se) Going forward, we’ll examine output from the model chosen by select_by_one_std_err() (final_fit_se). Exercise 3: Examining output: plot of coefficient paths A useful plot allows us to examine coefficient paths resulting from the final fitted LASSO models: coefficient estimates as a function of \\(\\lambda\\). glmnet_output &lt;- final_fit_se %&gt;% extract_fit_parsnip() %&gt;% pluck(&quot;fit&quot;) # get the original glmnet output # Plot coefficient paths as a function of lambda plot(glmnet_output, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20)) # Codebook for which variables the numbers correspond to rownames(glmnet_output$beta) # e.g., What are variables 2 and 4? rownames(glmnet_output$beta)[c(2,4)] There’s a lot of information in this plot! Each colored line corresponds to a different predictor. (Note that categorical variables have been split into different predictors via indicator variable creation.) The small number to the left of each line indicates a predictor by its position in rownames(glmnet_output$beta). The x-axis reflects the range of different \\(\\lambda\\) values (on the log-scale). At each \\(\\lambda\\), the y-axis reflects the coefficient estimates for the predictors in the corresponding LASSO model. At each \\(\\lambda\\), the numbers at the top of the plot indicate how many predictors remain in the corresponding model. Consider the coefficient estimates at the smallest value of \\(\\lambda\\). How closely should they correspond to the coefficient estimates from ordinary least squares in exercise 1? Why do all of the lines head toward y = 0 on the far right of the plot? What variables seem to be more “important” or “persistent” (persistently present in the model) variable? Does this make sense in context? In general, how might we use these “coefficient paths” to measure the relative importance of our predictors? Note: If you’re curious about code to automate this visual inspection of variable importance, look at the Digging Deeper exercise at the end. Exercise 4: Examining and evaluating the best LASSO model Take a look at the predictors and coefficients for the “best” LASSO model. Are the predictors that remain in the model sensible? Do the coefficient signs make sense? # Obtain the predictors and coefficients of the &quot;best&quot; model # Filter out the coefficient are 0 final_fit_se %&gt;% tidy() %&gt;% filter(estimate != 0) Evaluate the best LASSO model: Contextually interpret (with units) the CV MAE error for the best model. Make residual plots for the model by creating a dataset called lasso_mod_out which contains the original data as well as predicted values and residuals (.pred and resid). # Evaluation metrics tune_output %&gt;% collect_metrics() %&gt;% filter(penalty == (best_se_penalty %&gt;% pull(penalty))) # Residual plots lasso_mod_out &lt;- final_fit_se %&gt;% predict(new_data = cars2018) %&gt;% bind_cols(cars2018) %&gt;% mutate(resid = mpg - .pred) Digging deeper These exercises are recommended for further exploring code useful in an applied analysis. We used the plot of coefficient paths to evaluate the variable importance of our predictors. The code below does this systematically for each predictor so that we don’t have to eyeball. Step through and work out what each part is doing. It may help to look up function documentation with ?function_name in the Console. # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- glmnet_output$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { this_coeff_path &lt;- bool_predictor_exclude[row,] if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{ return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)} }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) If you want more practice, the Hitters data in the ISLR package (be sure to to install and load) contains the salaries and performance measures for 322 Major League Baseball players. Use LASSO to determine the “best” predictive model of Salary. "],["knn-regression-and-the-bias-variance-tradeoff.html", "Topic 7 KNN Regression and the Bias-Variance Tradeoff Learning Goals KNN models in tidymodels Exercises", " Topic 7 KNN Regression and the Bias-Variance Tradeoff Learning Goals Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN Slides from today are available here. KNN models in tidymodels To build KNN models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: # CV Folds data_cv10 &lt;- vfold_cv(___, v = 10) # Model Specification knn_spec &lt;- nearest_neighbor() %&gt;% # new type of model! set_args(neighbors = tune()) %&gt;% # tuning parameter is neighbor; tuning spec set_engine(engine = &quot;kknn&quot;) %&gt;% # new engine set_mode(&quot;regression&quot;) # Recipe with standardization (!) data_rec &lt;- recipe( ___ ~ ___ , data = ___) %&gt;% step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_novel(all_nominal_predictors()) %&gt;% # important if you have rare categorical variables step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for KNN step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables (important for KNN!) # Workflow (Recipe + Model) knn_wf &lt;- workflow() %&gt;% add_model(knn_spec) %&gt;% add_recipe(data_rec) # Tune model trying a variety of values for neighbors (using 10-fold CV) neighbors_grid &lt;- grid_regular( neighbors(range = c(1, 50)), # min and max of values for neighbors levels = 50 # number of neighbors values ) knn_fit_cv &lt;- tune_grid( knn_wf, # workflow resamples = data_cv10, # CV folds grid = neighbors_grid, # grid specified above metrics = metric_set(rmse, mae) ) Note: tidymodels defines neighbors as the cases that are the closest in terms of the Euclidean distance of the predictor values: \\[ d(case_i,case_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\cdots +(x_{ip} - x_{jp})^2 } \\] Identifying the “best” KNN model The “best” model in the sequence of models fit is defined relative to the chosen metric and the choice of select_best() or select_by_one_std_err(). knn_fit_cv %&gt;% autoplot() # Visualize Trained Model using CV knn_fit_cv %&gt;% show_best(metric = &quot;mae&quot;) # Show evaluation metrics for different values of neighbors, ordered # Choose value of Tuning Parameter (neighbors) tuned_knn_wf &lt;- knn_fit_cv %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(neighbors)) %&gt;% # Choose neighbors value that leads to the highest neighbors within 1 se of the lowest CV MAE finalize_workflow(knn_wf, .) # Fit final KNN model to data knn_fit_final &lt;- tuned_knn_wf %&gt;% fit(data = ___) # Use the best model to make predictions # new_data should be a data.frame with required predictors predict(knn_fit_final, new_data = ___) Exercises You can download a template RMarkdown file to start from here. We’ll explore KNN regression using the College dataset in the ISLR2 package (install it with install.packages(\"ISLR2\") in the Console). You can use ?College in the Console to look at the data codebook. library(ISLR2) library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions data(College) # Data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% # creates variable with school name filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Bias-variance tradeoff warmup Think back to the LASSO algorithm which depends upon tuning parameter \\(\\lambda\\). For which values of \\(\\lambda\\) (small or large) will LASSO be the most biased, and why? For which values of \\(\\lambda\\) (small or large) will LASSO be the most variable, and why? The bias-variance tradeoff also comes into play when comparing across algorithms, not just within algorithms. Consider LASSO vs. least squares: Which will tend to be more biased? Which will tend to be more variable? When will LASSO outperform least squares in the bias-variance tradeoff? Exercise 2: Impact of variable scale and distance measure Consider the 1-nearest neighbor algorithm to predict Grad.Rate on the basis of two predictors: Apps and Private. Let Yes for Private be represented with the value 1 and No with 0. We have a test case whose number of applications is 13,530 and is a private school. Suppose that we have the tiny 2-case training set below. What would the 1-nearest neighbor prediction be using Euclidean distance? college_clean %&gt;% filter(school %in% c(&quot;Princeton University&quot;, &quot;SUNY at Albany&quot;)) %&gt;% select(Apps, Private, Grad.Rate, school) sqrt( (13530 - ?)^2 + (1 - ?)^2) # Euclidean distance between test case and Princeton sqrt( (13530 - ?)^2 + (1 - ?)^2) # Euclidean distance between test case and SUNY Do you have any concerns about the resulting prediction? Based on this, comment on the impact of variable scaling and the distance measure on KNN performance. How might you change the distance calculation (or correspondingly rescale the data) to generate a more sensible prediction in this situation? Exercise 3: Implementing KNN in tidymodels Before continuing, install the kknn package by entering install.packages(\"kknn\") in the Console. We will step-by-step write code to “fit” a set of KNN models to predict Grad.Rate with the following specifications: Use the predictors Private, Top10perc (% of new students from top 10% of high school class), and S.F.Ratio (student/faculty ratio). Use 8-fold CV. (Why 8? Take a look at the sample size.) Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of neighbor values from 1 to 100 in increments of 5 (20 values in total). Step 1: Describe the model that we want to fit and how. (Describe it’s specification.) The general model type we are using is called nearest_neighbor. The KNN model has a parameter called neighbors (the number of nearest neighbors used to make predictions). The “engine” used to build the model is called \"kknn\". We using KNN for a regression task (quantitative outcome). (Nearly all of our methods can be used in both a regression and classification setting.) knn_spec &lt;- ___() %&gt;% # Insert the name of the general model type set_args(___ = tune()) %&gt;% # Insert the name of the parameter that we will tune set_engine(engine = ___) %&gt;% # Insert the engine name (in quotes) set_mode() # Indicate &quot;regression&quot; or &quot;classification&quot; Step 2: Divide data into folds for cross-validation. Use 8-fold CV. (Why 8? Take a look at the sample size with dim(college_clean) or nrow(college_clean).) set.seed(2023) # Why do we need this? college_cv &lt;- vfold_cv(___, v = ___) # Supply dataset and # of folds Step 3: Create our data preprocessing recipe. We first have to specify the outcome and predictors. We’re predicting Grad.Rate (graduation rate). Use the predictors Private, Top10perc (% of new students from top 10% of high school class), and S.F.Ratio (student/faculty ratio). Include step_dummy(all_nominal_predictors()). Because KNN needs to compute distances between cases, all predictors should be in numeric form. We can convert categorical variables (all_nominal_predictors()) to indicator (dummy) variables. Include step_normalize(all_numeric_predictors()). Why is this important based on Exercise 2? college_rec &lt;- recipe(___ ~ ___, data = ___) %&gt;% # Outcome, predictors, and dataset step_???() %&gt;% step_???() Step 4: Define our analysis workflow: our model specification (Step 1) and our data preprocessing recipe (Step 3). college_wf &lt;- workflow() %&gt;% add_model(___) %&gt;% # Model specification object add_recipe(___) # Data preprocessing recipe object Step 5: Set up “grid” of tuning parameters, and fit models for each tuning parameter value to find optimal value. Use 20 values between 1 and 100 for the number of neighbors. Compute rmse and mae in CV iterations. tuning_param_grid &lt;- grid_regular( neighbors(range = c(___, ___)), # min and max of values for neighbors levels = ___ # number of neighbors values ) knn_fit_cv &lt;- tune_grid( ___, # workflow object resamples = ___, # CV folds object grid = ___, # tuning parameter grid object metrics = metric_set(___, ___) # evaluation metric names (no quotes) ) After adapting the code (but before inspecting any output, which will happen in the next exercise), answer the following conceptual questions: Explain your choice for your recipe. Does KNN actually “fit” a model as part of training? (This feature of KNN is known as “lazy learning.”) How is test MAE estimated? What are the steps of the KNN algorithm with cross-validation? Draw a picture of how you expect test MAE to vary with \\(K\\), the number of neighbors. In terms of the bias-variance tradeoff, why do you expect the plot to look this way? Exercise 4: Inspecting the results The code below allows us to inspect our results. (It’s complete–nothing to fill in, but it’s helpful to look back at our LASSO code to see the similarities.) Use autoplot() to verify your expectations about the plot of test MAE vs. \\(K\\), the number of neighbors. Contextually interpret the test MAE for the “best” model. (There are two versions of best shown–what is the difference between them?) How else could you evaluate the KNN model? Does your KNN model help you understand which predictors of graduation rate are most important? Why or why not? autoplot(knn_fit_cv) + theme_classic() knn_fit_cv %&gt;% show_best(metric = &quot;mae&quot;) # Show evaluation metrics for different values of neighbors, ordered # Choose value of tuning parameter (# of neighbors) ## Overall lowest error knn_fit_cv %&gt;% select_best(metric = &quot;mae&quot;) ## Choose neighbors value that leads to the highest neighbors within 1 std. err. of the lowest CV MAE knn_fit_cv %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(neighbors)) ## The desc(neighbors) sorts the data from highest to lowest # of neighbors (most simple -&gt; most complex) Extra: Curse of dimensionality Just as with parametric models, we could keep going and add more and more predictors. However, the KNN algorithm is known to suffer from the “curse of dimensionality.” Explore this idea via the following resources: https://youtu.be/4v7ngaiFdp4 https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality "],["catch-up-day.html", "Topic 8 Catch-up Day Goals Building a tidymodels reference sheet", " Topic 8 Catch-up Day Slides from today are available here. Goals Check in with peers and instructor about conceptual questions Check in with instructor about project dataset Organize code we’ve encountered so far into a reference sheet Building a tidymodels reference sheet We’ve encountered a lot of tidymodels functions so far. Let’s try to build a reference sheet where we organize what functions are used at what point in an analysis. Revisit our topic pages for LASSO and KNN, and take a look at the “LASSO models in tidymodels” and “KNN models in tidymodels” sections to see how the tidymodels functions below are used. It may help to make a flow diagram indicating the order in which functions are generally run and what function outputs serve as inputs to other functions. It may help to insert screenshots of what function output looks like. tidymodels functions For specifying the model we want to fit and how: linear_reg() nearest_neighbor() set_args() tune() set_engine() set_mode() For CV: vfold_cv() For data preprocessing (recipes): recipe() step_normalize() step_dummy() all_predictors() all_nominal_predictors() all_numeric_predictors() Defining a modeling workflow: workflow() add_model() add_recipe() Tuning over a parameter grid: grid_regular() neighbors() penalty() tune_grid() metric_set() rmse() mae() Inspecting results: autoplot() collect_metrics() show_best() select_best() select_by_one_std_err() Using “best” parameters to fit the model to the full training data: finalize_workflow() fit() "],["splines.html", "Topic 9 Splines Learning Goals Splines in tidymodels Exercises", " Topic 9 Splines Learning Goals Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Slides from today are available here. Splines in tidymodels To build models with splines in tidymodels, we proceed with the same structure as we use for ordinary linear regression models but we’ll add some pre-processing steps to our recipe. To work with splines, we’ll use tools from the splines package. The ns() function in the splines package implements the variable transformations needed to create a natural cubic spline function for a quantitative predictor. The step_ns() function in tidymodels is an interface to ns(). # Linear regression model specification lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) # Recipe: linear regression model lm_rec &lt;- recipe(___ ~ ___, data = ___) # Recipe: linear regression model with natural splines ns_rec &lt;- lm_rec %&gt;% step_ns(__, deg_free = __) # natural cubic spline for a given predictor (higher deg_free means more knots) The deg_free argument in step_ns() stands for degrees of freedom: deg_free = # knots + 1 The degrees of freedom are the number of coefficients in the transformation functions that are free to vary (essentially the number of underlying parameters behind the transformations). The knots are chosen using percentiles of the observed values. Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the splines package by entering install.packages(\"splines\") in the Console. We’ll continue using the College dataset in the ISLR2 package to explore splines. You can use ?College in the Console to look at the data codebook. library(ISLR2) library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions data(College) # Data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% # creates variable with school name filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Evaluating a fully linear model We will model Grad.Rate as a function of 4 predictors: Private, Terminal, Expend, and S.F.Ratio. Make scatterplots of the quantitative predictors and the outcome with 2 different smoothing lines to explore potential nonlinearity. Adapt the following code to create a scatterplot with a smooth (curved) blue trend line and a red linear trend line. ggplot(___, aes(___)) + geom_point() + geom_smooth(color = &quot;blue&quot;, se = FALSE) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + theme_classic() Use tidymodels to fit a LASSO model (no splines yet) with the following specifications: Use 8-fold CV. Just use the 4 predictors Private, Terminal, Expend, and S.F.Ratio. Use CV mean absolute error (MAE) to evaluate models. Use the LASSO engine (\"glmnet\") to do variable selection to select the simplest model for which the metric is within one standard error of the best metric. Fit your “best” model and look at coefficients of that final model. set.seed(___) # Create CV folds data_cv8 &lt;- vfold_cv(___, v = ___) # Lasso Model Spec with tune lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = ___) %&gt;% set_mode(&quot;regression&quot;) # Recipe full_rec &lt;- recipe(___ ~ ___, data = college_clean) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% add_model(lm_lasso_spec_tune) # Tune Model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-3, 1)), # log10 transformed levels = 30 ) tune_output &lt;- tune_grid( lasso_wf_tune, # workflow resamples = data_cv8, # cv folds metrics = metric_set(___), grid = penalty_grid # penalty grid defined above ) # Select best model &amp; fit best_penalty &lt;- tune_output %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(penalty)) lasso_mod &lt;- finalize_workflow(lasso_wf_tune, best_penalty) %&gt;% fit(data = college_clean) # Note which variable is the &quot;least&quot; important lasso_mod %&gt;% tidy() Make plots of the residuals vs. the 3 quantitative predictors to evaluate the appropriateness of linear terms. lasso_mod_output &lt;- college_clean %&gt;% bind_cols(predict(lasso_mod, new_data = college_clean)) %&gt;% mutate(resid = ___ - ___) ggplot(lasso_mod_output, aes(___)) + ___ + ___ + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() Exercise 2: Evaluating a spline model We’ll extend our best linear regression model with spline functions of the quantitative predictors (leave Private as is). What tuning parameter is associated with splines? How do high/low values of this parameter relate to bias and variance? Update your recipe from Exercise 1 to fit a linear model (with the lm engine rather than LASSO) with the 2 best quantitative predictors with natural splines that have 2 knots (= 3 degrees of freedom) and include Private. Fit this model with CV, fit_resamples, (same folds as before) to compare MAE and then fit the model to the whole training data. Call this fit model ns_mod. # Model Spec lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) # New Recipe (remove steps needed for LASSO, add splines) spline_rec &lt;- recipe(___ ~ ___, data = ___) %&gt;% step___() # INSERT_ONE_OR_MORE_steps_here # Workflow (Recipe + Model) spline_wf &lt;- workflow() %&gt;% add_???() %&gt;% add_???() # CV to Evaluate cv_output &lt;- fit_resamples( ___, # workflow resamples = data_cv8, # cv folds metrics = metric_set(___) ) cv_output %&gt;% collect_metrics() # Fit with all data ns_mod &lt;- fit( ___, # workflow data = college_clean ) Make plots of the residuals vs. the 3 quantitative predictors to evaluate if splines improved the model. spline_mod_output &lt;- ___ # Residual plots Compare the CV MAE between models with and without the splines. tune_output %&gt;% collect_metrics() %&gt;% filter(penalty == (best_penalty %&gt;% pull(penalty))) cv_output %&gt;% collect_metrics() Extra! Variable scaling What is your intuition about whether variable scaling matters for the performance of splines? Check you intuition by reusing code from Exercise 2, except by adding in step_normalize(all_numeric_predictors()) before step_ns(). Call this ns_mod2. How do the predictions from ns_mod and ns_mod2 compare? You could use a plot to compare or check out the all.equal() function. all.equal(spline_mod_output$.pred, spline_mod_output2$.pred) plot(spline_mod_output$.pred, spline_mod_output2$.pred) "],["local-regression-gams.html", "Topic 10 Local Regression &amp; GAMs Learning Goals GAMs - Options for Fitting Exercises", " Topic 10 Local Regression &amp; GAMs Learning Goals Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Slides from today are available here. GAMs - Options for Fitting GAMs (splines + OLS) We’ve already talked about how to fit GAM models with splines (step_ns()) using the lm engine (ordinary least squares). GAMs (LOESS) The gam package provides tools for building GAMs with local regression (LOESS). We won’t explore this option further (via code) in class because there isn’t a tidymodels interface. GAMs (smoothing splines) in tidymodels Today, we’ll try fitting GAM models with smoothing splines in tidymodels. To build GAMs (using smoothing splines) in tidymodels, first load the package: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions Then adapt the following code: gam_spec &lt;- gen_additive_mod() %&gt;% set_args(select_features = TRUE, adjust_deg_free = tune()) %&gt;% set_engine(engine = &quot;mgcv&quot;) %&gt;% set_mode(&quot;regression&quot;) # The implementation of GAMs in tidymodels is different than other methods... # ...we don&#39;t specify a recipe gam_wf &lt;- workflow() %&gt;% add_variables(outcomes = YOUR_OUTCOME, predictors = c(PREDICTOR1, PREDICTOR2)) %&gt;% add_model(gam_spec, formula = YOUR_OUTCOME ~ CATEGORICAL_PREDICTORS + s(QUANTITATIVE_PREDICTOR, k=10)) # s(x1, k = 10): This makes the smoothing spline have 10 knots tuning_param_grid &lt;- grid_regular( adjust_deg_free(range = c(0.25, 4)), levels = 10 ) data_cv &lt;- vfold_cv(___, v = 10) tune_output &lt;- tune_grid( gam_wf, resamples = data_cv, metrics = metric_set(mae), grid = tuning_param_grid ) Picking the best tuning parameter and visualizing the GAM # Select best model &amp; fit to full training data best_param &lt;- tune_output %&gt;% select_best() best_by_1se_param &lt;- tune_output %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(adjust_deg_free)) gam_mod_best &lt;- finalize_workflow(gam_wf, best_param) %&gt;% fit(data = ___) gam_mod_best1se &lt;- finalize_workflow(gam_wf, best_by_1se_param) %&gt;% fit(data = ___) # Plot functions for each predictor # Dashed lines are +/- 2 SEs fit_gam_model %&gt;% pluck(&quot;fit&quot;) %&gt;% plot() Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the mgcv package by entering install.packages(\"mgcv\") in the Console. We’ll continue using the College dataset in the ISLR2 package to explore models for graduation rate. You can use ?College in the Console to look at the data codebook. library(ISLR2) library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Conceptual warmup How does high/low span relate to bias and variance of a local regression (LOESS) model? How does high/low lambda relate to bias and variance of a smoothing spline in a GAM model? Do you think that a GAM with all possible predictors will have better or worse performance than an ordinary (fully linear) least squares model with all possible predictors? Explain your thoughts. Why might we want to perform variable selection before fitting a GAM? How could stepwise selection or LASSO help with this? Exercise 2: Local regression (LOESS) Use LOESS (geom_smooth()) to explore the relationship between Apps and Grad.Rate for different values of span (between 0 and 1). How is varying span like varying number of neighbors in KNN? How would you describe the relationship between number of applications and graduation rate? college_clean %&gt;% ggplot(aes(x = Apps, y = Grad.Rate)) + geom_point(alpha = 0.2) + # Adjust point transparency with alpha geom_smooth(span = 0.4, method = &quot;loess&quot;, se = FALSE) + # vary span xlim(c(0,20000)) + theme_classic() Exercise 3: Building a GAM in tidymodels Suppose that initial variable selection investigations have given us a set of predictors to include in our GAM. The code below for building a GAM is complete (nothing to fill in), but before looking at output, we are going to closely examine this code in comparison to the code for our previous methods: LASSO and KNN. The adjust_deg_free argument is like lambda in LASSO. Higher values of this tuning parameter mean that “wiggliness” is penalized more. The select_features = TRUE allows for the ability to eliminate a predictor via penalization (more likely with higher adjust_deg_free). set.seed(123) gam_spec &lt;- gen_additive_mod() %&gt;% set_args(select_features = TRUE, adjust_deg_free = tune()) %&gt;% set_engine(engine = &quot;mgcv&quot;) %&gt;% set_mode(&quot;regression&quot;) # The implementation of GAMs in tidymodels is different than other methods... # ...we don&#39;t specify a recipe gam_wf &lt;- workflow() %&gt;% add_variables(outcomes = Grad.Rate, predictors = c(Private, Apps, Top10perc, P.Undergrad, Outstate, perc.alumni)) %&gt;% add_model(gam_spec, formula = Grad.Rate ~ Private + s(Apps, k=10) + s(Top10perc, k=10) + s(P.Undergrad, k=10) + s(Outstate, k=10) + s(perc.alumni, k=10)) tuning_param_grid &lt;- grid_regular( adjust_deg_free(range = c(0.25, 4)), levels = 10 ) data_cv8 &lt;- vfold_cv(college_clean, v = 8) # This takes a few seconds tune_output &lt;- tune_grid( gam_wf, resamples = data_cv8, metrics = metric_set(mae), grid = tuning_param_grid ) We can take a look at the test error metrics from CV and choose an optimal tuning parameter: tune_output %&gt;% collect_metrics() tune_output %&gt;% show_best() autoplot(tune_output) + theme_classic() # Select best model &amp; fit to full training data best_param &lt;- tune_output %&gt;% select_best() best_by_1se_param &lt;- tune_output %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(adjust_deg_free)) gam_mod_best &lt;- finalize_workflow(gam_wf, best_param) %&gt;% fit(data = college_clean) gam_mod_best1se &lt;- finalize_workflow(gam_wf, best_by_1se_param) %&gt;% fit(data = college_clean) Let’s visualize the estimated nonlinear functions. What about these plots indicates that using a GAM instead of ordinary linear regression was probably a good choice? Compare the plots resulting from the two different choices for “best” tuning parameter. Which choice would you prefer and why? For each of the plots, write a sentence describing what you learn about the relationship between graduation rate and that predictor. # Plot the estimated nonlinear functions for all predictors... # ...for the GAM resulting from the adjust_deg_free parameter value that gave the lowest error gam_mod_best$fit$fit$fit %&gt;% plot(all.terms = TRUE, pages = 1) # ...for the GAM resulting from the adjust_deg_free parameter value that gave the lowest error gam_mod_best1se$fit$fit$fit %&gt;% plot(all.terms = TRUE, pages = 1) "],["synthesis-regression.html", "Topic 11 Synthesis: Regression Exercises", " Topic 11 Synthesis: Regression Slides from today are available here. Exercises The topics in the regression unit of our course have been divided into variable selection methods and nonlinear modeling methods. What is the rationale for this breakdown? That is, why are LASSO and subset selection better for selecting useful variables than GAMs? Why don’t LASSO and subset selection automatically handle nonlinearity? Thus why is it useful to add KNN, splines, local regression, and GAMs to our toolbox? Suppose we fit a fully linear model (linear relationships for all predictors) - how would we use residual plots to assess the need for nonlinear transformations? Suppose that we have already selected important predictors and want to fit a GAM. Let’s consider a GAM built with splines. What are the steps of cross-validation to choose the “best” number of knots? (For simplicity, assume that all quantitative predictors will have the same number of knots.) GAMs can also be constructed with local regression. What steps are involved in tuning a GAM built with local regression? In terms of bias and variance, why does an underfit/overfit model have poor test performance? In terms of bias and variance, what is the rationale for using the select_by_one_std_err() function for choosing an optimal tuning parameter, as opposed to select_best()? Look back at the themes that we focused on in our KNN, splines, and local regression + GAMs class activities - what comes up as being most important? What questions do you have about these activities? "],["logistic-regression.html", "Topic 12 Logistic Regression Learning Goals Logistic regression in tidymodels Exercises", " Topic 12 Logistic Regression Learning Goals Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Slides from today are available here. Logistic regression in tidymodels To build logistic regression models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(ggplot2) library(tidymodels) library(probably) # install.packages(&quot;probably&quot;) tidymodels_prefer() set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code to fit a logistic regression model: # Make sure you set reference level (the outcome you are NOT interested in) data &lt;- data %&gt;% mutate(outcome = relevel(outcome, ref = &quot;failure&quot;)) # set reference level data_cv &lt;- vfold_cv(data, v = 10) # Logistic Regression Model Spec logistic_spec &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) # Recipe logistic_rec &lt;- recipe(outcome ~ ., data = data) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) # Workflow (Recipe + Model) logistic_wf &lt;- workflow() %&gt;% add_recipe(logistic_rec) %&gt;% add_model(logistic_spec) # Fit Model to Training Data logistic_fit &lt;- fit(logistic_wf, data = data) Examining the logistic model # Display coefficient estimates logistic_fit %&gt;% tidy() # Get exponentiated coefficients and confidence intervals logistic_fit %&gt;% tidy() %&gt;% mutate( OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error) ) %&gt;% mutate(OR = exp(estimate)) Making predictions from the logistic model # Make soft (probability) predictions predict(logistic_fit, new_data = ___, type = &quot;prob&quot;) # Make hard (class) predictions (using a default 0.5 probability threshold) predict(logistic_fit, new_data = ___, type = &quot;class&quot;) Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a spam dataset that contains information on different features of emails and whether or not the email was spam. The variables are as follows: spam: Either spam or not spam word_freq_WORD: percentage of words in the e-mail that match WORD (0-100) char_freq_CHAR: percentage of characters in the e-mail that match CHAR (e.g., exclamation points, dollar signs) capital_run_length_average: average length of uninterrupted sequences of capital letters capital_run_length_longest: length of longest uninterrupted sequence of capital letters capital_run_length_total: sum of length of uninterrupted sequences of capital letters Our goal will be to use email features to predict whether or not an email is spam - essentially, to build a spam filter! library(dplyr) library(readr) library(ggplot2) library(tidymodels) tidymodels_prefer() spam &lt;- read_csv(&quot;https://www.dropbox.com/s/leurr6a30f4l32a/spambase.csv?dl=1&quot;) # A little data cleaning to remove the space in &quot;not spam&quot; spam &lt;- spam %&gt;% mutate(spam = ifelse(spam==&quot;spam&quot;, &quot;spam&quot;, &quot;not_spam&quot;)) Exercise 1: Visualization warmup Let’s take a look at the frequency of the word “George” (the email recipient’s name is George) (word_freq_george) and the frequency of exclamation points (char_freq_exclam). Create appropriate visualizations to assess the predictive ability of these two predictors. # If you want to adjust the axis limits, you can add the following to your plot: # + coord_cartesian(ylim = c(0,1)) # + coord_cartesian(xlim = c(0,1)) ggplot(spam, aes(x = ___, y = ___)) + geom_???() Exercise 2: Implementing logistic regression in tidymodels Our goal is to fit a logistic regression model with word_freq_george and char_freq_exclam as predictors. Write down the corresponding logistic regression model formula using mathematical notation. Use tidymodels to fit this logistic regression model to the training data. Let’s try to do this from scratch (almost). Open up this tidymodels note sheet, and we’ll work through the thought process piece by piece. Work with your group to figure out what phase of the analysis is happening in each row. What do you think needs to be modified to implement logistic regression? (For now, we’re just fitting a model with a fixed set of predictors–not trying to estimate test performance with CV.) Key changes for implementing logistic regression: the model name is logistic_reg(), the model-building engine is \"glm\", and we are now performing \"classification\" rather than \"regression\" # Need to set reference level (to the outcome you are NOT interested in) spam &lt;- spam %&gt;% mutate(spam = relevel(factor(spam), ref=&quot;not_spam&quot;)) # Logistic regression model specification logistic_spec &lt;- # Recipe logistic_rec &lt;- # Workflow (Recipe + Model) log_wf &lt;- # Fit Model log_fit &lt;- fit(log_wf, data = spam) Exercise 3: Interpreting the model Take a look at the log-scale coefficients with tidy(log_fit). Do the signs of the coefficients for the 2 predictors agree with your visual inspection from Exercise 1? Display the exponentiated coefficients, and provide contextual interpretations for them (not the intercept). (Use the output of tidy() with mutate() and exp().) Exercise 4: Making predictions Consider a new email where the frequency of “George” is 0.25% and the frequency of exclamation points is 1%. Use the model summary to make both a soft (probability) and hard (class) prediction for this test case by hand. Use a default probability threshold of 0.5. (You can use math expressions to use R as a calculator. The exp() function exponentiates a number.) Check your work from part a by using predict(). predict(log_fit, new_data = data.frame(word_freq_george = 0.25, char_freq_exclam = 1), type = &quot;prob&quot;) predict(log_fit, new_data = data.frame(word_freq_george = 0.25, char_freq_exclam = 1), type = &quot;class&quot;) "],["homework-1.html", "Homework 1 Project Work Portfolio Work Metacognitive Reflection", " Homework 1 Due Friday, February 3 at midnight CST on Moodle Please turn in a single PDF document containing (1) your responses for the Project Work and Metacognitive Reflection sections and (2) a LINK to the Google Doc with your responses for the Portfolio Work section. Project Work Goal: Find a dataset to use for your final project, and begin getting acquainted with the data. Details: Your dataset should allow you to perform a supervised learning analysis (regression or classification) and an unsupervised learning analysis (clustering or dimension reduction). The following resources are good places to start looking for data: Google Dataset Search Kaggle UCI Machine Learning Repository Harvard Dataverse Macalester’s librarians are a great resource too! They can help you find data aligning with your interests. You can make an appointment using the Ask Us page on the library website. Even if you end up working in a group on the project (which isn’t required - working alone is fine), please complete this initial work individually. Check in with the instructor early if you need help. Deliverables: Write 1-2 paragraphs (no more than 350 words) summarizing: The information in the dataset and the context behind the data. Use the prompts below to guide your thoughts. (Note: in some situations, there may be incomplete information on the data context. That’s fine. Just do your best to summarize what information is available, and acknowledge the lack of information where relevant.) What are the cases? Broadly describe the variables contained in the data. Who collected the data? When, why, and how? 2 (tentative) research questions 1 that can be investigated in a supervised learning setting (regression OR classification–you’re welcome to do both if you wish!) 1 that can be investigated in an unsupervised learning setting (clustering OR dimension reduction–you’re welcome to do both if you wish!) Note: These research questions might evolve over the course of the semester, and that’s fine! Having some idea of potential research directions now will still be helpful. Also make sure that you can read the data into R. You don’t need to do any analysis in R yet, but making sure that you can read the data will make the next steps go more smoothly. Portfolio Work Over the course of the semester, you will use your understanding of course ideas to build up a written portfolio that you can refer to years down the line. Each homework assignment will have prompts that require you to reflect on Data Ethics as well as concepts related to machine learning methods that are enduring, important, and worth being familiar with. (Refer to our syllabus on Moodle for a reminder of this breakdown.) You will receive comments on your responses from the instructor and preceptors that you can use to evaluate your understanding and to revise your work on subsequent assignments throughout the semester. Logistics: Make a copy of this Google Doc. You will use this SINGLE Google Doc for your homework responses (and revisions) all semester. (A single doc facilitates seeing feedback and revisions over time.) Update the sharing permissions on your doc so that anyone with the link can provide comments. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Areas to address: Data Ethics: Read the article Amazon scraps secret AI recruiting tool that showed bias against women. Write a short (roughly 250 words), thoughtful response about the themes and cautions that the article brings forth. Overfitting: The video used the analogy of a cat picture model to explain overfitting. Come up with your own analogy to explain overfitting. Evaluating regression models: Describe how residuals are central to the evaluation of regression models. Explain how they arise in quantitative evaluation metrics and how they are used in evaluation plots. Include examples of plots that show desirable and undesirable model behavior (feel free to draw them by hand if you wish) and what steps can be taken to address that undesirable behavior. Cross-validation: In your own words, explain the rationale for cross-validation in relation to overfitting and model evaluation. Describe the algorithm in your own words in at most 2 sentences. Subset selection: Algorithmic understanding: Look at Conceptual exercise 1, parts (a) and (b) in ISLR Section 6.6. What are the aspects of the subset selection algorithm(s) that are essential to answering these questions, and why? (Note: you’ll have to try to answer the ISLR questions to respond to this prompt, but the focus of your writing should be on the question in bold here.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? LASSO: Algorithmic understanding: Come up with your own analogy for explaining how the penalized least squares criterion works. Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? Metacognitive Reflection How do you feel about your level of understanding for the topics that we have covered so far? What were some notable themes in self- and peer-noticings? (What has been confusing and/or intriguing based on pre-class videos, in-class activities, and this homework assignment?) How do these reflections inform your next steps for developing stronger understanding and/or advising future students learning about these topics? "],["homework-2.html", "Homework 2 Project Work Portfolio Work Metacognitive Reflection", " Homework 2 Due Friday, February 17 at midnight CST. Deliverables: Nothing to submit on Moodle this time. Your Portfolio work will continue to go in the same Google Doc from HW1. This time, you’ll add your Metacognitive Reflection to your Portfolio. Project Work No new deliverables this time. Continue working to finalize your dataset if you haven’t already. Check in with the instructor if you would like help. If you already have your dataset, load into R and make some exploratory plots to get a sense for the distributions of variables and some initial relationships. This will also inform any preprocessing/cleaning steps needed. Portfolio Work Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. Organization: On the left side of your Google Doc (in the gray area beneath the menu bar), there is a gray icon–click this to show the section headers. Write your responses under these section headers. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Revisions: Make any revisions desired to previous concepts. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homeworks will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance on Piazza and in drop-in hours. New concepts to address: The following prompts are shared for all methods: Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Subset selection: Bias-variance tradeoff Parametric / nonparametric LASSO: Bias-variance tradeoff Parametric / nonparametric KNN: Algorithmic understanding: Draw and annotate pictures that show how the KNN (K = 2) regression algorithm would work for a test case in a 2 quantitative predictor setting. Also explain how the curse of dimensionality affects KNN performance. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: The KNN algorithm is often called a “lazy” learner. Discuss how this relates to the model training process and the computations that must be performed when predicting on a new test case. (3 sentences max.) Interpretation of output: The “lazy” learner feature of KNN in relation to model training affects the interpretability of output. How? (3 sentences max.) Splines: Algorithmic understanding: Explain the advantages of natural cubic splines over global transformations and piecewise polynomials. Also explain the connection between splines and the ordinary (least squares) regression framework. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: When using splines, how does computation time compare to fitting ordinary (least squares) regression models? (1 sentence) Interpretation of output: SKIP - will be covered in the GAMs section Data Ethics: Read the article Automated background checks are deciding who’s fit for a home. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. What themes recur from the HW1 article (on an old Amazon recruiting tool)? What aspects are more particular to the context of equity in housing access? Metacognitive Reflection Deliverables: Add a new page to the top of your Portfolio Google Doc titled “Metacognitive Reflections” – for this and remaining homework assignments, your Metacognitive Reflections will go here. Create a subsection called “Homework 2” to put your reflection from the following prompts: How do you feel about your level of understanding for the topics that we have covered so far? What were some notable themes in self- and peer-noticings? (What has been confusing and/or intriguing based on pre-class videos, in-class activities, and this homework assignment?) What insights on your understanding did you gain from taking and reviewing feedback on Quiz 1? How do these reflections inform your next steps for developing stronger understanding and/or advising future students learning about these topics? "],["homework-3.html", "Homework 3 Project Work Portfolio Work Metacognitive Reflection", " Homework 3 Portfolio Work and Metacognitive Reflection due Friday, March 3 at midnight. (Continue working in the same Google Doc from HW1.) Project Work due Friday, March 10 at midnight CST on Moodle. (Just one person per project group needs to submit.) Project Work Goal: Begin an analysis of your dataset to answer your supervised research question. Collaboration: If you have already formed a group (of at most 3 members) for the project, this part should be done as a group. Only one group member should submit a Project Work section. Deliverables: Please use this template to knit an HTML document. Convert this HTML document to a PDF by opening the HTML document in your web browser. Print the document (Ctrl/Cmd-P) and change the destination to “Save as PDF.” Submit this one PDF to Moodle. Alternatively, you may knit your Rmd directly to PDF if you have LaTeX installed. Data cleaning: If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the R Resources page to see if your questions are answered there. If not, post on our Piazza forum in the coding folder. Required Analyses: Exploratory analyses Make a univariate plot showing the distribution of your outcome variable. If performing a classification analysis, provide a tabulation of the levels of the outcome with data %&gt;% dplyr::count(outcome). Comment on any peculiarities of the outcome distribution (e.g., skewed distribution, bimodality, rare outcomes, outliers). Make plots exploring the relationship between a handful of interesting predictors and the outcome. (Choose a small number of predictors that seem interesting.) Comment on what you see in these plots. LASSO modeling: Ignoring nonlinearity (for now) Fit a LASSO model for your outcome which includes linear relationships between all predictors and the outcome. Estimate test performance for your models using CV. Justify your choice for the number of folds by considering the sample size. Report and interpret (with units) the CV test performance estimates along with a measure of uncertainty in the estimate (std_error is readily available when you used collect_metrics(summarize=TRUE)). (If conducting a regression analysis) Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. What insights are expected? Surprising? Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected. Choose the “best” value of lambda using an appropriate criterion, and justify your choice. Inspect the coefficient signs and magnitudes in the model resulting from the “best” lambda. Do they make sense? KNN modeling Fit a KNN model for your outcome. Estimate test performance for your models using CV. Report and interpret (with units) the CV test performance estimates along with a measure of uncertainty in the estimate. Choose the “best” value for the # of neighbors using an appropriate criterion, and justify your choice. Using your “best” model, make plots of the predicted values vs. key predictors of interest to get a sense for the relationships that KNN is “learning.” (Learning is in quotes because of KNN’s “lazy learner” feature.) See the KNN solutions on Moodle for a guide on how to do this. Summarize investigations Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both? Societal impact Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work? Portfolio Work Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. Organization: On the left side of your Google Doc (in the gray area beneath the menu bar), there is a gray icon–click this to show the section headers. Write your responses under these section headers. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Revisions: Make any revisions desired to previous concepts. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homework assignments will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance on Piazza and in drop-in hours. New concepts to address: Local regression: Algorithmic understanding: Consider the R functions lm(), predict(), dist(), and dplyr::filter(). (Look up the documentation for unfamiliar functions in the Help pane of RStudio.) In what order would these functions need to be used in order to make a local regression prediction for a supplied test case? Explain. (5 sentences max.) Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: SKIP Interpretation of output: SKIP - will be covered in the GAMs section GAMs: Algorithmic understanding: How do linear regression, splines, and local regression each relate to GAMs? Why would we want to model with GAMs? (5 sentences max.) Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: How a GAM is specified affects the time required to fit the model - why? Focus on comparing a GAM with natural cubic splines to a GAM fit with local regression and backfitting (review GAM concept video for details). (3 sentences max.) Interpretation of output: How does the interpretation of ordinary regression coefficients compare to the interpretation of GAM output? (3 sentences max.) Logistic regression: Algorithmic understanding: Write your own example of a logistic regression model formula. (Don’t use the example from the video.) Using this example, show how to use the model to make both a soft and a hard prediction. Bias-variance tradeoff: (Answer this for LASSO logistic regression) What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: (Answer this for LASSO logistic regression) Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: SKIP Interpretation of output: In general, how can the coefficient for a quantitative predictor be interpreted? How can the coefficient for a categorical predictor (an indicator variable) be interpreted? Evaluating classification models: Consider this xkcd comic. Write a paragraph (around 250 words) that addresses the following questions. Craft this paragraph so it flows nicely and does not read like a disconnected list of answers. (Include transitions between sentences.) This comic is trying to parody a classification setting - what is the outcome variable here? The new “Is it Christmas” service presented in the comic is essentially a (silly) model - what is this model? (How is the “Is it Christmas” service predicting the outcome?) How do the ideas in this comic emphasize comparisons between overall accuracy and class-specific accuracy measures? What are the names of the relevant class-specific accuracy measures here, and what are there values? How does this comic connect to the no-information rate? Data Ethics: Read the article Getting Past Identity to What You Really Want. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. What skills do you think are essential for the leaders and data analysts of organizations to have to handle these issues with care? Metacognitive Reflection (Put this reflection in your Portfolio Google Doc in the Metacognitive Reflections section, and create a subsection called “Homework 3.”) We will be having 1-on-1 learning conferences the week before Spring Break (3/6 - 3/10) to discuss your learning so far and your course goals. The goal of this reflection is to prepare for these conferences. Please directly address all of the following prompts: What are your goals for this course? What are you hoping to be able to do or understand deeply by the end of the semester? How do you feel about your progress towards these learning goals? What can you do, and what can the instructor do to make the remainder of the semester as successful as possible? For each of the following topics, comment on what you understand well and what you don’t understand as well. In doing so, please look back at your Portfolio and Quiz responses and feedback. The difference between regression and classification tasks in supervised learning Over/underfitting, cross-validation, and the bias-variance tradeoff: connections between these concepts Evaluating regression models with evaluation metrics and residual plots Subset selection (best subset and stepwise) LASSO KNN Splines Local regression Generalized additive models Evaluating classification models Logistic regression (+ LASSO) Based on your responses above, propose a midterm grade for yourself using the guidelines discussed on pages 4 and 5 of our syllabus. I will read your reflection before our conference. The goal of our conference will be to discuss your progress so far and to clarify a plan for the remainder of the semester. "],["final-project.html", "Final Project Requirements", " Final Project Requirements Scope: You will be analyzing a dataset using both supervised and unsupervised learning tools. Finding data: Your dataset should allow you to perform a supervised learning analysis (regression or classification) and an unsupervised learning analysis (clustering or dimension reduction). The following resources are good places to start looking for data: Kaggle Tidy Tuesday GitHub repository UCI Machine Learning Repository Harvard Dataverse Google Dataset Search Macalester’s librarians are a great resource too! They can help you find data aligning with your interests. You can make an appointment using the Ask Us page on the library website. What qualities should you look for in a dataset? Should contain an outcome variable that you would be interested in modeling Should contain roughly 10 sensible predictors or more (ID variables aren’t useful; latitude and longitude are generally not useful as predictors) NOT data where the main predictor is a time variable (e.g., year, day). This usually is a situation where the goal is to forecast a trend into the future. (This type of research question can’t be handled well using methods that we’ll cover–appropriate tools are covered in STAT 452: Correlated Data.) Collaboration: You may work in teams of up to 3 members. Individual work is fine. The homework assignments will indicate whether work for that assignment should be submitted individually or if just one team member should submit work. There will be a required synthesis of the weekly homework investigations at the end of the course. If working on a team, this should be done in groups, rather than individually. Final deliverables: Each group will be required to give a practice presentation and a final presentation. The instructor will provide feedback on the practice presentation to incorporate into the final presentation. (See our syllabus on Moodle for details.) Requirements for what to cover in the presentation will be updated here later in the semester. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

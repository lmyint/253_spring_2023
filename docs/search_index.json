[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source: Flaticon This is the class manual for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2021. The content here was made by Leslie Myint and draws upon our class textbook, An Introduction to Statistical Learning with Applications in R, and on materials prepared by Alicia Johnson and Brianna Heggeseth. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule.html", "Schedule", " Schedule The schedule below is a tentative outline of our plans for the semester. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Goals. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Remember to take notes on where you paused/rewound/reread or smiled/nodded during the videos/readings. This is essential for the Metacognitive Reflection part of the course. Week 1: Intros and Evangelizing Evaluation (1/20 - 1/27) Dates Topic Videos/Readings Video Slides Class Slides 1/20 Introductions ISLR: Ch 1, Ch 2--Section 2.1(Skip 2.1.2, 2.1.3 for now.) 1/23 Evaluating Regression Models Evaluating Regression Models R: Introduction to TidyModels ISLR: 2.2 1/25 Overfitting Overfitting R: Preprocessing and Recipes ISLR: 5.1 1/27 Cross-validation Cross-validation R: Training, Testing and Cross-Validation ISLR: 5.1 N/A Start Homework 1 due Friday, 2/3 at midnight CST Week 2: A Soirée with Selection Strategies (1/30 - 2/3) Dates Topic Videos/Readings Video Slides Class Slides 1/30 Subset Selection Variable Subset Selection R: Subset Selection ISLR: 6.1 2/1 LASSO (Shrinkage/Regularization) LASSO (Shrinkage/Regularization) R: LASSO and Regularization ISLR: 6.2 2/3 LASSO (continued) Finish Homework 1 due Friday, 2/3 at midnight CST Week 3: A Treasure in Tradeoffs (2/6 - 2/10) Dates Topic Videos/Readings Video Slides Class Slides 2/6 KNN Regression and the Bias-Variance Tradeoff KNN Regression and the Bias-Variance Tradeoff R: KNN Regression ISLR: 2.2.2 (bias-variance tradeoff); 3.5 (KNN regression) 2/8 Quiz 1 (Covers Topics 1 to 6. You may bring a 3x5 notecard with you.) 2/10 Catch-up day: We'll practice deepening our understanding of the bias-variance tradeoff and work towards building a tidymodels reference sheet. Start Homework 2 due Friday, 2/17 at midnight CST Week 4: A Feast with Flexibility (2/13 - 2/17) Dates Topic Videos/Readings Video Slides Class Slides 2/13 Modeling Nonlinearity: Polynomial Regression and Splines Modeling Nonlinearity: Polynomial Regression and Splines R: Nonlinearity: Polynomial Regression and Splines ISLR: 7.1-7.4 2/15 Review of Quiz 1 2/17 Local Regression and Generalized Additive Models Local Regression and Generalized Additive Models R: Local Regression and GAMs ISLR: 7.6-7.7 Finish Homework 2 due Friday, 2/17 at midnight CST Week 5: Regression Wrap-Up and Commencing Classification (2/20 - 2/24) Dates Topic Videos/Readings Video Slides Class Slides 2/20 Review and synthesis of our regression unit 2/22 Quiz 2 (Covers bias-variance tradeoff, KNN, local regression, GAMs. You may bring a 3x5 notecard with you.) 2/24 Logistic Regression Logistic Regression R: Logistic Regression ISLR: 4.1 - 4.3 Start Homework 3 due Friday, 3/3 at midnight CST (Portfolio + Reflection) and Friday 3/10 at midnight (Project Work) Week 6: Staying Classy with Classification (2/27 - 3/3) Dates Topic Videos/Readings Video Slides Class Slides 2/27 Evaluating Classification Models & Revisiting LASSO+KNN Evaluating Classification Models R: Evaluating Classification 3/1 Evaluating Classification Models & Revisiting LASSO+KNN (continued) 3/3 Capstone Days! (no class - attend a talk instead!) Finish Homework 3 due Friday, 3/3 at midnight CST (Portfolio + Reflection) and Friday 3/10 at midnight (Project Work) Week 7: Learning Conferences (3/6 - 3/10) No class this week to make space for Learning Conferences. Schedule a conference with the instructor this week using the Calendly link under the Moodle course calendar. (Please sign up by 3/3.) Week 8: Travels with Trees (3/20 - 3/24) Dates Topic Videos/Readings Video Slides Class Slides 3/22 Decision Trees (Conceptual) Decision Trees ISLR: 8.1 3/24 Decision Trees (Coding) R: Decision Trees Finish Homework 4 due Friday, 3/24 at midnight CST Week 9: Classification Wrap-up(3/27 - 3/31) Dates Topic Videos/Readings Video Slides Class Slides 3/27 Bagging and Random Forests (Conceptual) Bagging and Random Forests ISLR: 8.2.1, 8.2.2 3/29 Bagging and Random Forests (Coding) R: Bagging and Random Forests 3/31 Quiz 3 (NOT IN CLASS--ON YOUR OWN) (Covers our classification unit: evaluating classification models, (LASSO) logistic regression, decision trees, bagging and random forests. You may use a 3x5 notecard but no other notes.) Start Homework 5 due Friday, 4/7 at midnight CST Week 10: Crazy for Clustering (4/3 - 4/7) Dates Topic Videos/Readings Video Slides Class Slides 4/3 K-Means Clustering (Conceptual) K-Means Clustering ISLR: 12.4.1, 12.4.3 4/5 Hierarchical Clustering (Conceptual) Hierarchical Clustering ISLR: 12.4.2, 12.4.3 4/7 Hierarchical and K-Means Clustering (Coding) Finish Homework 5 due Friday, 4/7 at midnight CST Week 11: Dabbling in Dimension Reduction (4/10 - 4/14) Dates Topic Videos/Readings Video Slides Class Slides 4/10 Principal Components Analysis (Conceptual) Principal Components Analysis ISLR: 12.2 4/12 Principal Components Analysis (Coding) 4/14 Quiz 4 (NOT IN CLASS--ON YOUR OWN) (Covers our unsupervised learning unit: k-means and hierarchical clustering, principal components analysis. You may use a 3x5 notecard but no other notes.) Start Homework 6 due Friday, 4/21 at midnight CST Week 12: Project Work Dates Topic Videos/Readings Video Slides Class Slides 4/17 Project work day: mini-HW and intermediate project deliverables will be posted here 4/19 Project work day: mini-HW and intermediate project deliverables will be posted here 4/21 Project work day: mini-HW and intermediate project deliverables will be posted here Finish Homework 6 due Friday, 4/21 at midnight CST Week 13: Project Work Dates Topic Videos/Readings Video Slides Class Slides 4/24 Project work day: mini-HW and intermediate project deliverables will be posted here 4/26 Project work day: mini-HW and intermediate project deliverables will be posted here 4/28 Project work day: mini-HW and intermediate project deliverables will be posted here Work on Final Project due Friday, 5/5 at midnight CST Week 14: Project Work Dates Topic Videos/Readings Video Slides Class Slides 5/1 Project work day: mini-HW and intermediate project deliverables will be posted here Finish Final Project due Friday, 5/5 at midnight CST "],["learning-goals.html", "Learning Goals", " Learning Goals Learning goals for our course topics are listed below. Use these to guide your synthesis of video and reading material. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN (not in the video–will be discussed in class) Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Logistic regression Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Evaluating classification models Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Decision trees Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Bagging and random forests Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class) K-means clustering Clearly describe / implement by hand the k-means algorithm Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Hierarchical clustering Clearly describe / implement by hand the hierarchical clustering algorithm Compare and contrast k-means and hierarchical clustering in their outputs and algorithms Interpret cuts of the dendrogram for single and complete linkage Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before class on Monday, January 23, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. As of January 17, 2023, the latest version of R is 4.2.2. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. As of January 17, 2023, the latest version of RStudio is 2022.12.0+353. Suggested: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;tidymodels&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(tidymodels) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(\"rlang\") in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],["r-resources.html", "R Resources Outside resources Example code", " R Resources Outside resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) RStudio cheat sheets ggplot2 reference R Programming Wikibook Debugging in R Article Video Colors in R Data import tutorial Free online textbooks R for Data Science Exploratory Data Analysis with R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "],["introductions.html", "Topic 1 Introductions Explorations", " Topic 1 Introductions Slides from today are available here. Explorations Phase 1 Each of the data contexts below prompts a broad research goal. Pick one context, and brainstorm as many research questions that would be worth investigating. Context 1 (Media and Publishing): The New York Times is trying to better understand the popularity of its different articles with the hope of using this understanding to improve hiring of writers and improve their website layout. Context 2 (Public Health): The Minnesota Department of Health is trying to better understand the different health trajectories of people who have contracted a certain illness to improve funding for relevant health services. Context 3 (Civics and Politics): The campaign management team for a political candidate is trying to better understand voter turnout in different regions of the country to run a better campaign in the next election. Context 4 (Sports): A coach (you pick the sport) wants to use data from games and practices to improve player and team performance. If you have time, repeat the brainstorm for additional contexts. Phase 2 Consider each of your brainstormed questions. Which can be framed as a regression exploration? Classification? Unsupervised learning? Which questions seem to not fit under any of these areas? Are there harms that you anticipate arising from the collection of data or its analysis? Put your responses in this Google Doc. "],["evaluating-regression-models.html", "Topic 2 Evaluating Regression Models Learning Goals Exercises", " Topic 2 Evaluating Regression Models Learning Goals Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way We’ll also start to develop some new ideas relating to our next topics. Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) (If you have not already installed the tidymodels package, install it by running install.packages(\"tidymodels\") in the Console.) library(readr) library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tidymodels) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────── tidymodels 1.0.0 ── ## ✔ broom 1.0.3 ✔ rsample 1.1.1 ## ✔ dials 1.1.0 ✔ tibble 3.1.8 ## ✔ infer 1.0.4 ✔ tidyr 1.3.0 ## ✔ modeldata 1.0.1 ✔ tune 1.0.1 ## ✔ parsnip 1.0.3 ✔ workflows 1.1.2 ## ✔ purrr 1.0.1 ✔ workflowsets 1.0.0 ## ✔ recipes 1.0.4 ✔ yardstick 1.1.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ yardstick::spec() masks readr::spec() ## ✖ recipes::step() masks stats::step() ## • Search for functions across packages at https://www.tidymodels.org/find/ tidymodels_prefer() bodyfat &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) ## Rows: 80 Columns: 17 ## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (17): fatBrozek, fatSiri, density, age, weight, height, neck, chest, abd... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Class investigations We’ll work through this section together to review concepts and code. You’ll then work on the remainder of the exercises in your groups. # Exploratory plots # Univariate distribution of outcome ggplot(bodyfat, aes(???)) + geom_???() # Scatterplot of fatSiri vs. weight ggplot(bodyfat, aes(???)) + geom_???() Let’s fit a linear regression model using tidymodels to predict body fat percentage from weight. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) mod1 &lt;- fit(lm_spec, ?? ~ ??, data = bodyfat) mod1 %&gt;% tidy() We can use the predict() function to use our fit model to predict the outcome based on values in our original data. We can calculate residuals by taking our true outcome values and subtracting the predicted value, stored as .pred. mod1_output &lt;- mod1 %&gt;% predict(new_data = bodyfat) %&gt;% bind_cols(??) %&gt;% mutate(resid = ?? - ??) head(mod1_output) We can use this data frame to compute error metrics by hand or by using functions from the yardstick package. # MSE - what is the interpretation with units? mod1_output %&gt;% summarize(mean(resid^2)) # RMSE - what is the interpretation with units? mod1_output %&gt;% summarize(sqrt(mean(resid^2))) mod1_output %&gt;% rmse(truth = ??, estimate = .pred) # MAE - what is the interpretation with units? mod1_output %&gt;% summarize(mean(abs(resid))) mod1_output %&gt;% mae(truth = ??, estimate = .pred) # R-squared - interpretation? (unit-less) mod1_output %&gt;% summarize(1 - (var(resid) / var(fatSiri))) mod1_output %&gt;% rsq(truth = ??, estimate = .pred) …and to create residual plots: # Residuals vs. predictions ggplot(mod1_output, aes(x = .pred, y = resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() # Residuals vs. predictors (x&#39;s) ggplot(mod1_output, aes(x = height, y = resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() Exercise 1 First decide on what you think would be a good model by picking variables based on context. Fit this model, calling it mod_initial. (Remember that you can include several predictors with a + in the model formula - like y ~ x1+x2.) # Code to fit initial model Use residual plot explorations to check if you need to update your model. # Residual plot explorations Fit your updated model, and call it model_updated. # Code to fit updated model Exercise 2 Compute and contextually interpret relevant evaluation metrics for your model. # Code to compute evaluation metrics Exercise 3 Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the new_data to use bodyfat_test instead of bodyfat. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Compare your evaluation metrics from Exercise 2 the metrics here. What do you notice? (Note: this observation is just based on your one particular fitted model. You’ll make some more comprehensive observations in the next exercise.) In general, do you think that models with more or fewer variables would perform better in this comparison? Explain. Exercise 4 The code below systematically looks at the same comparison that you made in Exercise 3 but for every possible linear regression model formed from inclusion/exclusion of the predictors (without transformations or interactions). See the plot below for the results. Note: You can try to run the code to make a plot of the results of this systematic investigation (it might take several minutes). Feel free to inspect the code if you’re curious, but otherwise, don’t worry about understanding it fully. What do you notice? What do you wonder? # This helper function computes MAE on the supplied datasets get_maes &lt;- function(mod, train_data, test_data) { mod_output_train &lt;- bind_cols(train_data, predict(mod, new_data = train_data)) mod_output_test &lt;- bind_cols(test_data, predict(mod, new_data = test_data)) train_mae &lt;- mod_output_train %&gt;% mae(truth = fatSiri, estimate = .pred) %&gt;% pull(.estimate) test_mae &lt;- mod_output_test %&gt;% mae(truth = fatSiri, estimate = .pred) %&gt;% pull(.estimate) c(train_mae, test_mae) } # Get just the possible PREDICTOR variables by taking all variables # and removing the fatSiri and hipin variables # hipin is hip circumference in inches (redundant with the hip variable) possible_predictors &lt;- setdiff(colnames(bodyfat), c(&quot;fatSiri&quot;, &quot;hipin&quot;)) # This code loops through all models (run time can be long) results &lt;- bind_rows(lapply(1:13, function(i) { combos &lt;- combn(possible_predictors, i) bind_rows(lapply(seq_len(ncol(combos)), function(j) { form &lt;- paste(&quot;fatSiri ~&quot;, paste(combos[,j], collapse = &quot;+&quot;)) mod &lt;- fit(lm_spec, as.formula(form), data = bodyfat) maes &lt;- get_maes(mod = mod, train_data = bodyfat, test_data = bodyfat_test) tibble( form = form, train_mae = maes[1], test_mae = maes[2], num_predictors = i ) })) })) # I save the results so that I don&#39;t have to rerun the code above save(results, file = &quot;allsubsets.rda&quot;) load(&quot;allsubsets.rda&quot;) # Relabel the categories (levels) of the num_predictors variable results &lt;- results %&gt;% mutate(num_predictors = factor(paste(&quot;# predictors:&quot;, num_predictors), levels = paste(&quot;# predictors:&quot;, 1:13))) # Plot results results %&gt;% ggplot(aes(x = train_mae, y = test_mae, color = num_predictors)) + geom_point() + coord_cartesian(xlim = c(2.5,7.5), ylim = c(2.5,7.5)) + geom_abline(slope = 1, intercept = 0) + facet_wrap(~ num_predictors) + guides(color = &quot;none&quot;) + labs(x = &quot;MAE on original data&quot;, y = &quot;MAE on new data on 172 men&quot;) + theme_classic() "],["overfitting.html", "Topic 3 Overfitting Learning Goals Exercises", " Topic 3 Overfitting Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Slides from today are available here. Exercises Exercise 1 Consider the following relationship. (From the code, we can tell that the true relationship between x and y is quadratic: \\(y=x^2\\) is the underlying relationship.) Imagine a model that is overfit to this training data. (You are not limited to lines.) Draw a picture of the predictions from this overfit model. Explain why your picture demonstrates overfitting: How does your model perform on the training data (the data displayed here)? If we received a new dataset from this same setting (test data), what would a plot of y vs. x look like? How would your model perform on this new test data? set.seed(123) data_train &lt;- tibble::tibble( x = runif(15,0,7), # Generate random numbers for predictor x y = x^2 + rnorm(15,sd = 7) # Generate y using y = x^2 + noise ) data_train %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + theme_classic() Exercise 2 Consider this xkcd comic on “modeling” the outcomes of US presidential elections. How is this comic related to overfitting? Check-in with groupmates What ideas have you found confusing, less clear, or intriguing based on videos and class exercises? Based on this, what would help improve your understanding or help you improve others’ understanding? You’ll reflect on these noticings in the Metacognitive Reflection part of your homework assignments. "],["cross-validation.html", "Topic 4 Cross-validation Learning Goals Exercises", " Topic 4 Cross-validation Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower # of folds in CV in terms of sample size and computing time Implement cross-validation in R using the tidymodels package Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat percentage dataset from before. Our goal is to build a good predictive model of body fat percentage from body circumference measurements. library(readr) library(ggplot2) library(dplyr) library(tidymodels) tidymodels_prefer() bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the extraneous outcome variables (fatBrozek and density) and the # redundant hip circumference variable (hipin = hip circ. in inches) bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Consider 4 models for body fat percentage: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) mod1 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- fit(lm_spec, fatSiri ~ ., # The . means all predictors data = bodyfat_train) Exercise 1: Cross-validation concepts Let’s conceptually step through exactly what our code will be doing to perform 10-fold cross-validation (CV) on the 80 cases in the training set. In your groups, take turns explaining these steps. Exercise 2: Cross-validation with tidymodels The code below performs 10-fold CV for mod1 to estimate the test RMSE (\\(\\text{CV}_{(10)}\\)). Do we need to use set.seed()? Why or why not? (Is there a number of folds for which we would not need to set the seed?) # Do we need to use set.seed()? bodyfat_cv &lt;- vfold_cv(bodyfat_train, v = 10) model_wf &lt;- workflow() %&gt;% add_formula(fatSiri ~ age+weight+neck+abdomen+thigh+forearm) %&gt;% add_model(lm_spec) mod1_cv &lt;- fit_resamples(model_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, mae, rsq) ) Explore mod1_cv %&gt;% unnest(.metrics). What information seems to be contained in this output, and how would you use this to calculate the 10-fold cross-validated RMSE by hand? (If you feel up to it, try using code to perform this calculation–use filter() and summarize().) mod1_cv %&gt;% unnest(.metrics) Check your manual calculation by directly printing out the CV metrics: mod1_cv %&gt;% collect_metrics(). Interpret this metric. mod1_cv %&gt;% collect_metrics() Exercise 3: Looking at the evaluation metrics Look at the completed table below of evaluation metrics for the 4 models. Which model performed the best on the training data? Which model performed best on test set (through CV)? Explain why there’s a discrepancy between these 2 answers and why CV, in general, can help prevent overfitting. Model Training RMSE \\(\\text{CV}_{(10)}\\) mod1 3.810712 4.389568 mod2 3.766645 4.438637 mod3 3.752362 4.517281 mod4 3.572299 4.543343 Exercise 4: Practical issues: choosing the number of folds In terms of sample size, what are the pros/cons of low vs. high number of folds? In terms of computational time, what are the pros/cons of low vs. high number of folds? If possible, it is advisable to choose the number of folds to be a divisor of the sample size. Why do you think that is? Digging deeper If you have time, consider the following questions to further explore concepts related to today’s ideas. Consider leave-one-out-cross-validation (LOOCV). Would we need set.seed()? Why or why not? How might you adapt the code above to implement this? Using the information from your_output %&gt;% unnest(.metrics) (which is a dataset), construct a visualization to examine the variability of RMSE from case to case. What might explain any very large values? What does this highlight about the quality of estimation of the LOOCV process? "],["variable-subset-selection.html", "Topic 5 Variable Subset Selection Learning Goals Exercises", " Topic 5 Variable Subset Selection Learning Goals Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time Describe how selection algorithms can give a measure of variable importance Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. library(readr) library(ggplot2) library(dplyr) library(tidymodels) tidymodels_prefer() bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove unneeded variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Exercise 1: Backward stepwise selection: by hand In the backward stepwise procedure, we start with the full model, full_model, with all predictors: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) full_model &lt;- fit(lm_spec, fatSiri ~ ., data = bodyfat_train) full_model %&gt;% tidy() %&gt;% arrange(desc(p.value)) To practice the backward selection algorithm, step through a few steps of the algorithm using p-values as a selection criterion: Identify which predictor contributes the least to the model. One (problematic) approach is to identify the least significant predictor (the one with the largest p-value). Fit a new model which eliminates this predictor. Identify the least significant predictor in this model. Fit a new model which eliminates this predictor. (Code note: to remove predictor X from the model, update the model formula to fatSiri ~ . - X.) Repeat 1 more time to get the hang of it. (We discussed in the video how the use of p-values for selection is problematic, but for now you’re just getting a handle on the algorithm. You’ll think about the problems with p-values in the next exercise.) Exercise 2: Interpreting the results Examine which predictors remain after the previous exercise. Are you surprised that, for example, wrist is still in the model but hip is not? Does this mean that wrist is a better predictor of body fat percentage than hip is? What statistical idea is relevant here? How would you determine which variables are the most important in predicting the outcome using the backwards algorithm? How about with forward selection? Exercise 3: Planning forward selection using CV Using p-values to perform stepwise selection presents some problems, as was discussed in the concept video. A better alternative to target predictive accuracy is to evaluate the models using cross-validation. Fully outline the steps required to use cross-validation to perform forward selection. Make sure to provide enough detail such that the stepwise selection and CV algorithms are made clear and could be implemented (no code, just describing the steps). Exercise 4: Practical considerations for variable subset selection Forward and backward selection provide computational shortcuts to the all (best) subsets approach. Let’s examine the computation requirements for these methods. Say that we have 5 predictors. How many models would be fit in all/best subsets selection? With forward/backward stepwise selection? Extra: Can we express the number of models that need to be fit for a general number of predictors \\(p\\)? The tidymodels package does not include a straightforward way to implement forward or backward selection because the authors of the package do not believe that it is a good technique for variable selection. (We’ll learn better approaches next.) List some reasons why these algorithms may not be encouraged for selecting variables to include in a model. Consider computational time and a situation where we have hundreds of variables, some of which may be collinear. "],["lasso-shrinkageregularization.html", "Topic 6 LASSO: Shrinkage/Regularization Learning Goals LASSO models in tidymodels Exercises", " Topic 6 LASSO: Shrinkage/Regularization Learning Goals Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain why variable scaling is important for the performance of shrinkage methods Explain how the lambda tuning parameter affects model performance and how this is related to overfitting Describe how output from LASSO models can give a measure of variable importance Slides from today are available here. LASSO models in tidymodels To build LASSO models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code to fit a LASSO model to a data set using several lambda values: # Lasso Model Spec lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% # mixture = 1 indicates LASSO set_engine(engine = &quot;glmnet&quot;) %&gt;% # note different engine than &quot;lm&quot; set_mode(&quot;regression&quot;) # Recipe with standardization (!) data_rec &lt;- recipe(___ ~ ___ , data = ___) %&gt;% # formula: outcome ~ predictors (generally predictors is . for all predictors) step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_novel(all_nominal_predictors()) %&gt;% # important if you have rare categorical variables step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for LASSO step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(data_rec) %&gt;% add_model(lm_lasso_spec_tune) # Create CV folds data_cv10 &lt;- vfold_cv(___, v = 10) # Tune Model (trying a variety of values of lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-5, 3)), # log10 transformed 10^-5 to 10^3 levels = 30 ) lasso_tune_out &lt;- tune_grid( # new function for tuning parameters lasso_wf_tune, # workflow resamples = data_cv10, # cv folds metrics = metric_set(rmse, mae), grid = penalty_grid # penalty grid defined above ) Examining the LASSO model for each \\(\\lambda\\) The glmnet engine fits models for each \\(\\lambda\\) automatically, so we can visualize the estimates for each penalty value. plot(lasso_fit %&gt;% extract_fit_parsnip() %&gt;% pluck(&#39;fit&#39;), # way to get the original glmnet output xvar = &quot;lambda&quot;) # glmnet fits the model with a variety of lambda penalty values Identifying the “best” LASSO model To identify the best model, we need to tune the model using cross validation. Adapt the following code to tune a Lasso Model to choose lambda: # Visualize model evaluation metrics from tuning autoplot(lasso_tune_out) + theme_classic() # Summarize model evaluation metrics collect_metrics(lasso_tune_out) %&gt;% filter(.metric == &quot;mae&quot;) %&gt;% # or choose rmse select(penalty, rmse = mean) # Choose penalty value based on lowest mae (or choose rmse) best_penalty &lt;- select_best(lasso_tune_out, metric = &quot;mae&quot;) # Choose largest penalty value within 1 SE of the lowest CV MAE best_se_penalty &lt;- select_by_one_std_err(lasso_tune_out, metric = &quot;mae&quot;, desc(penalty)) # Fit final model final_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow (or replace best_penalty with best_se_penalty) final_fit &lt;- fit(final_wf, data = ___) tidy(final_fit) Exercises You can download a template RMarkdown file to start from here. We’ll use a new data set to explore LASSO modeling. This data comes from the US Department of Energy. You will predict the fuel efficiency of modern cars from characteristics of these cars, like transmission and engine displacement. Fuel efficiency is a numeric value that ranges smoothly from about 15 to 40 miles per gallon. library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(123) cars2018 &lt;- read_csv(&quot;https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/cars2018.csv&quot;) head(cars2018) # Cleaning cars2018 &lt;- cars2018 %&gt;% select(-model_index) Exercise 1: A least squares model Let’s start by building an ordinary (not penalized) least squares model to review important concepts. We’ll fit a model to predict fuel efficiency measured in miles per gallon (mpg) with all possible predictors. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) full_rec &lt;- recipe(mpg ~ ., data = cars2018) %&gt;% update_role(model, new_role = &quot;ID&quot;) %&gt;% # we want to keep the name of the car model but not as a predictor or outcome step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for LASSO step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables full_lm_wf &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% add_model(lm_spec) full_model &lt;- fit(full_lm_wf, data = cars2018) full_model %&gt;% tidy() Use tidymodels to perform 10-fold cross-validation to estimate test MAE for this model. (The code below comes from our body fat modeling previously. Adapt it for our context here.) # Do we need to use set.seed()? bodyfat_cv &lt;- vfold_cv(bodyfat_train, v = 10) mod1_cv &lt;- fit_resamples(model_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, mae) ) How do you think the estimated test error would change with fewer predictors? This model fit with ordinary least squares corresponds to a special case of penalized least squares. What is the value of \\(\\lambda\\) in this special case? As \\(\\lambda\\) increases, what would you expect to happen to the number of predictors that remain in the model? Notice that our data preprocessing recipe (full_rec) contained a step to normalize all numeric predictors (step_normalize(all_numeric_predictors())). Why is this an important step for LASSO? Do you think that this is an important step for ordinary linear regression? (Hint: think about two models for body weight–one with height in inches as a predictor and one with height in feet as a predictor. Do the predictions from these two models differ?) Exercise 2: Fitting a LASSO model in tidymodels The code below (and in part d) fits a set of LASSO models with the following parameters: Use 10-fold CV. Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of 30 \\(\\lambda\\) values from 0.001 to 10. Before running the code, run install.packages(\"glmnet\") in the Console. set.seed(74) # Create CV folds data_cv10 &lt;- vfold_cv(cars2018, v = 10) # LASSO model specification where then `penalty` parameter needs to be tuned lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = &quot;glmnet&quot;) %&gt;% # note we are using a different engine set_mode(&quot;regression&quot;) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% # recipe defined above add_model(lm_lasso_spec_tune) # Tuning the model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-3, 1)), #log10 transformed levels = 30) tune_output &lt;- tune_grid( # new function for tuning parameters lasso_wf_tune, # workflow resamples = data_cv10, # CV folds metrics = metric_set(rmse, mae), grid = penalty_grid # penalty grid defined above ) Let’s visualize the model evaluation metrics from tuning. We can use autoplot(). # Visualize Model Evaluation Metrics from Tuning autoplot(tune_output) + theme_classic() # Zoomed-in version to focus on curvature of MAE plot autoplot(tune_output) + theme_classic() + coord_cartesian(ylim = c(1.96, 2.01)) Inspect the shape of the plot. The errors go down at the very beginning then start going back up. Based on this, what are the consequences of picking a \\(\\lambda\\) that is too small or too large? (This is an example of a very important idea that we’ll see shortly: the bias-variance tradeoff.) Next, we need to choose the lambda that leads to the best model. We can choose the lambda penalty value that leads to the lowest CV MAE, or we can take into account the variation of the CV MAE and choose the largest lambda penalty value that is within 1 standard error of the lowest CV MAE. How might the models that result from these two penalties differ? best_penalty &lt;- select_best(tune_output, metric = &quot;mae&quot;) # choose penalty value based on lowest CV MAE best_penalty best_se_penalty &lt;- select_by_one_std_err(tune_output, metric = &quot;mae&quot;, desc(penalty)) # choose largest penalty value within 1 se of the lowest CV MAE best_se_penalty Now check your understanding by fitting both “final” models and comparing the coefficients. How are these two models different? # Fit Final Model final_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow final_wf_se &lt;- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow final_fit &lt;- fit(final_wf, data = cars2018) final_fit_se &lt;- fit(final_wf_se, data = cars2018) tidy(final_fit) tidy(final_fit_se) Going forward, we’ll examine output from the model chosen by select_by_one_std_err() (final_fit_se). Exercise 3: Examining output: plot of coefficient paths A useful plot allows us to examine coefficient paths resulting from the final fitted LASSO models: coefficient estimates as a function of \\(\\lambda\\). glmnet_output &lt;- final_fit_se %&gt;% extract_fit_parsnip() %&gt;% pluck(&quot;fit&quot;) # get the original glmnet output # Plot coefficient paths as a function of lambda plot(glmnet_output, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20)) # Codebook for which variables the numbers correspond to rownames(glmnet_output$beta) # e.g., What are variables 2 and 4? rownames(glmnet_output$beta)[c(2,4)] There’s a lot of information in this plot! Each colored line corresponds to a different predictor. (Note that categorical variables have been split into different predictors via indicator variable creation.) The small number to the left of each line indicates a predictor by its position in rownames(glmnet_output$beta). The x-axis reflects the range of different \\(\\lambda\\) values (on the log-scale). At each \\(\\lambda\\), the y-axis reflects the coefficient estimates for the predictors in the corresponding LASSO model. At each \\(\\lambda\\), the numbers at the top of the plot indicate how many predictors remain in the corresponding model. Consider the coefficient estimates at the smallest value of \\(\\lambda\\). How closely should they correspond to the coefficient estimates from ordinary least squares in exercise 1? Why do all of the lines head toward y = 0 on the far right of the plot? What variables seem to be more “important” or “persistent” (persistently present in the model) variable? Does this make sense in context? In general, how might we use these “coefficient paths” to measure the relative importance of our predictors? Note: If you’re curious about code to automate this visual inspection of variable importance, look at the Digging Deeper exercise at the end. Exercise 4: Examining and evaluating the best LASSO model Take a look at the predictors and coefficients for the “best” LASSO model. Are the predictors that remain in the model sensible? Do the coefficient signs make sense? # Obtain the predictors and coefficients of the &quot;best&quot; model # Filter out the coefficient are 0 final_fit_se %&gt;% tidy() %&gt;% filter(estimate != 0) Evaluate the best LASSO model: Contextually interpret (with units) the CV MAE error for the best model. Make residual plots for the model by creating a dataset called lasso_mod_out which contains the original data as well as predicted values and residuals (.pred and resid). # Evaluation metrics tune_output %&gt;% collect_metrics() %&gt;% filter(penalty == (best_se_penalty %&gt;% pull(penalty))) # Residual plots lasso_mod_out &lt;- final_fit_se %&gt;% predict(new_data = cars2018) %&gt;% bind_cols(cars2018) %&gt;% mutate(resid = mpg - .pred) Digging deeper These exercises are recommended for further exploring code useful in an applied analysis. We used the plot of coefficient paths to evaluate the variable importance of our predictors. The code below does this systematically for each predictor so that we don’t have to eyeball. Step through and work out what each part is doing. It may help to look up function documentation with ?function_name in the Console. # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- glmnet_output$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { this_coeff_path &lt;- bool_predictor_exclude[row,] if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{ return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)} }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) If you want more practice, the Hitters data in the ISLR package (be sure to to install and load) contains the salaries and performance measures for 322 Major League Baseball players. Use LASSO to determine the “best” predictive model of Salary. "],["knn-regression-and-the-bias-variance-tradeoff.html", "Topic 7 KNN Regression and the Bias-Variance Tradeoff Learning Goals KNN models in tidymodels Exercises", " Topic 7 KNN Regression and the Bias-Variance Tradeoff Learning Goals Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN Slides from today are available here. KNN models in tidymodels To build KNN models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: # CV Folds data_cv10 &lt;- vfold_cv(___, v = 10) # Model Specification knn_spec &lt;- nearest_neighbor() %&gt;% # new type of model! set_args(neighbors = tune()) %&gt;% # tuning parameter is neighbor; tuning spec set_engine(engine = &quot;kknn&quot;) %&gt;% # new engine set_mode(&quot;regression&quot;) # Recipe with standardization (!) data_rec &lt;- recipe( ___ ~ ___ , data = ___) %&gt;% step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_novel(all_nominal_predictors()) %&gt;% # important if you have rare categorical variables step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for KNN step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables (important for KNN!) # Workflow (Recipe + Model) knn_wf &lt;- workflow() %&gt;% add_model(knn_spec) %&gt;% add_recipe(data_rec) # Tune model trying a variety of values for neighbors (using 10-fold CV) neighbors_grid &lt;- grid_regular( neighbors(range = c(1, 50)), # min and max of values for neighbors levels = 50 # number of neighbors values ) knn_fit_cv &lt;- tune_grid( knn_wf, # workflow resamples = data_cv10, # CV folds grid = neighbors_grid, # grid specified above metrics = metric_set(rmse, mae) ) Note: tidymodels defines neighbors as the cases that are the closest in terms of the Euclidean distance of the predictor values: \\[ d(case_i,case_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\cdots +(x_{ip} - x_{jp})^2 } \\] Identifying the “best” KNN model The “best” model in the sequence of models fit is defined relative to the chosen metric and the choice of select_best() or select_by_one_std_err(). knn_fit_cv %&gt;% autoplot() # Visualize Trained Model using CV knn_fit_cv %&gt;% show_best(metric = &quot;mae&quot;) # Show evaluation metrics for different values of neighbors, ordered # Choose value of Tuning Parameter (neighbors) tuned_knn_wf &lt;- knn_fit_cv %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(neighbors)) %&gt;% # Choose neighbors value that leads to the highest neighbors within 1 se of the lowest CV MAE finalize_workflow(knn_wf, .) # Fit final KNN model to data knn_fit_final &lt;- tuned_knn_wf %&gt;% fit(data = ___) # Use the best model to make predictions # new_data should be a data.frame with required predictors predict(knn_fit_final, new_data = ___) Exercises You can download a template RMarkdown file to start from here. We’ll explore KNN regression using the College dataset in the ISLR2 package (install it with install.packages(\"ISLR2\") in the Console). You can use ?College in the Console to look at the data codebook. library(ISLR2) library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions data(College) # Data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% # creates variable with school name filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Bias-variance tradeoff warmup Think back to the LASSO algorithm which depends upon tuning parameter \\(\\lambda\\). For which values of \\(\\lambda\\) (small or large) will LASSO be the most biased, and why? For which values of \\(\\lambda\\) (small or large) will LASSO be the most variable, and why? The bias-variance tradeoff also comes into play when comparing across algorithms, not just within algorithms. Consider LASSO vs. least squares: Which will tend to be more biased? Which will tend to be more variable? When will LASSO outperform least squares in the bias-variance tradeoff? Exercise 2: Impact of variable scale and distance measure Consider the 1-nearest neighbor algorithm to predict Grad.Rate on the basis of two predictors: Apps and Private. Let Yes for Private be represented with the value 1 and No with 0. We have a test case whose number of applications is 13,530 and is a private school. Suppose that we have the tiny 2-case training set below. What would the 1-nearest neighbor prediction be using Euclidean distance? college_clean %&gt;% filter(school %in% c(&quot;Princeton University&quot;, &quot;SUNY at Albany&quot;)) %&gt;% select(Apps, Private, Grad.Rate, school) sqrt( (13530 - ?)^2 + (1 - ?)^2) # Euclidean distance between test case and Princeton sqrt( (13530 - ?)^2 + (1 - ?)^2) # Euclidean distance between test case and SUNY Do you have any concerns about the resulting prediction? Based on this, comment on the impact of variable scaling and the distance measure on KNN performance. How might you change the distance calculation (or correspondingly rescale the data) to generate a more sensible prediction in this situation? Exercise 3: Implementing KNN in tidymodels Before continuing, install the kknn package by entering install.packages(\"kknn\") in the Console. We will step-by-step write code to “fit” a set of KNN models to predict Grad.Rate with the following specifications: Use the predictors Private, Top10perc (% of new students from top 10% of high school class), and S.F.Ratio (student/faculty ratio). Use 8-fold CV. (Why 8? Take a look at the sample size.) Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of neighbor values from 1 to 100 in increments of 5 (20 values in total). Step 1: Describe the model that we want to fit and how. (Describe it’s specification.) The general model type we are using is called nearest_neighbor. The KNN model has a parameter called neighbors (the number of nearest neighbors used to make predictions). The “engine” used to build the model is called \"kknn\". We using KNN for a regression task (quantitative outcome). (Nearly all of our methods can be used in both a regression and classification setting.) knn_spec &lt;- ___() %&gt;% # Insert the name of the general model type set_args(___ = tune()) %&gt;% # Insert the name of the parameter that we will tune set_engine(engine = ___) %&gt;% # Insert the engine name (in quotes) set_mode() # Indicate &quot;regression&quot; or &quot;classification&quot; Step 2: Divide data into folds for cross-validation. Use 8-fold CV. (Why 8? Take a look at the sample size with dim(college_clean) or nrow(college_clean).) set.seed(2023) # Why do we need this? college_cv &lt;- vfold_cv(___, v = ___) # Supply dataset and # of folds Step 3: Create our data preprocessing recipe. We first have to specify the outcome and predictors. We’re predicting Grad.Rate (graduation rate). Use the predictors Private, Top10perc (% of new students from top 10% of high school class), and S.F.Ratio (student/faculty ratio). Include step_dummy(all_nominal_predictors()). Because KNN needs to compute distances between cases, all predictors should be in numeric form. We can convert categorical variables (all_nominal_predictors()) to indicator (dummy) variables. Include step_normalize(all_numeric_predictors()). Why is this important based on Exercise 2? college_rec &lt;- recipe(___ ~ ___, data = ___) %&gt;% # Outcome, predictors, and dataset step_???() %&gt;% step_???() Step 4: Define our analysis workflow: our model specification (Step 1) and our data preprocessing recipe (Step 3). college_wf &lt;- workflow() %&gt;% add_model(___) %&gt;% # Model specification object add_recipe(___) # Data preprocessing recipe object Step 5: Set up “grid” of tuning parameters, and fit models for each tuning parameter value to find optimal value. Use 20 values between 1 and 100 for the number of neighbors. Compute rmse and mae in CV iterations. tuning_param_grid &lt;- grid_regular( neighbors(range = c(___, ___)), # min and max of values for neighbors levels = ___ # number of neighbors values ) knn_fit_cv &lt;- tune_grid( ___, # workflow object resamples = ___, # CV folds object grid = ___, # tuning parameter grid object metrics = metric_set(___, ___) # evaluation metric names (no quotes) ) After adapting the code (but before inspecting any output, which will happen in the next exercise), answer the following conceptual questions: Explain your choice for your recipe. Does KNN actually “fit” a model as part of training? (This feature of KNN is known as “lazy learning.”) How is test MAE estimated? What are the steps of the KNN algorithm with cross-validation? Draw a picture of how you expect test MAE to vary with \\(K\\), the number of neighbors. In terms of the bias-variance tradeoff, why do you expect the plot to look this way? Exercise 4: Inspecting the results The code below allows us to inspect our results. (It’s complete–nothing to fill in, but it’s helpful to look back at our LASSO code to see the similarities.) Use autoplot() to verify your expectations about the plot of test MAE vs. \\(K\\), the number of neighbors. Contextually interpret the test MAE for the “best” model. (There are two versions of best shown–what is the difference between them?) How else could you evaluate the KNN model? Does your KNN model help you understand which predictors of graduation rate are most important? Why or why not? autoplot(knn_fit_cv) + theme_classic() knn_fit_cv %&gt;% show_best(metric = &quot;mae&quot;) # Show evaluation metrics for different values of neighbors, ordered # Choose value of tuning parameter (# of neighbors) ## Overall lowest error knn_fit_cv %&gt;% select_best(metric = &quot;mae&quot;) ## Choose neighbors value that leads to the highest neighbors within 1 std. err. of the lowest CV MAE knn_fit_cv %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(neighbors)) ## The desc(neighbors) sorts the data from highest to lowest # of neighbors (most simple -&gt; most complex) Extra: Curse of dimensionality Just as with parametric models, we could keep going and add more and more predictors. However, the KNN algorithm is known to suffer from the “curse of dimensionality.” Explore this idea via the following resources: https://youtu.be/4v7ngaiFdp4 https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality "],["catch-up-day.html", "Topic 8 Catch-up Day Goals Building a tidymodels reference sheet", " Topic 8 Catch-up Day Slides from today are available here. Goals Check in with peers and instructor about conceptual questions Check in with instructor about project dataset Organize code we’ve encountered so far into a reference sheet Building a tidymodels reference sheet We’ve encountered a lot of tidymodels functions so far. Let’s try to build a reference sheet where we organize what functions are used at what point in an analysis. Revisit our topic pages for LASSO and KNN, and take a look at the “LASSO models in tidymodels” and “KNN models in tidymodels” sections to see how the tidymodels functions below are used. It may help to make a flow diagram indicating the order in which functions are generally run and what function outputs serve as inputs to other functions. It may help to insert screenshots of what function output looks like. tidymodels functions For specifying the model we want to fit and how: linear_reg() nearest_neighbor() set_args() tune() set_engine() set_mode() For CV: vfold_cv() For data preprocessing (recipes): recipe() step_normalize() step_dummy() all_predictors() all_nominal_predictors() all_numeric_predictors() Defining a modeling workflow: workflow() add_model() add_recipe() Tuning over a parameter grid: grid_regular() neighbors() penalty() tune_grid() metric_set() rmse() mae() Inspecting results: autoplot() collect_metrics() show_best() select_best() select_by_one_std_err() Using “best” parameters to fit the model to the full training data: finalize_workflow() fit() "],["splines.html", "Topic 9 Splines Learning Goals Splines in tidymodels Exercises", " Topic 9 Splines Learning Goals Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Slides from today are available here. Splines in tidymodels To build models with splines in tidymodels, we proceed with the same structure as we use for ordinary linear regression models but we’ll add some pre-processing steps to our recipe. To work with splines, we’ll use tools from the splines package. The ns() function in the splines package implements the variable transformations needed to create a natural cubic spline function for a quantitative predictor. The step_ns() function in tidymodels is an interface to ns(). # Linear regression model specification lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) # Recipe: linear regression model lm_rec &lt;- recipe(___ ~ ___, data = ___) # Recipe: linear regression model with natural splines ns_rec &lt;- lm_rec %&gt;% step_ns(__, deg_free = __) # natural cubic spline for a given predictor (higher deg_free means more knots) The deg_free argument in step_ns() stands for degrees of freedom: deg_free = # knots + 1 The degrees of freedom are the number of coefficients in the transformation functions that are free to vary (essentially the number of underlying parameters behind the transformations). The knots are chosen using percentiles of the observed values. Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the splines package by entering install.packages(\"splines\") in the Console. We’ll continue using the College dataset in the ISLR2 package to explore splines. You can use ?College in the Console to look at the data codebook. library(ISLR2) library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions data(College) # Data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% # creates variable with school name filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Evaluating a fully linear model We will model Grad.Rate as a function of 4 predictors: Private, Terminal, Expend, and S.F.Ratio. Make scatterplots of the quantitative predictors and the outcome with 2 different smoothing lines to explore potential nonlinearity. Adapt the following code to create a scatterplot with a smooth (curved) blue trend line and a red linear trend line. ggplot(___, aes(___)) + geom_point() + geom_smooth(color = &quot;blue&quot;, se = FALSE) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + theme_classic() Use tidymodels to fit a LASSO model (no splines yet) with the following specifications: Use 8-fold CV. Just use the 4 predictors Private, Terminal, Expend, and S.F.Ratio. Use CV mean absolute error (MAE) to evaluate models. Use the LASSO engine (\"glmnet\") to do variable selection to select the simplest model for which the metric is within one standard error of the best metric. Fit your “best” model and look at coefficients of that final model. set.seed(___) # Create CV folds data_cv8 &lt;- vfold_cv(___, v = ___) # Lasso Model Spec with tune lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = ___) %&gt;% set_mode(&quot;regression&quot;) # Recipe full_rec &lt;- recipe(___ ~ ___, data = college_clean) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% add_model(lm_lasso_spec_tune) # Tune Model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-3, 1)), # log10 transformed levels = 30 ) tune_output &lt;- tune_grid( lasso_wf_tune, # workflow resamples = data_cv8, # cv folds metrics = metric_set(___), grid = penalty_grid # penalty grid defined above ) # Select best model &amp; fit best_penalty &lt;- tune_output %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(penalty)) lasso_mod &lt;- finalize_workflow(lasso_wf_tune, best_penalty) %&gt;% fit(data = college_clean) # Note which variable is the &quot;least&quot; important lasso_mod %&gt;% tidy() Make plots of the residuals vs. the 3 quantitative predictors to evaluate the appropriateness of linear terms. lasso_mod_output &lt;- college_clean %&gt;% bind_cols(predict(lasso_mod, new_data = college_clean)) %&gt;% mutate(resid = ___ - ___) ggplot(lasso_mod_output, aes(___)) + ___ + ___ + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() Exercise 2: Evaluating a spline model We’ll extend our best linear regression model with spline functions of the quantitative predictors (leave Private as is). What tuning parameter is associated with splines? How do high/low values of this parameter relate to bias and variance? Update your recipe from Exercise 1 to fit a linear model (with the lm engine rather than LASSO) with the 2 best quantitative predictors with natural splines that have 2 knots (= 3 degrees of freedom) and include Private. Fit this model with CV, fit_resamples, (same folds as before) to compare MAE and then fit the model to the whole training data. Call this fit model ns_mod. # Model Spec lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) # New Recipe (remove steps needed for LASSO, add splines) spline_rec &lt;- recipe(___ ~ ___, data = ___) %&gt;% step___() # INSERT_ONE_OR_MORE_steps_here # Workflow (Recipe + Model) spline_wf &lt;- workflow() %&gt;% add_???() %&gt;% add_???() # CV to Evaluate cv_output &lt;- fit_resamples( ___, # workflow resamples = data_cv8, # cv folds metrics = metric_set(___) ) cv_output %&gt;% collect_metrics() # Fit with all data ns_mod &lt;- fit( ___, # workflow data = college_clean ) Make plots of the residuals vs. the 3 quantitative predictors to evaluate if splines improved the model. spline_mod_output &lt;- ___ # Residual plots Compare the CV MAE between models with and without the splines. tune_output %&gt;% collect_metrics() %&gt;% filter(penalty == (best_penalty %&gt;% pull(penalty))) cv_output %&gt;% collect_metrics() Extra! Variable scaling What is your intuition about whether variable scaling matters for the performance of splines? Check you intuition by reusing code from Exercise 2, except by adding in step_normalize(all_numeric_predictors()) before step_ns(). Call this ns_mod2. How do the predictions from ns_mod and ns_mod2 compare? You could use a plot to compare or check out the all.equal() function. all.equal(spline_mod_output$.pred, spline_mod_output2$.pred) plot(spline_mod_output$.pred, spline_mod_output2$.pred) "],["local-regression-gams.html", "Topic 10 Local Regression &amp; GAMs Learning Goals GAMs - Options for Fitting Exercises", " Topic 10 Local Regression &amp; GAMs Learning Goals Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Slides from today are available here. GAMs - Options for Fitting GAMs (splines + OLS) We’ve already talked about how to fit GAM models with splines (step_ns()) using the lm engine (ordinary least squares). GAMs (LOESS) The gam package provides tools for building GAMs with local regression (LOESS). We won’t explore this option further (via code) in class because there isn’t a tidymodels interface. GAMs (smoothing splines) in tidymodels Today, we’ll try fitting GAM models with smoothing splines in tidymodels. To build GAMs (using smoothing splines) in tidymodels, first load the package: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions Then adapt the following code: gam_spec &lt;- gen_additive_mod() %&gt;% set_args(select_features = TRUE, adjust_deg_free = tune()) %&gt;% set_engine(engine = &quot;mgcv&quot;) %&gt;% set_mode(&quot;regression&quot;) # The implementation of GAMs in tidymodels is different than other methods... # ...we don&#39;t specify a recipe gam_wf &lt;- workflow() %&gt;% add_variables(outcomes = YOUR_OUTCOME, predictors = c(PREDICTOR1, PREDICTOR2)) %&gt;% add_model(gam_spec, formula = YOUR_OUTCOME ~ CATEGORICAL_PREDICTORS + s(QUANTITATIVE_PREDICTOR, k=10)) # s(x1, k = 10): This makes the smoothing spline have 10 knots tuning_param_grid &lt;- grid_regular( adjust_deg_free(range = c(0.25, 4)), levels = 10 ) data_cv &lt;- vfold_cv(___, v = 10) tune_output &lt;- tune_grid( gam_wf, resamples = data_cv, metrics = metric_set(mae), grid = tuning_param_grid ) Picking the best tuning parameter and visualizing the GAM # Select best model &amp; fit to full training data best_param &lt;- tune_output %&gt;% select_best() best_by_1se_param &lt;- tune_output %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(adjust_deg_free)) gam_mod_best &lt;- finalize_workflow(gam_wf, best_param) %&gt;% fit(data = ___) gam_mod_best1se &lt;- finalize_workflow(gam_wf, best_by_1se_param) %&gt;% fit(data = ___) # Plot functions for each predictor # Dashed lines are +/- 2 SEs fit_gam_model %&gt;% pluck(&quot;fit&quot;) %&gt;% plot() Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the mgcv package by entering install.packages(\"mgcv\") in the Console. We’ll continue using the College dataset in the ISLR2 package to explore models for graduation rate. You can use ?College in the Console to look at the data codebook. library(ISLR2) library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Conceptual warmup How does high/low span relate to bias and variance of a local regression (LOESS) model? How does high/low lambda relate to bias and variance of a smoothing spline in a GAM model? Do you think that a GAM with all possible predictors will have better or worse performance than an ordinary (fully linear) least squares model with all possible predictors? Explain your thoughts. Why might we want to perform variable selection before fitting a GAM? How could stepwise selection or LASSO help with this? Exercise 2: Local regression (LOESS) Use LOESS (geom_smooth()) to explore the relationship between Apps and Grad.Rate for different values of span (between 0 and 1). How is varying span like varying number of neighbors in KNN? How would you describe the relationship between number of applications and graduation rate? college_clean %&gt;% ggplot(aes(x = Apps, y = Grad.Rate)) + geom_point(alpha = 0.2) + # Adjust point transparency with alpha geom_smooth(span = 0.4, method = &quot;loess&quot;, se = FALSE) + # vary span xlim(c(0,20000)) + theme_classic() Exercise 3: Building a GAM in tidymodels Suppose that initial variable selection investigations have given us a set of predictors to include in our GAM. The code below for building a GAM is complete (nothing to fill in), but before looking at output, we are going to closely examine this code in comparison to the code for our previous methods: LASSO and KNN. The adjust_deg_free argument is like lambda in LASSO. Higher values of this tuning parameter mean that “wiggliness” is penalized more. The select_features = TRUE allows for the ability to eliminate a predictor via penalization (more likely with higher adjust_deg_free). set.seed(123) gam_spec &lt;- gen_additive_mod() %&gt;% set_args(select_features = TRUE, adjust_deg_free = tune()) %&gt;% set_engine(engine = &quot;mgcv&quot;) %&gt;% set_mode(&quot;regression&quot;) # The implementation of GAMs in tidymodels is different than other methods... # ...we don&#39;t specify a recipe gam_wf &lt;- workflow() %&gt;% add_variables(outcomes = Grad.Rate, predictors = c(Private, Apps, Top10perc, P.Undergrad, Outstate, perc.alumni)) %&gt;% add_model(gam_spec, formula = Grad.Rate ~ Private + s(Apps, k=10) + s(Top10perc, k=10) + s(P.Undergrad, k=10) + s(Outstate, k=10) + s(perc.alumni, k=10)) tuning_param_grid &lt;- grid_regular( adjust_deg_free(range = c(0.25, 4)), levels = 10 ) data_cv8 &lt;- vfold_cv(college_clean, v = 8) # This takes a few seconds tune_output &lt;- tune_grid( gam_wf, resamples = data_cv8, metrics = metric_set(mae), grid = tuning_param_grid ) We can take a look at the test error metrics from CV and choose an optimal tuning parameter: tune_output %&gt;% collect_metrics() tune_output %&gt;% show_best() autoplot(tune_output) + theme_classic() # Select best model &amp; fit to full training data best_param &lt;- tune_output %&gt;% select_best() best_by_1se_param &lt;- tune_output %&gt;% select_by_one_std_err(metric = &quot;mae&quot;, desc(adjust_deg_free)) gam_mod_best &lt;- finalize_workflow(gam_wf, best_param) %&gt;% fit(data = college_clean) gam_mod_best1se &lt;- finalize_workflow(gam_wf, best_by_1se_param) %&gt;% fit(data = college_clean) Let’s visualize the estimated nonlinear functions. What about these plots indicates that using a GAM instead of ordinary linear regression was probably a good choice? Compare the plots resulting from the two different choices for “best” tuning parameter. Which choice would you prefer and why? For each of the plots, write a sentence describing what you learn about the relationship between graduation rate and that predictor. # Plot the estimated nonlinear functions for all predictors... # ...for the GAM resulting from the adjust_deg_free parameter value that gave the lowest error gam_mod_best$fit$fit$fit %&gt;% plot(all.terms = TRUE, pages = 1) # ...for the GAM resulting from the adjust_deg_free parameter value that gave the lowest error gam_mod_best1se$fit$fit$fit %&gt;% plot(all.terms = TRUE, pages = 1) "],["synthesis-regression.html", "Topic 11 Synthesis: Regression Exercises", " Topic 11 Synthesis: Regression Slides from today are available here. Exercises The topics in the regression unit of our course have been divided into variable selection methods and nonlinear modeling methods. What is the rationale for this breakdown? That is, why are LASSO and subset selection better for selecting useful variables than GAMs? Why don’t LASSO and subset selection automatically handle nonlinearity? Thus why is it useful to add KNN, splines, local regression, and GAMs to our toolbox? Suppose we fit a fully linear model (linear relationships for all predictors) - how would we use residual plots to assess the need for nonlinear transformations? Suppose that we have already selected important predictors and want to fit a GAM. Let’s consider a GAM built with splines. What are the steps of cross-validation to choose the “best” number of knots? (For simplicity, assume that all quantitative predictors will have the same number of knots.) GAMs can also be constructed with local regression. What steps are involved in tuning a GAM built with local regression? In terms of bias and variance, why does an underfit/overfit model have poor test performance? In terms of bias and variance, what is the rationale for using the select_by_one_std_err() function for choosing an optimal tuning parameter, as opposed to select_best()? Look back at the themes that we focused on in our KNN, splines, and local regression + GAMs class activities - what comes up as being most important? What questions do you have about these activities? "],["logistic-regression.html", "Topic 12 Logistic Regression Learning Goals Logistic regression in tidymodels Exercises", " Topic 12 Logistic Regression Learning Goals Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Slides from today are available here. Logistic regression in tidymodels To build logistic regression models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(ggplot2) library(tidymodels) library(probably) # install.packages(&quot;probably&quot;) tidymodels_prefer() set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code to fit a logistic regression model: # Make sure you set reference level (the outcome you are NOT interested in) data &lt;- data %&gt;% mutate(outcome = relevel(outcome, ref = &quot;failure&quot;)) # set reference level data_cv &lt;- vfold_cv(data, v = 10) # Logistic Regression Model Spec logistic_spec &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) # Recipe logistic_rec &lt;- recipe(outcome ~ ., data = data) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) # Workflow (Recipe + Model) logistic_wf &lt;- workflow() %&gt;% add_recipe(logistic_rec) %&gt;% add_model(logistic_spec) # Fit Model to Training Data logistic_fit &lt;- fit(logistic_wf, data = data) Examining the logistic model # Display coefficient estimates logistic_fit %&gt;% tidy() # Get exponentiated coefficients and confidence intervals logistic_fit %&gt;% tidy() %&gt;% mutate( OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error) ) %&gt;% mutate(OR = exp(estimate)) Making predictions from the logistic model # Make soft (probability) predictions predict(logistic_fit, new_data = ___, type = &quot;prob&quot;) # Make hard (class) predictions (using a default 0.5 probability threshold) predict(logistic_fit, new_data = ___, type = &quot;class&quot;) Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a spam dataset that contains information on different features of emails and whether or not the email was spam. The variables are as follows: spam: Either spam or not spam word_freq_WORD: percentage of words in the e-mail that match WORD (0-100) char_freq_CHAR: percentage of characters in the e-mail that match CHAR (e.g., exclamation points, dollar signs) capital_run_length_average: average length of uninterrupted sequences of capital letters capital_run_length_longest: length of longest uninterrupted sequence of capital letters capital_run_length_total: sum of length of uninterrupted sequences of capital letters Our goal will be to use email features to predict whether or not an email is spam - essentially, to build a spam filter! library(dplyr) library(readr) library(ggplot2) library(tidymodels) tidymodels_prefer() spam &lt;- read_csv(&quot;https://www.dropbox.com/s/leurr6a30f4l32a/spambase.csv?dl=1&quot;) # A little data cleaning to remove the space in &quot;not spam&quot; spam &lt;- spam %&gt;% mutate(spam = ifelse(spam==&quot;spam&quot;, &quot;spam&quot;, &quot;not_spam&quot;)) Exercise 1: Implementing LASSO logistic regression in tidymodels Our goal is to fit a logistic regression model with word_freq_george and char_freq_exclam as predictors. Write down the corresponding logistic regression model formula using mathematical notation. Use tidymodels to fit this logistic regression model to the training data. Let’s try to do this from scratch (almost). Open up this tidymodels note sheet, and we’ll work through the thought process piece by piece. Work with your group to figure out what phase of the analysis is happening in each row. What do you think needs to be modified to implement logistic regression? (For now, we’re just fitting a model with a fixed set of predictors–not trying to estimate test performance with CV.) Key changes for implementing logistic regression: the model name is logistic_reg(), the model-building engine is \"glm\", and we are now performing \"classification\" rather than \"regression\" # Need to set reference level (to the outcome you are NOT interested in) spam &lt;- spam %&gt;% mutate(spam = relevel(factor(spam), ref=&quot;not_spam&quot;)) # Logistic regression model specification logistic_spec &lt;- # Recipe logistic_rec &lt;- # Workflow (Recipe + Model) log_wf &lt;- # Fit Model log_fit &lt;- fit(log_wf, data = spam) Exercise 3: Interpreting the model Take a look at the log-scale coefficients with tidy(log_fit). Do the signs of the coefficients for the 2 predictors agree with your visual inspection from Exercise 1? Display the exponentiated coefficients, and provide contextual interpretations for them (not the intercept). (Use the output of tidy() with mutate() and exp().) Exercise 4: Making predictions Consider a new email where the frequency of “George” is 0.25% and the frequency of exclamation points is 1%. Use the model summary to make both a soft (probability) and hard (class) prediction for this test case by hand. Use a default probability threshold of 0.5. (You can use math expressions to use R as a calculator. The exp() function exponentiates a number.) Check your work from part a by using predict(). predict(log_fit, new_data = data.frame(word_freq_george = 0.25, char_freq_exclam = 1), type = &quot;prob&quot;) predict(log_fit, new_data = data.frame(word_freq_george = 0.25, char_freq_exclam = 1), type = &quot;class&quot;) "],["evaluating-classification-models-part-1.html", "Topic 13 Evaluating Classification Models (Part 1) Learning Goals Exercises", " Topic 13 Evaluating Classification Models (Part 1) Learning Goals Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Slides from today are available here. Exercises No R work today - exercises are just conceptual. Exercise 1: Revisiting LASSO LASSO for the logistic regression setting works analogously to the regression setting in terms of its penalization of predictors. What would you expect about the number of predictors in the model for high vs. low lambda? Suppose that a LASSO logistic model has been built. Describe the steps required to make a soft (probability) and hard (class) prediction for a test case. Why should we not solely rely on the default probability threshold of 0.5 in making predictions? Thus what is the motivation for using the ROC_AUC metric? (Make sure that you can describe how the ROC curve is constructed and what the optimal value of ROC_AUC is.) How would you expect a plot of test ROC_AUC vs. \\(\\lambda\\) to look, and why? (Draw it!) Exercise 2: Revisiting KNN The KNN algorithm has a natural extension in the classification setting. Suppose that we’re using KNN to build a spam classifier. How do we find the nearest neighbors for a test case? Why might it be a sensible idea to normalize all quantitative predictors to have a standard deviation of 1? Suppose that the 5 nearest neighbors for a given test case have outcome values: spam, spam, not_spam, spam, not_spam. How might we make a soft (probability) and hard (class) prediction for this test case? How would you expect a plot of test overall accuracy vs. # neighbors to look, and why? (Draw it!) Exercise 3: Confusion matrix computations We obtain the following confusion matrix after using a “trained” KNN model on a test dataset. Truth Not spam Spam ---------- ------ Not spam | 2670 220 Predicted Spam | 118 1593 Compute and interpret the following metrics: overall accuracy, sensitivity, specificity. Compute the no-information rate and use it to give context for how “high” the overall accuracy is. When building a spam classifier, which of overall accuracy, sensitivity, or specificity do you think matters most and why? Exercise 4: Connecting ROC curves with predicted probability boxplots Suppose that Model 1 has an ROC_AUC of 0.85 and that Model 2 has an ROC_AUC of 0.6. What would you expect the predicted probability boxplots (predicted probabilities vs. true class) to look like for these two models, and why? (Draw them.) "],["evaluating-classification-models-part-2.html", "Topic 14 Evaluating Classification Models (Part 2) Learning Goals Exercises", " Topic 14 Evaluating Classification Models (Part 2) Learning Goals Contextually interpret overall accuracy, sensitivity, specificity, and AUC Appropriately use and interpret the no-information rate to evaluate accuracy metrics Implement LASSO logistic regression in tidymodels Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll continue working with the spam dataset from last time. spam: Either spam or not spam (outcome) word_freq_WORD: percentage of words in the e-mail that match WORD (0-100) char_freq_CHAR: percentage of characters in the e-mail that match CHAR (e.g., exclamation points, dollar signs) capital_run_length_average: average length of uninterrupted sequences of capital letters capital_run_length_longest: length of longest uninterrupted sequence of capital letters capital_run_length_total: sum of length of uninterrupted sequences of capital letters Our goal will be to use email features to predict whether or not an email is spam - essentially, to build a spam filter! library(dplyr) library(readr) library(ggplot2) library(tidymodels) library(probably) # install.packages(&quot;probably&quot;) tidymodels_prefer() spam &lt;- read_csv(&quot;https://www.dropbox.com/s/leurr6a30f4l32a/spambase.csv?dl=1&quot;) # A little data cleaning to remove the space in &quot;not spam&quot; spam &lt;- spam %&gt;% mutate(spam = ifelse(spam==&quot;spam&quot;, &quot;spam&quot;, &quot;not_spam&quot;)) Exercise 1: Implementing LASSO logistic regression in tidymodels Open up this tidymodels note sheet as a reference for writing your code from scratch. Fit a LASSO logistic regression model for the spam outcome, and allow all possible predictors to be considered (spam ~ . for the model formula). Use 10-fold CV. Use the roc_auc and accuracy (overall accuracy) metrics when tuning. Initially try a sequence of 100 \\(\\lambda\\)’s from 1 to 10. Diagnose whether this sequence should be updated by looking at the plot of test AUC vs. \\(\\lambda\\). If needed, adjust the max value in the sequence up or down by a factor of 10. (You’ll be able to determine from the plot whether to adjust up or down.) set.seed(123) # Need to set reference level (to the outcome you are NOT interested in) spam &lt;- spam %&gt;% mutate(spam = relevel(factor(spam), ref=&quot;not_spam&quot;)) # Set up CV folds data_cv &lt;- # LASSO logistic regression model specification logistic_lasso_spec &lt;- # Recipe logistic_lasso_rec &lt;- # Workflow (Recipe + Model) log_lasso_wf &lt;- # Tune model: specify grid of parameters and tune penalty_grid &lt;- tune_output &lt;- Exercise 2: Inspecting the LASSO logistic model Use autoplot() to inspect the plot of CV AUC vs. \\(\\lambda\\) once more (after adjusting the penalty grid). Is anything surprising about the results relative to your expectations from Exercise 1? Brainstorm some possible explanations in consideration of the data context. # Visualize evaluation metrics as a function of tuning parameters Choose a final model whose CV AUC is within one standard error of the overall best metric. Comment on the variables that are removed from the model. # Select &quot;best&quot; penalty best_se_penalty &lt;- # Define workflow with &quot;best&quot; penalty value final_wf &lt;- # Use final_wf to fit final model with &quot;best&quot; penalty value final_fit_se &lt;- final_fit_se %&gt;% tidy() Comment on the variable importance based on the how long a variable stayed in the model. Connect the output to the data context. glmnet_output &lt;- final_fit_se %&gt;% extract_fit_engine() # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- glmnet_output$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { this_coeff_path &lt;- bool_predictor_exclude[row,] if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{ return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)} }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) Exercise 3: Interpreting evaluation metrics Inspect the overall CV results for the “best” \\(\\lambda\\), and compute the no-information rate (NIR) to give context to the overall accuracy: # CV results for &quot;best lambda&quot; tune_output %&gt;% collect_metrics() %&gt;% filter(penalty == best_se_penalty %&gt;% pull(penalty)) # Count up number of spam and not_spam emails in the training data spam %&gt;% count(spam) # Name of the outcome variable goes inside count() # Compute the NIR Why is an AUC of 1 the best possible value for this metric? How does the AUC for our spam model look relative to this best value? "],["trees-conceptual.html", "Topic 15 Trees (Conceptual) Learning Goals Exercises", " Topic 15 Trees (Conceptual) Learning Goals Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Slides from today are available here. Exercises Exercise 1: Core theme: parametric/nonparametric What does it mean for a method to be nonparametric? In general, when might we prefer nonparametric to parametric methods? When might we not? Where do you think trees fall on the parametric/nonparametric spectrum? Exercise 2: Core theme: Tuning parameters and the BVT The key feature governing complexity of a tree model is the number of splits used in the tree. How is the number of splits related to model complexity, bias, and variance? In practice, the number of splits is controlled indirectly through the following tuning parameters. For each, discuss how low/high parameter settings would affect the number of tree splits. min_n: the minimum number of observations that must exist in a node in order for a split to be attempted. cost_complexity: complexity parameter. Any split that does not increase node purity by cost_complexity is not attempted. depth: Set the maximum depth of any node of the final tree. The depth of a node is the number of branches that need to be followed to get to a given node from the root node. (The root node has depth 0.) Exercise 3: Regression trees As discussed in the video, trees can also be used for regression! Let’s work through a step of building a regression tree by hand. For the two possible splits below, determine the better split for the tree by computing the sum of squared residuals as the measure of node impurity. (The numbers following Yes: and No: indicate the outcome value of the cases in the left (Yes) and right (No) regions.) Split 1: x1 &lt; 3 - Yes: 1, 1, 2, 4 - No: 2, 2, 4, 4 Split 2: x1 &lt; 4 - Yes: 1, 1, 2 - No: 2, 2, 4, 4, 4 Mini-homework: Building &amp; tuning trees in tidymodels Your mini-homework to prepare for next class is to implement a tree for your project dataset. Use our tidymodels note sheet as a reference for writing your code from scratch. New coding aspects for decision trees: Model name: decision_tree() (as opposed to linear_reg() or logistic_reg(), for example) Engine: \"rpart\" Depending on your outcome variable, make sure to appropriately set_mode() as either \"regression\" or \"classification\". Use the following to setup the tuning parameters in the model specification: set_args( cost_complexity = tune(), min_n = tune(), tree_depth = NULL ) Use the following to setup the grid of tuning parameter values: # This creates 10 values (levels) of cost_complexity from 10^-5 to 10 # and 3 values (levels) of min_n from 2 to 20 # 30 total parameter combinations tuning_param_grid &lt;- grid_regular( cost_complexity(range = c(-5, -1)), min_n(range = c(2, 20)), levels = c(10,3) ) Use autoplot() to inspect the bias-variance tradeoff plot (estimated test performance vs. tuning parameters). Select the optimal tuning parameter combination with select_by_one_std_err(). The desc(cost_complexity), desc(min_n) sorts the results first by cost_complexity (highest to lowest (simplest to most complex)) and then by min_n (highest to lowest (simplest to most complex)). select_by_one_std_err(___, metric = ___, desc(cost_complexity), desc(min_n)) Fit your tree to the full training data using finalize_workflow() and fit(). "],["trees-coding.html", "Topic 16 Trees (Coding) Learning Goals Trees in tidymodels Exercises", " Topic 16 Trees (Coding) Learning Goals Implement trees in tidymodels Visualize learned trees and use visualizations to explore predictions made from trees Interpret variable importance measures from trees Trees in tidymodels To build tree models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(readr) library(ggplot2) library(tidymodels) tidymodels_prefer() set.seed(___) # Pick your favorite number to fill in the parentheses To fit a classification tree, we can adapt the following: ct_spec &lt;- decision_tree() %&gt;% set_engine(engine = &quot;rpart&quot;) %&gt;% set_args( cost_complexity = NULL, # default is 0.01 (used for pruning a tree) min_n = NULL, tree_depth = NULL ) %&gt;% set_mode(&quot;classification&quot;) # change this for regression tree data_rec &lt;- recipe(___ ~ ___, data = ______) data_wf &lt;- workflow() %&gt;% add_model(ct_spec) %&gt;% add_recipe(data_rec) fit_mod &lt;- data_wf %&gt;% # or use tune_grid() to tune any of the parameters above fit(data = _____) Visualizing and interpreting the “best” tree # Plot the tree (make sure to load the rpart.plot package first) fit_mod %&gt;% extract_fit_engine() %&gt;% rpart.plot() # Get variable importance metrics # Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable. fit_mod %&gt;% extract_fit_engine() %&gt;% pluck(&#39;variable.importance&#39;) Exercises You will need to install the rpart.plot package before proceeding. Bias-variance tradeoff plot If tune_out is your output from tune_grid(), use autoplot to inspect the plot of test performance vs. tuning parameters: autoplot(tune_out) + theme_classic() Inspecting evaluation metrics Use collect_metrics() to view evaluation metrics for all tuning parameter combinations. Use filter() to only show metics for the best values of the tuning parameters. Interpret these metrics in the context of your data. Is this tree model good? Interpret in light of the no-information rate for your dataset. tune_out %&gt;% collect_metrics() %&gt;% filter(___) Visualizing your tree If final_fit is your output after finalize_workflow() and fit(), use the following to plot your tree. Look at page 3 of the rpart.plot package vignette to understand what the plot components mean. Take a look at what variables were used for splitting early on (near the top) and what variables tended to be chosen for splitting overall. Do these make sense contextually? final_fit %&gt;% extract_fit_engine() %&gt;% rpart.plot() Variable importance We can obtain numerical variable importance measures from trees. These measure, roughly, “the total decrease in node impurities from splitting on the variable” (even if the variable isn’t ultimately used in the split). What are the 3 most important predictors by this measure? Does this agree with you might have expected based on the plot of the best fitted tree? What might greedy behavior have to do with this? final_fit %&gt;% extract_fit_engine() %&gt;% pluck(&quot;variable.importance&quot;) Predictions and exploring error Classification setting If your outcome is categorical: use the following to add hard and soft predictions to your dataset. YOUR_DATA &lt;- YOUR_DATA %&gt;% mutate( pred_prob = predict(final_fit, new_data = YOUR_DATA, type = &quot;prob&quot;), pred_class = predict(final_fit, new_data = YOUR_DATA, type = &quot;class&quot;) ) Make some plots exploring the relationship between soft predictions and the predictors. Do the same for hard predictions. This will help you visualize what relationships the tree model is learning. We can also make plots to explore misclassification rates. Use the code below to create a misclassified variable that indicates whether or not a case was misclassified. Explore how this variable relates to your predictors. mutate(misclassified = YOUR_OUTCOME!=pred_class) Regression setting If your outcome is quantitative: use the following to add predictions to your dataset. YOUR_DATA &lt;- YOUR_DATA %&gt;% mutate( pred = predict(final_fit, new_data = YOUR_DATA) ) Within mutate(), include the calculations needed to calculate residuals and make residual plots to assess any systematic errors in your model. Backup dataset If you weren’t able to get trees working for your project, explore the following dataset on urban land cover. Context: Our goal will be to classify types of urban land cover in small subregions within a high resolution aerial image of a land region. Data from the UCI Machine Learning Repository include the observed type of land cover (determined by human eye) and “spectral, size, shape, and texture information” computed from the image. See this page for the data codebook. library(dplyr) library(readr) library(ggplot2) library(rpart.plot) library(tidymodels) tidymodels_prefer() # Read in the data land &lt;- read_csv(&quot;https://www.dropbox.com/s/r59esfepjw7qsg0/land_cover_training.csv?dl=1&quot;) # There are 9 land types, but we&#39;ll focus on 3 of them land &lt;- land %&gt;% filter(class %in% c(&quot;asphalt&quot;, &quot;grass&quot;, &quot;tree&quot;)) set.seed(123) # don&#39;t change this data_fold &lt;- vfold_cv(land, v = 10) ct_spec_tune &lt;- decision_tree() %&gt;% set_engine(engine = &#39;rpart&#39;) %&gt;% set_args(cost_complexity = tune(), min_n = 2, tree_depth = NULL) %&gt;% set_mode(&#39;classification&#39;) data_rec &lt;- recipe(class ~ ., data = land) data_wf_tune &lt;- workflow() %&gt;% add_model(ct_spec_tune) %&gt;% add_recipe(data_rec) param_grid &lt;- grid_regular(cost_complexity(range = c(-5, 1)), levels = 10) tune_res &lt;- tune_grid( data_wf_tune, resamples = data_fold, grid = param_grid, metrics = metric_set(accuracy, roc_auc) #change this for regression trees ) best_complexity &lt;- select_by_one_std_err(tune_res, metric = &#39;accuracy&#39;, desc(cost_complexity)) data_wf_final &lt;- finalize_workflow(data_wf_tune, best_complexity) land_final_fit &lt;- fit(data_wf_final, data = land) "],["bagging-and-random-forests.html", "Topic 17 Bagging and Random Forests Learning Goals Exercises", " Topic 17 Bagging and Random Forests Learning Goals Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors Slides from today are available here. Exercises Exercise 1: Bagging: Bootstrap Aggregation First, explain to each other what bootstrapping is (the algorithm). Discuss why we might utilize bootstrapping? What do we gain? If you’ve seen bootstrapping before in STAT 155, why did we do bootstrapping then? Note: In practice, we don’t often use bagged trees as the final classifier because the trees end up looking too similar to each other so we create random forests (bagged trees + use random subset of variables to choose split from). Exercise 2: Preparation to build a random forest Suppose we wanted to evaluate the performance of a random forest which uses 500 classification trees. Describe the 10-fold CV approach to evaluating the random forest. In this process, how many total trees would we need to construct? The out-of-bag (OOB) error rate provides an alternative approach to evaluating forests. Unlike CV, OOB summarizes misclassification rates (1-accuracy) when applying each of the 500 trees to the “test” cases that were not used to build the tree. How many total trees would we need to construct in order to calculate the OOB error estimate? Moving forward, we’ll use OOB and not CV to evaluate forest performance. Explain why. Exercise 3: Building the random forest In this exercise, you’ll implement a random forest in tidymodels for your project dataset. Let’s start by thinking about tuning parameters and recipes. min_n is a random forest tuning parameter that gets inherited from single trees. It represents the minimum number of cases that must exist in a node in order for a split to be attempted. In light of the random forest algorithm, do you think this variable should be low or high? Explain. The trees parameter gives the number of trees in the forest. What would you expect about the variability of random forest predictions as this parameter increases? The mtry parameter gives the number of random variables chosen at each split. Describe the bias-variance tradeoff for this parameter. In light of the random forest algorithm, why should you NOT include a step_dummy(all_nominal_variables()) in your tidymodels recipe? We can now put together our work to train our random forest model. Build a single random forest model (for computational time reasons) using the following template: # Model Specification rf_spec &lt;- rand_forest() %&gt;% set_engine(engine = &quot;ranger&quot;) %&gt;% set_args( mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors)) trees = 1000, # Number of trees min_n = 2, probability = FALSE, # FALSE: get hard predictions (not needed for regression) importance = &quot;impurity&quot; ) %&gt;% set_mode(&quot;classification&quot;) # change this for regression # Recipe data_rec &lt;- recipe(outcome ~ ., data = YOUR_DATA) # Workflows data_wf &lt;- workflow() %&gt;% add_model(rf_spec) %&gt;% add_recipe(data_rec) # Note how we&#39;re not using tune_grid() or vfold_cv() information here rf_fit &lt;- fit(data_wf, data = YOUR_DATA) "],["bagging-and-random-forests-coding.html", "Topic 18 Bagging and Random Forests (Coding) Exercises", " Topic 18 Bagging and Random Forests (Coding) Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the ranger and vip packages. Our goal will be to classify types of urban land cover in small subregions within a high resolution aerial image of a land region. Data from the UCI Machine Learning Repository include the observed type of land cover (determined by human eye) and “spectral, size, shape, and texture information” computed from the image. See this page for the data codebook. Source: https://ncap.org.uk/sites/default/files/EK_land_use_0.jpg library(dplyr) library(readr) library(ggplot2) library(tidymodels) tidymodels_prefer() # Read in the data land &lt;- read_csv(&quot;https://www.dropbox.com/s/r59esfepjw7qsg0/land_cover_training.csv?dl=1&quot;) # There are 9 land types, but we&#39;ll focus on 3 of them land &lt;- land %&gt;% filter(class %in% c(&quot;asphalt&quot;, &quot;grass&quot;, &quot;tree&quot;)) %&gt;% mutate(class = factor(class)) Exercise 1: Building a random forest in tidymodels If you want to use the land dataset, the code is complete, but you should step through to understand what each line is doing. If you are using your project dataset, you don’t need to do anything here if you finished implementing a random forest from last class. set.seed(123) # Model Specification rf_spec &lt;- rand_forest() %&gt;% set_engine(engine = &quot;ranger&quot;) %&gt;% set_args( mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors)) trees = 1000, # Number of trees min_n = 2, probability = FALSE, # FALSE: get hard predictions (not needed for regression) importance = &quot;impurity&quot; ) %&gt;% set_mode(&quot;classification&quot;) # change this for regression # Recipe data_rec &lt;- recipe(class ~ ., data = land) # Workflows data_wf &lt;- workflow() %&gt;% add_model(rf_spec) %&gt;% add_recipe(data_rec) # Note how we&#39;re not using tune_grid() or vfold_cv() information here rf_fit &lt;- fit(data_wf, data = land) Exercise 2: Evaluate model performance Printing the rf_fit object displays information about the random forest model. Report and interpret the OOB prediction error value in the context of your data. (Misclassification rate is reported for classification, and MSE is reported for regression.) rf_fit Classification We can look at a confusion matrix resulting from the out-of-bag (OOB) predictions. Check your conceptual understanding: How are OOB predictions made? Based on the confusion matrix, what classes are predicted most accurately, and does this make sense contextually? rf_output &lt;- land %&gt;% mutate(OOB_pred_class = rf_fit %&gt;% extract_fit_engine() %&gt;% pluck(&quot;predictions&quot;)) # This extracts the OOB predictions conf_mat( data = rf_output, truth = class, estimate = OOB_pred_class ) We can also explore how misclassification rates relate to predictors. The code below creates a new variable is_misclass indicating whether or not a case was misclassified in the OOB predictions. Make visualizations of is_misclass and each of your predictors individually to explore how misclassification rate might vary across predictors. rf_output &lt;- rf_output %&gt;% mutate(is_misclass = class!=OOB_pred_class) Regression We can compute residuals resulting from OOB predictions. Check your conceptual understanding: How are OOB predictions made? Make residual plots to visualize how errors might relate to your predictors. rf_output &lt;- land %&gt;% mutate( OOB_pred = rf_fit %&gt;% extract_fit_engine() %&gt;% pluck(&quot;predictions&quot;), # This extracts the OOB predictions resid = YOUR_OUTCOME - OOB_pred ) # Residual plots vs individual predictors Exercise 3: Variable importance (You’ll need to install the vip package before proceeding.) Because bagging and random forests use many trees, the nice interpretability of single decision trees is lost. However, we can still get a measure of how important the different predictors were in this predicting the outcome. For each of the predictors, the code below gives the “total decrease in node impurities from splitting on the variable, averaged over all trees” (package documentation). Do the variable importance results make sense contextually? (If you’re using the land dataset, check out the codebook for these variables here.) Conceptual question: It has been found that measures of variable importance from random forests can tend to favor predictors with a lot of unique values. Explain briefly why this makes sense by thinking about the recursive binary splitting algorithm for a single tree. (Note: similar cautions arise for variable importance in single trees.) library(vip) # Plot of the variable importance information rf_fit %&gt;% extract_fit_engine() %&gt;% vip(num_features = 30) + theme_classic() # Extract the numerical information on variable importance and display the most and least important predictors rf_var_imp &lt;- rf_fit %&gt;% extract_fit_engine() %&gt;% vip::vi() head(rf_var_imp) tail(rf_var_imp) "],["k-means-clustering.html", "Topic 19 K-Means Clustering Learning Goals Exercises", " Topic 19 K-Means Clustering Learning Goals Clearly describe / implement by hand the k-means algorithm Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. In this paper, Gorman et al. study characteristics of penguin populations in the Antarctic. We’ll be looking at a dataset of penguin body measurements available in the palmerpenguins package. (Make sure to install this package before beginning.) Our goal in using this data is to better understand the following questions: What similarities are there among the penguins? Do there appear to be different species? If so, how many species are there? library(dplyr) library(ggplot2) library(palmerpenguins) data(penguins) # Remove observations with missing data on key variables penguins &lt;- penguins %&gt;% filter(!is.na(bill_length_mm), !is.na(bill_depth_mm), !is.na(flipper_length_mm)) Exercise 1: Visual explorations We’ll first explore clustering based on characteristics of the penguins’ bills/beaks. There are two variables that measure the length and depth of the penguins’ bills (in mm): bill_length_mm and bill_depth_mm. Make a scatterplot of these two measurements. If you had to visually designate 3 different penguin clusters (possible species), how would you designate them? ggplot(penguins, aes(???)) + geom_point() Based on the plot, are there any differences in scale that you might be concerned about? Exercise 2: K-means clustering on bill length and depth The kmeans() function in R performs k-means clustering. Use the code below to run k-means for \\(k = 3\\) clusters. Why is it important to use set.seed()? (In practice, it’s best to run the algorithm for many values of the seed and compare results.) # Select just the bill length and depth variables penguins_sub &lt;- penguins %&gt;% select(bill_length_mm, bill_depth_mm) # Run k-means for k = centers = 3 set.seed(253) km_out_3grps &lt;- kmeans(penguins_sub, centers = 3) # Display the cluter assignments km_out_3grps$cluster # Add a variable (kclust_3) to the original dataset # containing the cluster assignments penguins &lt;- penguins %&gt;% mutate( kclust_3 = factor(km_out_3grps$cluster) ) Update your original scatterplot to add a color aesthetic that corresponds to the kclust_3 variable created above. Do the cluster assignments correspond to your intuition from exercise 1? Why might this be? # Visualize the cluster assignments on the original scatterplot Exercise 3: Addressing variable scale We can use the code below to rerun k-means clustering on the scaled data. The scaled data have been rescaled so that the standard deviation of each variable is 1. Remake the scatterplot to visualize the updated cluster assignments. Do the cluster assignments correspond to your intuition from exercise 1? # Run k-means on the *scaled* data (all variables have SD = 1) set.seed(253) km_out_3grps_scaled &lt;- kmeans(scale(penguins_sub), centers = 3) penguins &lt;- penguins %&gt;% mutate( kclust_3_scale = factor(km_out_3grps_scaled$cluster) ) # Visualize the new cluster assignments Exercise 4: Clustering on more variables We can use as many variables in our clustering as makes sense given our goals. The dataset contains another body measurement variable of interest to us: flipper_length_mm (flipper length in mm). Complete the code below to cluster on bill length and depth as well as flipper length. Looking at the summary statistics, do you think it would be best to scale the variables? # Select the variables to be used in clustering penguins_sub &lt;- penguins %&gt;% select(???) # Look at summary statistics of the 3 variables summary(penguins_sub) # Perform clustering: should you use scale()? set.seed(253) km_out_3grps_3vars_scaled &lt;- kmeans(???) penguins &lt;- penguins %&gt;% mutate(kclust_3_3vars = factor(km_out_3grps_3vars_scaled$cluster)) Exercise 5: Interpreting the clusters One way to interpet the resulting clusters is to explore how variables differ across the clusters. We can look at the 3 variables used in the clustering as well as a body mass variable available in the dataset. Run the code below to look at the mean bill length, bill depth, flipper length, and body mass across the 3 clusters. What characterizes each of the 3 clusters? Try to come up with contextual “names” for the clusters (e.g., “big beaks” or “small penguins”). penguins %&gt;% group_by(kclust_3_3vars) %&gt;% summarize(across(c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g), mean)) Exercise 6: Picking \\(k\\) We’ve been using \\(k = 3\\) so far, but how can we pick \\(k\\) using a data-driven approach. One strategy is to compare the total squared distance of each case from its assigned centroid for different values of \\(k\\). (This measure is available within the $tot.withinss component of objects resulting from kmeans().) Run the code below to create this plot for choices of \\(k\\) from 1 to 15. Using this plot and thinking about data context and our scientific goals, what are some reasonable choices for the number of clusters? # Create storage vector for total within-cluster sum of squares tot_wc_ss &lt;- rep(0, 15) # Loop for (k in 1:15) { # Perform clustering km_out &lt;- kmeans(scale(penguins_sub), centers = k) # Store the total within-cluster sum of squares tot_wc_ss[k] &lt;- km_out$tot.withinss } plot(1:15, tot_wc_ss, xlab = &quot;Number of clusters&quot;, ylab = &quot;Total within-cluster sum of squares&quot;) "],["hierarchical-clustering.html", "Topic 20 Hierarchical Clustering Learning Goals Exercises", " Topic 20 Hierarchical Clustering Learning Goals Clearly describe / implement by hand the hierarchical clustering algorithm Compare and contrast k-means and hierarchical clustering in their outputs and algorithms Interpret cuts of the dendrogram for single and complete linkage Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. In this paper, Gorman et al. study characteristics of penguin populations in the Antarctic. We’ll be looking at a dataset of penguin body measurements available in the palmerpenguins package. (Make sure to install this package before beginning.) Our goal in using this data is to better understand the following questions: What similarities are there among the penguins? Do there appear to be different species? If so, how many species are there? library(dplyr) library(ggplot2) library(palmerpenguins) data(penguins) # Remove observations with missing data on key variables penguins &lt;- penguins %&gt;% filter(!is.na(bill_length_mm), !is.na(bill_depth_mm), !is.na(flipper_length_mm)) Exercise 1: Hierarchical clustering by hand To practice the hierarchical clustering algorithm, let’s look at a small example. Suppose we collect the following bill depth and length measurements from 5 penguins: # NOTE: these data are already scaled! penguins_small &lt;- data.frame( depth = c(2.5, 2.7, 3.2, 3.5, 3.6), length = c(5.5, 6.0, 4.5, 5.0, 4.7) ) penguins_small ggplot(penguins_small, aes(x = depth, y = length)) + geom_point() + geom_text(aes(label = 1:5), vjust = 1.5) In the exercises below, you’ll draw a dendrogram for these 5 penguins by hand. Sketch the following plotting frame on some scrap paper: Step 1: First fusion Calculate the distance between each pair of penguins: round(dist(penguins_small), 2) Which pair of penguins 1-5 is most similar? Draw the fusion between this pair of leaves on your plot. Clearly indicate the height at which you draw this fusion. Step 2: Second fusion Construct a new distance matrix reflecting the distances between each pair of branches (where 4 and 5 have now been fused). Use complete linkage. That is, the distance between one branch and another is the maximum distance between any pair of leaves in those branches. We can do this by taking the distances from the distance matrix above and using the complete linkage strategy to obtain distances to the 4+5 cluster. 1 2 3 4 &amp; 5 1 0 2 xxxx 0 3 xxxx xxxx 0 4 &amp; 5 xxxx xxxx xxxx 0 Which pair of branches is most similar? Draw the fusion between this pair on the plot. Step 3: Third fusion Repeat! Construct a new distance matrix reflecting the distances between each pair of branches, those that have been fused already and those that have not. Which pair of branches is most similar? Draw the fusion between this pair on the plot. Step 4: Final fusion At this point, you should have 2 penguins in one cluster and 3 in another. The final step is to combine these into the tree trunk. Draw this fusion. Final step: Check your work in R: penguin_cluster &lt;- hclust(dist(penguins_small), method = &quot;complete&quot;) plot(penguin_cluster) Exercise 2: Exploring penguin dendrograms Let’s use hierarchical clustering under different linkage strategies to look at nestings of clusters for the full set of penguins. We’ll continue to cluster based on bill length and depth as well as flipper length. (We’ll use a smaller random subset of 50 penguins for ease of visualization.) Remind yourselves about the importance of considering variable scaling by looking at the summary statistics for the 3 variables. In looking at the dendrograms under the different linkage types, do you notice any particular features of how the dendrograms look (as mentioned in the video for single and centroid linkage)? # Random subsample of 50 penguins set.seed(253) penguins &lt;- penguins %&gt;% slice_sample(n = 50) # Select the variables to be used in clustering penguins_sub &lt;- penguins %&gt;% select(bill_length_mm, bill_depth_mm, flipper_length_mm) # Summary statistics for the variables summary(penguins_sub) # Compute a distance matrix on the scaled data dist_mat_scaled &lt;- dist(scale(penguins_sub)) # The (scaled) distance matrix is the input to hclust() # The method argument indicates the linkage type hc_complete &lt;- hclust(dist_mat_scaled, method = &quot;complete&quot;) hc_single &lt;- hclust(dist_mat_scaled, method = &quot;single&quot;) hc_average &lt;- hclust(dist_mat_scaled, method = &quot;average&quot;) hc_centroid &lt;- hclust(dist_mat_scaled, method = &quot;centroid&quot;) # Plot dendrograms plot(hc_complete) plot(hc_single) plot(hc_average) plot(hc_centroid) Exercise 3: Interpreting the clusters visually Let’s continue exploring the dendrogram from complete linkage. The plot() function for hclust() output allows a labels argument which can show custom labels for the leaves (cases). The code below labels the leaves with the species of each penguin. What do you notice about the clustering in relation to the actual penguin species? Try changing the labels to show the sex of the penguin. What do you notice? plot(hc_complete, labels = penguins$species) Exercise 4: Tree-cutting and interpretation When we cut a dendrogram at a given height, we indicate that the branches that have fused before (below) that point should be distinct clusters. For the complete linkage dendrogram, say that we cut the tree at a height of 3? How many clusters should result (eyeball from dendrogram)? How can we interpret this cut in terms of the distances between cases in the resulting clusters? We can add new variables containing cluster assignments with mutate() and cutree(), as below. Using either set of cluster assignments, make some visualizations to explore how variables of interest are related to the cluster assignments. penguins &lt;- penguins %&gt;% mutate( hclust_height3 = factor(cutree(hc_complete, h = 3)), # Cut at height (h) 3 hclust_num6 = factor(cutree(hc_complete, k = 6)) # Cut into 6 clusters (k) ) Exercise 5: K-means vs. hierarchical Brainstorm some pros/cons of hierarchical clustering vs. k-means in terms of their algorithms and the output produced. "],["clustering-project-work.html", "Topic 21 Clustering (Project Work) Learning Goals Dataset choice Clustering in the Wild R Coding challenges", " Topic 21 Clustering (Project Work) Learning Goals Implement k-means and hierarchical clustering for your project dataset and interpret findings Slides from today are available here. Dataset choice If not working on your project dataset, feel free to choose one of the following three datasets to work with: Wine Attributes (download here) 178 Italian wines were analyzed Variables (from Chemical Analysis) Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline library(readr) wine &lt;- read_csv(&quot;wine.csv&quot;) Mall Customers (download here) 200 individuals Variables Binary Gender Age Annual Income (in $1000’s) Spending Score (summary of buying behavior) library(readr) customers &lt;- read_csv(&quot;mall_customers.csv&quot;) Credit Card Clients (download here) Almost 9000 credit card holders Variables based on 6 months of time CUSTID: Identification of Credit Card holder BALANCE : Balance amount left in their account to make purchases BALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated) PURCHASES : Number of purchases made from account ONEOFFPURCHASES : Maximum purchase amount done in one-go INSTALLMENTSPURCHASES : Amount of purchase done in installment CASHADVANCE : Cash in advance given by the user PURCHASESFREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased) ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased) PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done) CASHADVANCEFREQUENCY : How frequently the cash in advance being paid CASHADVANCETRX : Number of Transactions made with “Cash in Advanced” PURCHASESTRX : Number of purchase transactions made CREDITLIMIT : Limit of Credit Card for user PAYMENTS : Amount of Payment done by user MINIMUM_PAYMENTS : Minimum amount of payments made by user PRCFULLPAYMENT : Percent of full payment paid by user TENURE : Tenure of credit card service for user library(readr) credit &lt;- read_csv(&quot;creditcard.csv&quot;) Analysis Whether you are using your project dataset or one of the above datasets, the goals and analysis plans are as follows: Goal: Cluster the data to gain insights into latent groupings and patterns Available clustering methods K-means with all quantitative variables Partitioning around medoids (pam) as a robust version of K-means If you have at least one categorical variable, daisy() will calculate Gower’s distance. (Gower’s distance automatically handles scaling.) Hierarchical clustering If you have at least one categorical variable, use daisy() to calculate Gower’s distance. When using daisy(), you will need to make sure that all categorical variables are of the factor type in R: data &lt;- data %&gt;% mutate( cat_var1 = as.factor(cat_var1), cat_var2 = as.factor(cat_var2) ) # K-means with all quantitative variables kmeans(data, centers = k) # PAM for a robust version of k-means when there are some categorical variables # You will need to install the &quot;cluster&quot; package to use daisy() library(cluster) pam(daisy(data), k = k) # Choosing an appropriate number of clusters # Create storage vector for total within-cluster sum of squares tot_wc_ss &lt;- rep(0, 15) # Loop for (k in 1:15) { # Perform clustering pam_out &lt;- pam(daisy(mtcars), k = k) # Store the total within-cluster sum of squares tot_wc_ss[k-1] &lt;- sum(pam_out$clusinfo[,&quot;av_diss&quot;]*pam_out$clusinfo[,&quot;size&quot;]) } plot(1:15, tot_wc_ss, xlab = &quot;Number of clusters&quot;, ylab = &quot;Total within-cluster sum of squares&quot;) # Hierarchical clustering on... # ...all quantitative variables hclust(dist(scale(data)), method = &quot;CHOOSE_LINKAGE_TYPE&quot;) # ...a mix of quantitative and categorical variables hclust(daisy(data), method = &quot;CHOOSE_LINKAGE_TYPE&quot;) General process Decide which variables you want to use in your clustering and why. Use select(your_data, chosen_var1, chosen_var2, etc) to choose this subset of variables. Implement k-means (if all variables are quantitative) or PAM (if some variables are categorical). Decide on a reasonable number of clusters by inspecting a plot like in Exercise 6 of Topic 19. Running these methods multiple times (to see if the results are consistent despite the random initialization steps) might not be possible if computational time is high for your dataset. Implement hierarchical clustering. Visualizing the dendrogram might be tough if you have a lot of cases. You may need to rely on just cutting the tree and inspecting the characteristics of the resulting clusters. Use the same number of clusters as from k-means/PAM as a starting point for the number of clusters here. Insights Use summary statistics like in the Topic 19 exercises and visualizations like in the Topic 20 exercises to understand the features of the clusters. Clustering in the Wild To give you a taste of how these methods get use in “the wild” world of science, here are a few papers (quality varies): Image Segmentation (https://link.springer.com/article/10.1007/s11042-021-10594-9) Bacteria Clustering (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0002843) Document Clustering (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7790388/) Clustering ICD10 Diagnosis Codes (https://arxiv.org/abs/1909.00306) Clustering Activity Sequences (https://www.sciencedirect.com/science/article/abs/pii/S0968090X21000395?via%3Dihub) R Coding challenges The best way to learn new things about R is to work on a data project. The goals drive what code is needed. Learn them as you need them. What things have come up so far for you? What has been the most frustrating? When do you get stuck? What are you wanting to do with your data? Besides class projects, you can practice visualizing data: TidyTuesday Challenges Check out David Robinson’s TidyTuesday’s Screencasts "],["principal-components-analysis.html", "Topic 22 Principal Components Analysis Learning Goals Exercises", " Topic 22 Principal Components Analysis Learning Goals Explain the goal of dimension reduction and how this can be useful in a supervised learning setting Interpret and use the information provided by principal component loadings and scores Interpret and use a scree plot to guide dimension reduction Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Exercise 1: Core concepts For this first exercise, we will work through some key ideas and terminology related to PCA using the information below, which comes from a small data set of 3 variables: \\(x_1, x_2, x_3\\). \\[ \\text{PC1} = 0.672 x_1 - 0.287 x_2 - 0.683 x_3 \\] \\[ \\text{PC2} = -0.244 x_1 - 0.956 x_2 + 0.162 x_3 \\] \\[ \\text{PC3} = 0.699 x_1 - 0.058 x_2 - 0.713 x_3 \\] What are the loadings of principal components 1 to 3? In general, what information does a loading give us? What are the two most important variables for forming PC1? PC2? PC3? A case has variable values \\((x_1, x_2, x_3) = (1, 1, 1)\\). What are the PC1, PC2, and PC3 scores for this case? How can we interpret these scores? What can be said about the amount of variation in the dataset explained by the three PCs? In thinking about the PCs as defining new “directions”, how are PCs 2 and above selected relative to the first ones? Now let’s use PCA to explore a gene expression dataset. The Khan dataset in the ISLR2 package contains gene expression measurements in cancer tissue samples. (Khan is the first author’s last name.) Such data are commonly part of biological studies to better understand the molecular basis for disease. You can find information about this dataset by entering ?Khan in the Console. library(dplyr) library(purrr) library(ggplot2) library(ISLR2) data(Khan) # train_data contains 2308 gene expression measurements for 63 samples train_data &lt;- Khan %&gt;% pluck(&quot;xtrain&quot;) # Rename the variables to be gene1, gene2, etc. colnames(train_data) &lt;- paste0(&quot;gene&quot;, seq_len(ncol(train_data))) # train_labels contains information on which of 4 cancer subtypes each sample comes from train_labels &lt;- Khan %&gt;% pluck(&quot;ytrain&quot;) Exercise 2: Exploring PC loadings The prcomp() function performs PCA. Look at the help page for the prcomp() function under the “Value” section, and recall that pluck('name') or $ extracts named components of objects (e.g., list_object %&gt;% pluck('name_of_component') or list_object$name_of_component). pca_out &lt;- prcomp(train_data, center = TRUE, scale = TRUE) Use the head() function to display the first few rows of the loadings matrix. Using just the first 3 genes, write out the equation for principal component 4. \\[PC4 = ??*gene1 + ??*gene2 + ??*gene3\\] Describe how you would use the loadings matrix to find the genes that contribute most to the largest source of variation in the dataset. In R, we can extract the first column of a matrix object mat using mat[,1] or we can convert the matrix to a data frame and use the name of the column mat %&gt;% as.data.frame() %&gt;% select(PC1). Use the head(), arrange() for data frames or sort() for vectors, and abs() functions to display the 10 most important genes that contribute to the largest source of variation. Exercise 3: Exploring PC scores We can plot the PC1 and PC2 scores against each other in a scatterplot to see if these new variables cluster the cases according to some other information. For example, in this data, we have tumor type labels for each case. (4 tumor types) The x component of pca_out contains these scores. Complete the code below to make a scatterplot of the PC2 scores versus the PC1 scores. # Color the points by the information in train_labels (the 4 cancer subtypes present) pca_out %&gt;% pluck(&quot;x&quot;) %&gt;% as.data.frame() %&gt;% mutate(labels = train_labels) %&gt;% ggplot(aes(x = ??, y = ??, color = factor(??))) + geom_point() + labs(x = &quot;PC1&quot;, y = &quot;PC2&quot;) + scale_color_viridis_d() + theme_classic() Do you notice any clustering by tumor type? How could we use k-means and hierarchical clustering to see whether the cases (tissue samples) cluster by tumor type? How can we use loadings and the information in the score plot to understand what genes drive groupings of the tissue samples? Exercise 4: Scree plots and dimension reduction Let’s explore how to use PCA for dimension reduction. The sdev component of pca_out gives the standard deviation explained by each principal component. Explain what the first 2 lines of code below are doing. var_explained &lt;- (pca_out %&gt;% pluck(&quot;sdev&quot;))^2 pve &lt;- var_explained/sum(var_explained) var_data &lt;- tibble( PC = seq_len(length(var_explained)), var_explained = var_explained, pve = pve ) # Construct scree plots p1 &lt;- var_data %&gt;% ggplot(aes(x = PC, y = pve)) + geom_point() + geom_line() + labs(x = &quot;Principal Component&quot;, y = &quot;Proportion of variance explained&quot;) + theme_classic() p2 &lt;- var_data %&gt;% ggplot(aes(x = PC, y = cumsum(pve))) + geom_point() + geom_line() + labs(x = &quot;Principal Component&quot;, y = &quot;Cumulative proportion of variance explained&quot;) + theme_classic() library(ggpubr) ggarrange(p1, p2) Explain why the plots above look the way they do. (These plots are called scree plots.) We can think of principal components as new variables. PCA allows us to perform dimension reduction to use a smaller set of variables, often to accompany supervised learning. How can we use the plots above to guide a choice about the number of PCs to use? Carefully describe how we could also use cross-validation to pick the number of PCs. (For concreteness, suppose that we’re in a linear regression setting.) Exercise 5: Variable scaling You likely noticed the scale = TRUE within prcomp() above. This scales the variables to all have unit variance. Explain why this is often advisable by thinking generally about the ranges of different variables. In what other methods would scaling be important? "],["homework-1.html", "Homework 1 Project Work Portfolio Work Metacognitive Reflection", " Homework 1 Due Friday, February 3 at midnight CST on Moodle Please turn in a single PDF document containing (1) your responses for the Project Work and Metacognitive Reflection sections and (2) a LINK to the Google Doc with your responses for the Portfolio Work section. Project Work Goal: Find a dataset to use for your final project, and begin getting acquainted with the data. Details: Your dataset should allow you to perform a supervised learning analysis (regression or classification) and an unsupervised learning analysis (clustering or dimension reduction). The following resources are good places to start looking for data: Google Dataset Search Kaggle UCI Machine Learning Repository Harvard Dataverse Macalester’s librarians are a great resource too! They can help you find data aligning with your interests. You can make an appointment using the Ask Us page on the library website. Even if you end up working in a group on the project (which isn’t required - working alone is fine), please complete this initial work individually. Check in with the instructor early if you need help. Deliverables: Write 1-2 paragraphs (no more than 350 words) summarizing: The information in the dataset and the context behind the data. Use the prompts below to guide your thoughts. (Note: in some situations, there may be incomplete information on the data context. That’s fine. Just do your best to summarize what information is available, and acknowledge the lack of information where relevant.) What are the cases? Broadly describe the variables contained in the data. Who collected the data? When, why, and how? 2 (tentative) research questions 1 that can be investigated in a supervised learning setting (regression OR classification–you’re welcome to do both if you wish!) 1 that can be investigated in an unsupervised learning setting (clustering OR dimension reduction–you’re welcome to do both if you wish!) Note: These research questions might evolve over the course of the semester, and that’s fine! Having some idea of potential research directions now will still be helpful. Also make sure that you can read the data into R. You don’t need to do any analysis in R yet, but making sure that you can read the data will make the next steps go more smoothly. Portfolio Work Over the course of the semester, you will use your understanding of course ideas to build up a written portfolio that you can refer to years down the line. Each homework assignment will have prompts that require you to reflect on Data Ethics as well as concepts related to machine learning methods that are enduring, important, and worth being familiar with. (Refer to our syllabus on Moodle for a reminder of this breakdown.) You will receive comments on your responses from the instructor and preceptors that you can use to evaluate your understanding and to revise your work on subsequent assignments throughout the semester. Logistics: Make a copy of this Google Doc. You will use this SINGLE Google Doc for your homework responses (and revisions) all semester. (A single doc facilitates seeing feedback and revisions over time.) Update the sharing permissions on your doc so that anyone with the link can provide comments. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Areas to address: Data Ethics: Read the article Amazon scraps secret AI recruiting tool that showed bias against women. Write a short (roughly 250 words), thoughtful response about the themes and cautions that the article brings forth. Overfitting: The video used the analogy of a cat picture model to explain overfitting. Come up with your own analogy to explain overfitting. Evaluating regression models: Describe how residuals are central to the evaluation of regression models. Explain how they arise in quantitative evaluation metrics and how they are used in evaluation plots. Include examples of plots that show desirable and undesirable model behavior (feel free to draw them by hand if you wish) and what steps can be taken to address that undesirable behavior. Cross-validation: In your own words, explain the rationale for cross-validation in relation to overfitting and model evaluation. Describe the algorithm in your own words in at most 2 sentences. Subset selection: Algorithmic understanding: Look at Conceptual exercise 1, parts (a) and (b) in ISLR Section 6.6. What are the aspects of the subset selection algorithm(s) that are essential to answering these questions, and why? (Note: you’ll have to try to answer the ISLR questions to respond to this prompt, but the focus of your writing should be on the question in bold here.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? LASSO: Algorithmic understanding: Come up with your own analogy for explaining how the penalized least squares criterion works. Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? Metacognitive Reflection How do you feel about your level of understanding for the topics that we have covered so far? What were some notable themes in self- and peer-noticings? (What has been confusing and/or intriguing based on pre-class videos, in-class activities, and this homework assignment?) How do these reflections inform your next steps for developing stronger understanding and/or advising future students learning about these topics? "],["homework-2.html", "Homework 2 Project Work Portfolio Work Metacognitive Reflection", " Homework 2 Due Friday, February 17 at midnight CST. Deliverables: Nothing to submit on Moodle this time. Your Portfolio work will continue to go in the same Google Doc from HW1. This time, you’ll add your Metacognitive Reflection to your Portfolio. Project Work No new deliverables this time. Continue working to finalize your dataset if you haven’t already. Check in with the instructor if you would like help. If you already have your dataset, load into R and make some exploratory plots to get a sense for the distributions of variables and some initial relationships. This will also inform any preprocessing/cleaning steps needed. Portfolio Work Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. Organization: On the left side of your Google Doc (in the gray area beneath the menu bar), there is a gray icon–click this to show the section headers. Write your responses under these section headers. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Revisions: Make any revisions desired to previous concepts. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homeworks will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance on Piazza and in drop-in hours. New concepts to address: The following prompts are shared for all methods: Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Subset selection: Bias-variance tradeoff Parametric / nonparametric LASSO: Bias-variance tradeoff Parametric / nonparametric KNN: Algorithmic understanding: Draw and annotate pictures that show how the KNN (K = 2) regression algorithm would work for a test case in a 2 quantitative predictor setting. Also explain how the curse of dimensionality affects KNN performance. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: The KNN algorithm is often called a “lazy” learner. Discuss how this relates to the model training process and the computations that must be performed when predicting on a new test case. (3 sentences max.) Interpretation of output: The “lazy” learner feature of KNN in relation to model training affects the interpretability of output. How? (3 sentences max.) Splines: Algorithmic understanding: Explain the advantages of natural cubic splines over global transformations and piecewise polynomials. Also explain the connection between splines and the ordinary (least squares) regression framework. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: When using splines, how does computation time compare to fitting ordinary (least squares) regression models? (1 sentence) Interpretation of output: SKIP - will be covered in the GAMs section Data Ethics: Read the article Automated background checks are deciding who’s fit for a home. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. What themes recur from the HW1 article (on an old Amazon recruiting tool)? What aspects are more particular to the context of equity in housing access? Metacognitive Reflection Deliverables: Add a new page to the top of your Portfolio Google Doc titled “Metacognitive Reflections” – for this and remaining homework assignments, your Metacognitive Reflections will go here. Create a subsection called “Homework 2” to put your reflection from the following prompts: How do you feel about your level of understanding for the topics that we have covered so far? What were some notable themes in self- and peer-noticings? (What has been confusing and/or intriguing based on pre-class videos, in-class activities, and this homework assignment?) What insights on your understanding did you gain from taking and reviewing feedback on Quiz 1? How do these reflections inform your next steps for developing stronger understanding and/or advising future students learning about these topics? "],["homework-3.html", "Homework 3 Project Work Portfolio Work Metacognitive Reflection", " Homework 3 Portfolio Work and Metacognitive Reflection due Friday, March 3 at midnight. (Continue working in the same Google Doc from HW1.) Project Work due Friday, March 10 at midnight CST on Moodle. (Just one person per project group needs to submit.) Project Work Goal: Begin an analysis of your dataset to answer your supervised research question. Collaboration: If you have already formed a group (of at most 3 members) for the project, this part should be done as a group. Only one group member should submit a Project Work section. Deliverables: Please use this template to knit an HTML document. Convert this HTML document to a PDF by opening the HTML document in your web browser. Print the document (Ctrl/Cmd-P) and change the destination to “Save as PDF.” Submit this one PDF to Moodle. Alternatively, you may knit your Rmd directly to PDF if you have LaTeX installed. Data cleaning: If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the R Resources page to see if your questions are answered there. If not, post on our Piazza forum in the coding folder. Required Analyses: Exploratory analyses Make a univariate plot showing the distribution of your outcome variable. If performing a classification analysis, provide a tabulation of the levels of the outcome with data %&gt;% dplyr::count(outcome). Comment on any peculiarities of the outcome distribution (e.g., skewed distribution, bimodality, rare outcomes, outliers). Make plots exploring the relationship between a handful of interesting predictors and the outcome. (Choose a small number of predictors that seem interesting.) Comment on what you see in these plots. LASSO modeling: Ignoring nonlinearity (for now) Fit a LASSO model for your outcome which includes linear relationships between all predictors and the outcome. Estimate test performance for your models using CV. Justify your choice for the number of folds by considering the sample size. Report and interpret (with units) the CV test performance estimates along with a measure of uncertainty in the estimate (std_error is readily available when you used collect_metrics(summarize=TRUE)). (If conducting a regression analysis) Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. What insights are expected? Surprising? Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected. Choose the “best” value of lambda using an appropriate criterion, and justify your choice. Inspect the coefficient signs and magnitudes in the model resulting from the “best” lambda. Do they make sense? KNN modeling Fit a KNN model for your outcome. Estimate test performance for your models using CV. Report and interpret (with units) the CV test performance estimates along with a measure of uncertainty in the estimate. Choose the “best” value for the # of neighbors using an appropriate criterion, and justify your choice. Using your “best” model, make plots of the predicted values vs. key predictors of interest to get a sense for the relationships that KNN is “learning.” (Learning is in quotes because of KNN’s “lazy learner” feature.) See the KNN solutions on Moodle for a guide on how to do this. Summarize investigations Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both? Societal impact Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work? Portfolio Work Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. Organization: On the left side of your Google Doc (in the gray area beneath the menu bar), there is a gray icon–click this to show the section headers. Write your responses under these section headers. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Revisions: Make any revisions desired to previous concepts. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homework assignments will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance on Piazza and in drop-in hours. New concepts to address: Local regression: Algorithmic understanding: Consider the R functions lm(), predict(), dist(), and dplyr::filter(). (Look up the documentation for unfamiliar functions in the Help pane of RStudio.) In what order would these functions need to be used in order to make a local regression prediction for a supplied test case? Explain. (5 sentences max.) Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: SKIP Interpretation of output: SKIP - will be covered in the GAMs section GAMs: Algorithmic understanding: How do linear regression, splines, and local regression each relate to GAMs? Why would we want to model with GAMs? (5 sentences max.) Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: How a GAM is specified affects the time required to fit the model - why? Focus on comparing a GAM with natural cubic splines to a GAM fit with local regression and backfitting (review GAM concept video for details). (3 sentences max.) Interpretation of output: How does the interpretation of ordinary regression coefficients compare to the interpretation of GAM output? (3 sentences max.) Logistic regression: Algorithmic understanding: Write your own example of a logistic regression model formula. (Don’t use the example from the video.) Using this example, show how to use the model to make both a soft and a hard prediction. Bias-variance tradeoff: (Answer this for LASSO logistic regression) What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: (Answer this for LASSO logistic regression) Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: SKIP Interpretation of output: In general, how can the coefficient for a quantitative predictor be interpreted? How can the coefficient for a categorical predictor (an indicator variable) be interpreted? Evaluating classification models: Consider this xkcd comic. Write a paragraph (around 250 words) that addresses the following questions. Craft this paragraph so it flows nicely and does not read like a disconnected list of answers. (Include transitions between sentences.) This comic is trying to parody a classification setting - what is the outcome variable here? The new “Is it Christmas” service presented in the comic is essentially a (silly) model - what is this model? (How is the “Is it Christmas” service predicting the outcome?) How do the ideas in this comic emphasize comparisons between overall accuracy and class-specific accuracy measures? What are the names of the relevant class-specific accuracy measures here, and what are there values? How does this comic connect to the no-information rate? Data Ethics: Read the article Getting Past Identity to What You Really Want. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. What skills do you think are essential for the leaders and data analysts of organizations to have to handle these issues with care? Metacognitive Reflection (Put this reflection in your Portfolio Google Doc in the Metacognitive Reflections section, and create a subsection called “Homework 3.”) We will be having 1-on-1 learning conferences the week before Spring Break (3/6 - 3/10) to discuss your learning so far and your course goals. The goal of this reflection is to prepare for these conferences. Please directly address all of the following prompts: What are your goals for this course? What are you hoping to be able to do or understand deeply by the end of the semester? How do you feel about your progress towards these learning goals? What can you do, and what can the instructor do to make the remainder of the semester as successful as possible? For each of the following topics, comment on what you understand well and what you don’t understand as well. In doing so, please look back at your Portfolio and Quiz responses and feedback. The difference between regression and classification tasks in supervised learning Over/underfitting, cross-validation, and the bias-variance tradeoff: connections between these concepts Evaluating regression models with evaluation metrics and residual plots Subset selection (best subset and stepwise) LASSO KNN Splines Local regression Generalized additive models Evaluating classification models Logistic regression (+ LASSO) Based on your responses above, propose a midterm grade for yourself using the guidelines discussed on pages 4 and 5 of our syllabus. I will read your reflection before our conference. The goal of our conference will be to discuss your progress so far and to clarify a plan for the remainder of the semester. "],["homework-4.html", "Homework 4 Project Work Portfolio Work", " Homework 4 Portfolio Work due Friday, March 24 at midnight. (Continue working in the same Google Doc from HW1.) Project Work You will be working on your project throughout the week via our class activities, but there is nothing to submit for HW4. Portfolio Work Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. Organization: On the left side of your Google Doc (in the gray area beneath the menu bar), there is a gray icon–click this to show the section headers. Write your responses under these section headers. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Revisions: (REQUIRED) Make revisions to previous concepts based on the “STAT 253 (Instructor Reflections)” document shared with you. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homework assignments will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance on Slack and in drop-in hours. New concepts to address: Decision trees: Algorithmic understanding: Consider a dataset with two predictors: x1 is categorical with levels A, B, or C. x2 is quantitative with integer values from 1 to 100. How many different splits must be considered when recursive binary splitting attempts to make a split? Explain. (2 sentences max.) Explain the “recursive”, “binary”, and “splitting” parts of the recursive binary splitting algorithm. Make sure to discuss the concept of node (im)purity and how it is measured for classification and regression trees. Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: Recursive binary splitting does not find the overall optimal sequence of splits for a tree. What type of behavior is this? What method have we seen before that also exhibits this type of behavior? Briefly explain the parallels between these methods and what implications this have for computational time. (5 sentences max.) Interpretation of output: Explain the rationale behind the variable importance measures that decision trees provide. (4 sentences max.) Data Ethics: Read the article How to Support your Data Interpretations. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. Which pillar(s) do you think is/are hardest to do well for groups that rely on data analytics, and why? "],["homework-5.html", "Homework 5 Project Work Portfolio Work Metacognitive Reflection", " Homework 5 Portfolio Work due Friday, April 7 at midnight. (Continue working in the same Google Doc from HW1.) Project Work You will continue working on your project via our class activities. There is nothing to submit for HW5. Portfolio Work Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. Organization: On the left side of your Google Doc (in the gray area beneath the menu bar), there is a gray icon–click this to show the section headers. Write your responses under these section headers. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Revisions: Continue making revisions to previous concepts based on the “STAT 253 (Instructor Reflections)” document shared with you. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homework assignments will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance on Slack and in drop-in hours. New concepts to address: Bagging &amp; Random Forests: Algorithmic understanding: Explain the rationale for extending single decision trees to bagging models and then to random forest models. (5 sentences max.) Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: Explain why cross-validation is computationally intensive for algorithms that build many trees. What method do we have to reduce this computational burden, and why is it faster? (5 sentences max.) Interpretation of output: Explain the rationale behind the variable importance measures that random forest models provide. What specifically is different between this measure and the variable importance measure for single decision trees? (4 sentences max.) K-Means Clustering: Algorithmic understanding: Perform two iterations of k-means with k = 2 on the dataset below. The data has just 1 variable x1, and the random initial cluster assignment is shown in the cluster column. Show your work: in particular, show the centroids computed for iterations 1 and 2 and the updated cluster assignments for iterations 1 and 2. x1 cluster ---- --------- 1 1 1 2 3 2 4 1 5 1 Bias-variance tradeoff: (This is a prompt about clustering in general, but put your response in the K-Means section.) In clustering, we don’t quite have the same concepts of bias and variance as we do with supervised learning methods, but a similar type of tradeoff exists. Discuss the pros and cons of high vs. low number of clusters in terms of (1) ease of learning more about each cluster and (2) within-cluster homogeneity (closeness of cases within clusters). (5 sentences max.) Parametric / nonparametric: SKIP Scaling of variables: (This is a prompt that pertains to both k-means and hierarchical clustering, but put your response in the K-Means section.) Does the scale on which variables are measured matter for the performance of clustering? Why or why not? If scale does matter, how should this be addressed? (3 sentences max.) Computational time: Consider a single round of the cluster reassignment step of k-means with \\(n\\) cases and \\(k\\) clusters. How many distance calculations are required in this step? Explain in at most 2 sentences. Interpretation of output: (This is a prompt about clustering in general, but put your response in the K-Means section.) Describe data explorations we could use to interpret / learn more about the cluster assignments that clustering algorithms produce. Hierarchical clustering: Algorithmic understanding: We have a dataset with 4 cases, and the Euclidean distance between every pair of cases is shown below. (The column labeled 1 gives the distances of case 1 to cases 2, 3, and 4 from top to bottom.) Draw the dendrogram that would result from single-linkage clustering. Clearly label what cases are at each leaf and the heights at which fusions occur. Show any intermediate work. | 1 2 3 -----|------- ------- ------- 2 | 0.69 3 | 1.23 0.55 4 | 0.94 1.39 1.75 Bias-variance tradeoff: How does the tree cutting height relate to the tradeoff you discussed in the K-Means section? (2 sentences max.) Parametric / nonparametric: SKIP Scaling of variables: SKIP Computational time: Consider the very first step of hierarchical clustering in which all \\(n\\) cases are in their own cluster. How many distance calculations are required as a function of \\(n\\)? (Note: \\(1 + 2 + \\cdots + n = n(n+1)/2\\).) Explain in at most 2 sentences. Interpretation of output: SKIP Data Ethics: Read the article Introducing Identity Sorting Dials. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. Reflect on the use of these dials in the data underlying your own project (if applicable) or in the design of a survey that you have taken. What dials could have used more attention to improve data collection? Metacognitive Reflection (Put this reflection in your Portfolio Google Doc in the Metacognitive Reflections section, and create a subsection called “Homework 5.”) For the bagging &amp; random forests and clustering topics, what specific concepts were most challenging in the concept video? In the R video? In the class exercises? How did you work to improve your understanding of these concepts? Sit down with a friend and explain the following themes to them for the bagging &amp; random forests, k-means clustering, and hierarchical clustering: Algorithmic understanding: how does the model work? Bias-variance tradeoff: what tuning parameters control the performance of the method and how they related to over/underfitting? Interpretation of output: what output can be explored and interpreted? Scaling of variables: is variable normalization important for this algorithm? What did you learn about your understanding of these topics from this exercise? "],["homework-6.html", "Homework 6 Project Work Portfolio Work Metacognitive Reflection", " Homework 6 Portfolio Work due Friday, April 21 at midnight. (Continue working in the same Google Doc from HW1.) Project Work You will communicate the findings from you final project in two phases: a draft presentation and a polished video recording. Through this homework, you will prepare your draft presentation. Throughout the semester you have explored several methods. Your presentation should tie together the results of all of your investigations into a cohesive story. Presentation requirements: Length: Aim for a 10-12 minute presentation. Clarify your research questions. Explain how your supervised and unsupervised learning investigations help to answer your research questions. Describe your methods: What models were fit? How were models evaluated? Describe your results in the context of your research questions. Report and interpret evaluation metrics and plots. Report and interpret variable importance measures and any other modeling output that helps you answer your research question. (e.g., Explorations of predicted values from the model, coefficient estimates) Comment on any cautions that have to be kept in mind when analyzing the data or interpreting the results. Draw from insights you have taken from our Data Ethics readings over the course of the semester. Class time during the week of 4/17 - 4/21 will be devoted to working on these presentations and getting peer review. During the week of 4/24 - 4/28, we will not have class, but your group will sign up for a time to meet with me during class time to present your draft presentation. I will give you feedback to incorporate into a final, recorded version of your presentation which will be due on Moodle by Friday 5/5 at midnight. Portfolio Work Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. Organization: On the left side of your Google Doc (in the gray area beneath the menu bar), there is a gray icon–click this to show the section headers. Write your responses under these section headers. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Remember that writing is a superpower that we are intentionally honing this semester. Revisions: Continue making revisions to previous concepts based on the “STAT 253 (Instructor Reflections)” document shared with you. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homework assignments will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance on Slack and in drop-in hours. New concepts to address: Principal components analysis: Algorithmic understanding: In no more than 4 sentences, summarize the goal of principal components analysis and how it allows us to perform dimension reduction. Use the following terms / ideas in your response: linear combination, variance. Bias-variance tradeoff: How is dimension reduction related to the bias-variance tradeoff for some of the supervised methods we’ve covered? How is the use of the scree plot from PCA related to the tradeoff? Parametric / nonparametric: SKIP Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: SKIP Interpretation of output: What information can we gain by looking at the loadings of the first few principal components? Explain in at most 3 sentences. Data Ethics: Read the article Predicting the Future - Big Data, Machine Learning, and Clinical Medicine. Write a short (roughly 250 words), thoughtful response about ideas in that made you intrigued and/or concerned. Draw from your reflection on previous data ethics readings in your response. Also comment on the quote “Algorithms may be good at predicting outcomes, but predictors are not causes” in light of your project. Metacognitive Reflection (Put this reflection in your Portfolio Google Doc in the Metacognitive Reflections section, and create a subsection called “Homework 6.”) Describe your learning process for principal components analysis. What parts of the concept video were most confusing or most clear? How did your understanding change after the in-class activities? After working through the portfolio? What ideas remain uncertain? "],["final-project.html", "Final Project Requirements", " Final Project Requirements Scope: You will be analyzing a dataset using both supervised and unsupervised learning tools. Finding data: Your dataset should allow you to perform a supervised learning analysis (regression or classification) and an unsupervised learning analysis (clustering or dimension reduction). The following resources are good places to start looking for data: Kaggle Tidy Tuesday GitHub repository UCI Machine Learning Repository Harvard Dataverse Google Dataset Search Macalester’s librarians are a great resource too! They can help you find data aligning with your interests. You can make an appointment using the Ask Us page on the library website. What qualities should you look for in a dataset? Should contain an outcome variable that you would be interested in modeling Should contain roughly 10 sensible predictors or more (ID variables aren’t useful; latitude and longitude are generally not useful as predictors) NOT data where the main predictor is a time variable (e.g., year, day). This usually is a situation where the goal is to forecast a trend into the future. (This type of research question can’t be handled well using methods that we’ll cover–appropriate tools are covered in STAT 452: Correlated Data.) Collaboration: You may work in teams of up to 3 members. Individual work is fine. The homework assignments will indicate whether work for that assignment should be submitted individually or if just one team member should submit work. There will be a required synthesis of the weekly homework investigations at the end of the course. If working on a team, this should be done in groups, rather than individually. Final deliverables: Each group will be required to give a practice presentation and a final presentation. The instructor will provide feedback on the practice presentation to incorporate into the final presentation. (See our syllabus on Moodle for details.) Requirements for what to cover in the presentation will be updated here later in the semester. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

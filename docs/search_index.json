[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source: Flaticon This is the class manual for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2021. The content here was made by Leslie Myint and draws upon our class textbook, An Introduction to Statistical Learning with Applications in R, and on materials prepared by Alicia Johnson and Brianna Heggeseth. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule.html", "Schedule", " Schedule The schedule below is a tentative outline of our plans for the semester. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Goals. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Week 1: Intros and Evangelizing Evaluation (1/20 - 1/27) Dates Topic Videos/Readings Video Slides Class Slides 1/20 Introductions ISLR: Ch 1, Ch 2--Section 2.1(Skip 2.1.2, 2.1.3 for now.) 1/23 Evaluating Regression Models Evaluating Regression Models R: Introduction to TidyModels ISLR: 2.2 1/25 Overfitting Overfitting R: Preprocessing and Recipes ISLR: 5.1 1/27 Cross-validation Cross-validation R: Training, Testing and Cross-Validation ISLR: 5.1 Start Homework 1 due Friday, 2/3 at midnight CST Week 2: A Soirée with Selection Strategies (1/30 - 2/3) Dates Topic Videos/Readings Video Slides Class Slides 1/30 Subset Selection Variable Subset Selection R: Subset Selection ISLR: 6.1 2/1-2/3 LASSO (Shrinkage/Regularization) LASSO (Shrinkage/Regularization) R: LASSO and Regularization ISLR: 6.2 2/3 Community-building with the 12 Favorite Problems framework! Finish Homework 1 due Friday, 2/3 at midnight CST Week 3: A Treasure in Tradeoffs (2/6 - 2/10) Dates Topic Videos/Readings Video Slides Class Slides 2/6 KNN Regression and the Bias-Variance Tradeoff KNN Regression and the Bias-Variance Tradeoff R: KNN Regression ISLR: 2.2.2 (bias-variance tradeoff); 3.5 (KNN regression) 2/8 Quiz 1 2/10 Modeling Nonlinearity: Polynomial Regression and Splines Modeling Nonlinearity: Polynomial Regression and Splines R: Nonlinearity: Polynomial Regression and Splines ISLR: 7.1-7.4 Start Homework 2 due Friday, 2/17 at midnight CST Week 4: A Feast with Flexibility (2/13 - 2/17) Dates Topic Videos/Readings Video Slides Class Slides 2/13 Local Regression and Generalized Additive Models Local Regression and Generalized Additive Models R: Local Regression and GAMs ISLR: 7.6-7.7 2/15 Catch-up day: Regression 2/17 Logistic Regression Logistic Regression R: Logistic Regression ISLR: 4.1 - 4.3 Finish Homework 2 due Friday, 2/17 at midnight CST "],["learning-goals.html", "Learning Goals", " Learning Goals Learning goals for our course topics are listed below. Use these to guide your synthesis of video and reading material. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN (not in the video–will be discussed in class) Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Logistic regression Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Evaluating classification models Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Decision trees Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Bagging and random forests Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class) K-means clustering Clearly describe / implement by hand the k-means algorithm Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Hierarchical clustering Clearly describe / implement by hand the hierarchical clustering algorithm Compare and contrast k-means and hierarchical clustering in their outputs and algorithms Interpret cuts of the dendrogram for single and complete linkage Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before class on Monday, January 23, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. As of January 17, 2023, the latest version of R is 4.2.2. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. As of January 17, 2023, the latest version of RStudio is 2022.12.0+353. Suggested: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;tidymodels&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(tidymodels) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(\"rlang\") in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],["r-resources.html", "R Resources Outside resources Example code", " R Resources Outside resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) RStudio cheat sheets ggplot2 reference R Programming Wikibook Debugging in R Article Video Colors in R Data import tutorial Free online textbooks R for Data Science Exploratory Data Analysis with R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "],["introductions.html", "Topic 1 Introductions", " Topic 1 Introductions "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
